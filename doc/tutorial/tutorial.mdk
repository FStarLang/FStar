Title         : Verified programming in F\*
Sub title     : A tutorial
Heading Base  : 2
Author        : The F* Team
Affiliation   : MSR, MSR-Inria, Inria
Colorizer     : fstar
Package       : fullpage

.Comment      : replace=""
~CH           : .comment color=green author=CH
~CK           : .comment color=blue author=Chantal
~BB           : .comment color=purple author="Benjamin Beurdouche"
~NS           : .comment color=red author="Nik"
~MK           : .comment color=blue author=Markulf
~KK           : .comment color=orange author="Konrad Kohbrok"
~Todo         : .comment author="Todo"

~Remark       : font-style=italic padding-left=1em

~Exercise     : @h1-exercise=lower-case @h1-exercise label="@h1@h1-exercise"
                margin-left=1em
                before="[**Exercise &label;: **]{margin-left=-1em}&br;"

~Answer       : replace="[exercise answer]{.answer-link}&br;~Begin AnswerContent&nl;&source;&nl;~End AnswerContent&nl;"
                margin-top=-1em tight
~AnswerContent: border-style=solid border-width=1px border-color=black padding=0.5ex

~Code,~Pre : language=fstar
                replace=/([a-zA-Z])(\d)\b/\(\1\2|\1~\2~\)/g
                replace=/([a-zA-Z])_([\dnm])\b/\(\1_\2|\1~\2~\)/g
                replace=/_n1\b/\(_n1|~n-1~\)/g

~Fragment     : replace="```{.pre-fragment}&nl;&source;&nl;```&nl;&nl;[&link;]{float=left; font-style: italic; color: Gray}[Load in editor](&link;){float=right}&br;&nl;"
.pre-fragment : margin-bottom=0pt .framed padding=0.5ex

~ExerciseFragment : replace="~Begin Fragment {link=code/exercises/&exname;.fst}&nl;&source;&nl;~End Fragment&nl;"

~CodeFragment : replace="~Begin Fragment {link=code/&filename;?&label;&name;}&nl;&source;&nl;~End Fragment&nl;"


CatalinComment : replace="Can't understand why this doesn't work (it eats up the following solution. Also, why can't I write a proper <!-- comment here?"
~EmptyExerciseFragment : replace="~Begin Fragment {link=code/exercises/&exname;.fst}&nl;(* Write your code here *)&nl;~End Fragment&nl;"

~SolutionFragment : replace="~Begin Fragment {link=code/solutions/&exname;.fst}&nl;&source;&nl;~End Fragment&nl;"

~AnswerFragment : replace="~Begin Answer&nl;~Begin SolutionFragment {exname=&exname;}&nl;&source;&nl;~End SolutionFragment&nl;~End Answer&nl;"

~Code         : replace=/^:/\(:|\)/

<style>
.answer.view .answercontent,
.preview[data-view=full] .answer.view .answercontent,
.preview .answer .answercontent {
  display: block;
}
.answer .answercontent, .preview[data-view=full] .answer .answercontent {
  display:none;
}

.answer-link {
  cursor:pointer;
  text-decoration: underline;
  color: blue;
}
.answer.view .answer-link:before {
  content: "hide ";
}
.answercontent p:first-child {
  margin-top: 0em;
}
.answercontent p:last-child {
  margin-bottom: 0em;
}
</style>

[TITLE]
[TOC]

<style>
.fstar .token.keyword { color: blue; }
.fstar .token.type.keyword.operator { color: teal; }
</style>


<!-- Daan's hints: -->
<!-- * Refer to Section [#sec-getstarted]. -->
<!-- * Use `Alt-Q` to reformat paragraphs. -->
<!-- * Drag&Drop a `.bib` file on the editor and start including citations. -->

# Introduction

F\* is a verification-oriented programming language developed at
[Microsoft Research], MSR-Inria, and [Inria].
It follows in
the tradition of the [ML] family of languages in that it is a typed,
strict, functional programming language. However, its type system is
significantly richer than ML's, allowing functional correctness
specifications to be stated and checked semi-automatically. This
tutorial provides a first taste of verified programming in F\*. More
information about F\*, including papers and technical reports,
can be found on the [F\* website].

[Microsoft Research]: http://research.microsoft.com/en-us
[MSR-Inria]: http://www.msr-inria.fr/
[Inria]: https://www.inria.fr/
[IMDEA Software]: http://software.imdea.org/
[ML]: https://en.wikipedia.org/wiki/ML_%28programming_language%29
[F\* website]: http://www.fstar-lang.org
[mailing list]: https://lists.gforge.inria.fr/mailman/listinfo/fstar-club
[github issue]: https://github.com/FStarLang/FStar/issues

It will help if you are already familiar with functional programming
languages in the ML family (e.g., [OCaml], [F#], [Standard ML]), or
with [Haskell]---we provide a quick review of some basic concepts if you're
a little rusty, but if you feel you need more background, there are many useful
resources freely available on the web, e.g., [TryF#],
[F# for fun and profit], [Introduction to Caml], or the
[Real World OCaml] book.

[OCaml]: https://ocaml.org
[F#]: http://fsharp.org/
[Standard ML]: http://sml-family.org/
[Haskell]: https://www.haskell.org

[F# for fun and profit]: http://fsharpforfunandprofit.com/
[TryF#]: http://www.tryfsharp.org
[Introduction to Caml]: https://pl.cs.jhu.edu/pl/lectures/caml-intro.html
[Real World OCaml]: https://realworldocaml.org/

~KK
Without any experience in ML or Ocaml my experience has been that the later
exercises are very hard to solve, as some of the notation was not obvious to me. Even
knowing the correct solution, I had to either infer syntax from the exercise
code (which is fine) or go by trial and error (which was frustrating at times).
I will leave comments on the exercises detailing what kind of notation I was (or
am still) missing. Is there a resource (maybe in the wiki) that we can point
readers to, that includes examples for most of the concepts? Something like the
[F# reference] would be really helpful. Also, it might help to specify the
audience of this tutorial a bit more. As a programmer with slightly faded memory
of how inductive proofs work, the lemmas are not very straight forward. As
someone who has never seen ML or has never done any functional programming, the
syntax and some of the patterns are hard to grasp, I feel.
~

[F# reference]: https://docs.microsoft.com/en-us/dotnet/articles/fsharp/language-reference/

The easiest way to try F\* and solve the verification exercises in this tutorial is
directly in your browser by using the [online F\* editor]. You can
load the boilerplate code needed for each exercise into the online
editor by clicking the "Load in editor" link in the body of each
exercise. Your progress on each exercise will be stored in browser
local storage, so you can return to your code later (e.g. if your
browser crashes, etc).

[online F\* editor]: https://www.fstar-lang.org/run.php

You can also do this tutorial by installing F\* locally on your
machine.  F\* is open source and cross-platform, and you can get
[binary packages] for Windows, Linux, and MacOS X or compile F\* from
the [source code on github] using these [instructions].

[binary packages]: https://github.com/FStarLang/FStar/releases
[source code on github]: http://github.com/FStarLang/FStar
[instructions]: https://github.com/FStarLang/FStar/blob/master/INSTALL.md

You can edit F\* code using your favorite text editor, but for Emacs
the community maintains [fstar-mode.el], a sophisticated extension that adds special
support for F\*, including syntax highlighting, completion, type
hints, navigation, documentation queries, and interactive development
(in the style of CoqIDE or ProofGeneral).
You can find more details about [editor support] on the [F\* wiki].

The code for the exercises in this tutorial and their solutions are in the [F\*
repository] on Github. For some exercises, you have to include
additional libraries, as done by the provided Makefiles.
To include libraries for the Emacs interactive mode follow the
[instructions here](https://github.com/FStarLang/fstar-mode.el#including-non-standard-libraries-when-using-fstar-mode).

~KK
The code available on the tutorial page and on github differs
quite a bit (as does the F\* version I guess). In my case, this lead to some
unexpected errors when copying code from the online-editor to Emacs. It would be
nice to have a pointer to the actual file and maybe the proper parameters to
verify it, in case someone prefers emacs over the online editor.
~

[fstar-mode.el]: https://github.com/FStarLang/fstar-mode.el
[Atom]: https://github.com/FStarLang/fstar-interactive
[Vim]: https://github.com/FStarLang/VimFStar
[editor support]: https://github.com/FStarLang/FStar/wiki/Editor-support-for-F*
[F\* wiki]: https://github.com/FStarLang/FStar/wiki
[F\* repository]: https://github.com/FStarLang/FStar/tree/master/doc/tutorial/code/

By default F\* only verifies the input code, **it does not execute it**.
To execute F\* code one needs to extract it to OCaml or F# and then
compile it using the OCaml or F# compiler. More details on
[executing F\* code] on the [F\* wiki].

[executing F\* code]: https://github.com/FStarLang/FStar/wiki/Executing-F*-code

<!-- CH: TODO: add a chapter on extraction to tutorial -->

<!-- CH: this is just distraction
**A note on compatibility with F\#**: The syntax of F\* and F\#
overlap on a common subset. In fact, the F\* compiler is currently
programmed entirely in this intersection. Aside from this, the
languages and their implementations are entirely distinct. In this
tutorial, we will use the full syntax of F\*, even when it is possible
to express the same program in the fragment shared with F\#. The F\*
compiler sources are a good resource to turn to when trying to figure
out the syntax of this shared fragment.
-->

<!-- # Getting started  { #sec-getstarted } -->

## Your first F\* program: a simple model of access control {#sec-access-control}

Let's get started by diving into a simple model of access control on
files. Along the way, we'll see some basic concepts from functional
programming in general and a couple of F\*-specific features.

Let's say we want to model a simple access control discipline on
files---certain files are readable or writable, whereas others may be
unauthorized. We'd like to write a policy to describe the privileges
on each file, and we'd like to write programs whose file accesses
are checked against this policy, guaranteeing statically that an
unauthorized file is never accessed mistakenly.

Our model of this scenario (which we'll make more realistic in later
sections) proceeds in three easy steps.

  1. Define the policy using simple pure functions.


  2. Identify the security-sensitive primitives in the program
     and protect them with the policy.


  3. Write the rest of the program, resting assured that the F\*
     type-checker will verify statically
     that the policy-protected primitives are never improperly accessed.


### Defining a policy { #sec-policy }

The syntax of F\* is based closely on the syntax of OCaml and the non-light
syntax of F\#. Our F\* program is made up of one module per file, and the
body of the module contains a number of definitions, and optionally 
includes a 'main' expression.

Here are the first three definitions from the program:

~ Fragment {link="code/exercises/Ex01a.fst"}
[INCLUDE=code/exercises/Ex01a.fst:ACLs]
~

The first definition defines a type synonym: we'll use `:string`s to
model `:filename`s.

After that, we define two boolean functions:
`canWrite` and `canRead`:
The `canWrite` function inspects its argument `f`
using a *pattern matching expression*: it returns the boolean `true`
when `f` equals `"demo/tempfile"` and `false` otherwise. The function
`canRead` is similar---a file is readable if it is writable, or if it
is the `"demo/README"` file.

### Tying the policy to the file-access primitives

To enforce the policy, we need to connect the policy to the primitives
in our programs that actually implement the file reading and writing
operations.

F\* provides a facility to specify interfaces to external modules that
are implemented elsewhere. For example, operations that perform file
input/output are implemented by the operating system and made available
to F\* programs via the underlying framework, e.g., .NET or OCaml. We can
describe a simple interface provided by the framework as follows:

~ Fragment {link=code/exercises/Ex01a.fst}
[INCLUDE=code/exercises/Ex01a.fst:FileIO]
~

This interface provides two functions, `read` and `write`, which
in a real setup would implement the corresponding operations on files.
In our simplified example they just print things on screen.

<!-- By using the `assume val` notation, we *declare* that this module provides
     the `read` and `write` operations (they are *values*), but that we are
     *_not_ defining* those values here---they will be provided by an external
     module. Our use of `assume val` is much like the `extern`{language=csharp}
     keyword in C and other programming languages.

     As with `extern`{language=csharp}, when declaring a value, we also
     provide its type signature. -->

* The type of `read` says that it is a function expecting a `filename`
  argument `f` for which `canRead f` evaluates to `true` and returning a
  `string`-typed result. Thus the type-checker prevents any calls to
  `read` for which the `canRead` predicate can't be statically proved
  for the argument.

* The type of `write` says that it is a function of two arguments: the
  first is a `filename` `f`, such that `canWrite f` evaluates to `true`;
  the second is a `string`, the data to be written to the file `f`. The `write`
  function returns `unit`, roughly analogous to the `void` type in
  C. The type `unit` has only a single value, written `()`.

The crucial bits of these declarations are, of course, the use of the
`canRead` and `canWrite` functions in the signature, which allow us to
connect the types of these security-sensitive functions to our policy.

### Writing programs and verifying their security

Trying to check that a piece of code conforms to an access-control
policy is a tedious and error-prone process. Even if every access is
guarded by a security check, how can we make sure that each check is
adequate? We can use F\*'s type-checker to automate this kind of code
review.

Here's some simple, untrusted client code. It defines some common file names, 
and then a function called `staticChecking`, 
which tries to read and write a few files.

~ Fragment {link=code/exercises/Ex01a.fst}
[INCLUDE=code/exercises/Ex01a.fst:UntrustedClientCode]

[INCLUDE=code/exercises/Ex01a.fst:StaticChecking]
~

The type-checker ensures statically that this untrusted code complies
with the security policy. The reads to `tmp` and `readme`, and the
write to `tmp` are allowed by the policy, so the program successfully
type-checks. On the other hand, if we uncomment the lines that read
or write `"junk"` to the `password` file, the F\* type-checker complains
that the `passwd` file does not have the type
`:f:filename{canRead f}`, respectively `:f:filename{canWrite f}`.
For instance, if we uncomment the `write` we get the following error message:

<!-- fstar.exe code/exercises/Ex01a.fst -->
``` {language=""}
.\Ex01a.fst(40,10-40,16): Subtyping check failed; 
expected type (f:Ex01a.filename{(Prims.b2t (Ex01a.canWrite f))}); 
got type Ex01a.filename
```
<!-- CH: It would be nice if at some point such error messages were
     "real", coming directly from the type-checker without being
     touched by human hands. -->

You'll understand more about what that message means as you work
through this tutorial, but, for now, take it to mean that F\* expected
`canWrite passwd` to evaluate to `true`, which isn't the case.

The `staticChecking` function above illustrates how F\* can be used to
specify a security policy, and statically enforce complete and correct
mediation of that policy via type-checking. We will now illustrate
that checking of the policy doesn't have to happen all statically, but
that the dynamic checks added by the programmer are taken into account
by the type system. In particular we implement a `checkedRead` function
that consults the policy and only performs the `read` if the policy
allows it, otherwise it raises an exception.
~ Fragment {link=code/exercises/Ex01a.fst}
[INCLUDE=code/exercises/Ex01a.fst:CheckedRead]
~
Note that the type of `checkRead` imposes no condition on the
the input file `f`. When the `canRead f` check succeeds the
type-checker knows that the `read f` call is safe.

~Exercise
Write a function called `checkedWrite` that takes a filename `f` and a
string `s` as argument, checks the policy to make sure the file `f` is
writable, and only if that is the case writes `s` to `f`. If the file
is not writable your `checkedWrite` should raise an exception. As for
`checkedRead`, your `checkedRead` should have no preconditions.
~~ ExerciseFragment {exname=Ex01a}
[INCLUDE=code/exercises/Ex01a.fst:CheckedWriteType]
~~
~~ Answer
~~~ SolutionFragment {exname=Ex01a}
[INCLUDE=code/solutions/Ex01a.fst:CheckedWrite]
~~~
~~
~
You can use `checkedRead` and your `checkedWrite` to replace `read`
and `write` in `staticChecking`, so that now even
the accesses to `passwd` are well-typed.
~ ExerciseFragment {exname=Ex01a}
[INCLUDE=code/exercises/Ex01a.fst:DynamicChecking]
~
This is secure because `checkedRead` and `checkedWrite` defer to
runtime the same checks that were previously performed at compile time
by F\*, and perform the IO actions only if those checks succeed.

# Basics: Types and effects

You may not have noticed them all, but our first example already covers
several distinctive features of F\*. Let's take a closer look.

## Refinement types

One distinctive feature of F\* is its use of refinement types. We have
already seen two uses of this feature, e.g, the type
`:f:filename{canRead f}` is a *refinement* of the type `:filename`. It's
a refinement because only a subset of the `:filename` values have this
type, namely those that satisfy the `canRead` predicate. The
refinement in the type of `write` is similar. In traditional
ML-like languages, such types are inexpressible.

In general, a refinement type in F\* has the form `:x:t{phi(x)}`, a
refinement of the type `:t` to those elements `x` that satisfy the formula
`:phi(x)`. For now, you can think of `:phi(x)` as any pure boolean valued
expression. The full story is more general ([#sec-bool-versus-type]).

## Inferred types and effects for computations

~CH
Too much back and forth about type and effect _inference_ seems to be
obscuring the explanation. How about:
(1) effects (just remove every reference to inference,
  e.g. replace "is inferred to have effect E" with "has effect E"),
(2) function types,
(3) (type and) effect inference
~

Although we didn't write down any types for functions like `canRead`
or `canWrite`, F\* (like other languages in the ML family) infers
types for your program (if F\* believes it is indeed type
correct). However, what is inferred by F\* is more precise than what
can be expressed in ML. For instance, in addition to inferring a type,
F\* also infers the *side-effects* of an expression (e.g., exceptions,
state, etc).

For instance, in ML `(canRead "foo.txt")` is inferred to have type `:bool`.
However, in F\*, we infer `(canRead "foo.txt" : Tot bool)`. This
indicates that `canRead "foo.txt"` is a pure total expression, which always
evaluates to a boolean. For that matter, any expression that is
inferred to have type-and-effect `:Tot t`, is guaranteed (provided the
computer has enough resources) to evaluate to a `:t`-typed result,
without entering an infinite loop; reading or writing the program's
state; throwing exceptions; performing input or output; or, having any
other effect whatsoever.

On the other hand, an expression like `(read "foo.txt")` is
inferred to have type-and-effect `:ML string`, meaning that this term
may have arbitrary effects (it may loop, do IO, throw exceptions,
mutate the heap, etc.), but if it returns, it always returns a
string. The effect name `:ML` is chosen to represent the default,
implicit effect in all ML programs.

`:Tot` and `:ML` are just two of the possible effects. Some others
include:

* `:Dv`, the effect of a computation that may diverge;
* `:ST`, the effect of a computation that may diverge, read,
  write or allocate new references in the heap;
* `:Exn`, the effect of a computation that may diverge
  or raise an exception.

~CH
Should we add Ghost to the list of effects or would that be too
confusing at this point?
~

The primitive effects `:{Tot, Dv, ST, Exn, ML}` are arranged in a
lattice, with `:Tot` at the
bottom (less than all the others), `:ML` at the top (greater than all
the others), and with `:ST` unrelated to `:Exn`. This means that a
computation with a particular effect can be treated as having any
other effect that is greater than it in the effect ordering---we call
this feature *sub-effecting*.

In fact, you can configure F\* with your own family of effects and effect
ordering and have F\* infer those effects for your programs---but that's
more advanced and beyond the scope of this tutorial. (If curious you can
check out [this paper](https://www.fstar-lang.org/papers/dm4free/).)

From now on, we'll just refer to a type-and-effect pair (e.g. `:Tot
int` where `:Tot` is the effect and `:int` is the result type), as a
*computation* type.

## Function types

F\* is a functional language, so functions are first-class values that
are assigned function types. For instance, in ML the function `is_positive`
testing whether an integer is positive is given type `:int -> bool`,
i.e. it is a function taking an integer argument and producing a
boolean result. Similarly, a maximum function on integers `max` is given
type `:int -> int -> int`, i.e. it is a function taking two integers
and producing an integer. Function types in F\* are more precise and
capture not just the types of the arguments and the result, but also
the effect of the function's body. As such, every function value has a
type of the form `:t_1 -> ... -> t_n -> E t`, where `:t_1 .. t_n` are the
types of each of the arguments; `:E` is the effect of its body; and
`:t` is the type of its result. (*Note to the experts*: We
will generalize this to dependent functions and indexed effects
progressively.) The pure functions
above are given a type using effect `Tot` in F\*:
```
val is_positive : int -> Tot bool
let is_positive i = i > 0

val max : int -> int -> Tot int
let max i1 i2 = if i1 > i2 then i1 else i2
```
Impure functions, like reading the contents of a file,
are given corresponding effects:
```
val read : filename -> ML string
```

F\* is a _call-by-value_ functional language (like OCaml and F\#, and
unlike Haskell), so function arguments are fully evaluated to
values before being passed to the function. Values have no (immediate) effect,
thus the types of a function's arguments have no effect annotation. On
the other hand, the body of a function may perform some non-trivial
computation and so the result is always given both a type and an
effect, as explained above.

Like many functional languages, function application has higher precedence
than almost any other operation.  Remembering this can help you keep track
of when you do (and do not) need parentheses.  For example, if, using the 
definitions for `is_positive` and `max` above, you try to write:
```
let x = is_positive max 1 2
```
you will get a type resolution failure, since F\* will try to apply
`is_positive` to the `max` function itself, and it will complain that
`is_positive` expects an `int` argument, whereas `max` is a function
that maps two integers to another integer.  In this case, you need one 
set of parentheses around `max`:
```
let x = is_positive (max 1 2)
```

~Exercise
As you could see above, we use `val` to ask F\* to check that a
definition has a certain expected type.  Write down the expected types
for `canRead` and `canWrite` from [#sec-policy] and ask F\* to check it for you.
~~ ExerciseFragment {exname=Ex02a}
[INCLUDE=code/exercises/Ex02a.fst]
~~

~~ AnswerFragment {exname=Ex02a}
val canRead: filename -> Tot bool
val canWrite: filename -> Tot bool
~~
~

### The default effect is `Tot`

Since total functions are very commonly used in verified programs,
writing `Tot` is optional in F\*'s syntax.  As such, `filename ->
bool` is a shorthand for `filename -> Tot bool`. Nevertheless, we
consider it good practice to annotate functions with their result
effect, even when it is `Tot`.

For more details on the syntax of function types and definitions, in
particular the rules for default effects, see [this wiki
page][wiki-default-effects].

[wiki-default-effects]: https://github.com/FStarLang/FStar/wiki/The-syntax-of-arrow-types-and-default-effects

~CH
This seems like the right place to explain higher-order functions.
Could start by explaining currying, then functions taking functions
as arguments (e.g. sorting).
~

### Computing functions

_Skip this section on a first read, if you are new to functional programming._


Sometimes it is useful to return a function after performing some
computation. For example, here is a function that makes a new counter,
initializing it to `init`.

    val new_counter: int -> St (unit -> St int)
    let new_counter init =
      let c = ST.alloc init in
      fun () -> c := !c + 1; !c

F\* can show that `new_counter` has type `:int -> St (unit -> St int)`,
the type of a function that given an integer, may read, write or
allocate references, and then returns a `:unit -> St unit`
function. Using sub-effecting, we can also write the type below
and F\* will check that `new_counter` has that type as well.

    val new_counter: int -> ML (unit -> ML int)

Note, the type above, is *not* the same as `:int -> unit -> ML int`.
The latter is a shorthand for `:int -> Tot (unit -> ML int)`.

# First proofs about functions on integers { #sec-firstproofs }

<!--
Main ideas in this section:

* F\* provides several ways of specifying and statically verifying
  properties of programs:
  - assertions
  - refinement types
  - predicate transformers (e.g. Lemma)
* F\* types allow for two complementary verification styles:
  - intrinsic vs extrinsic (ie. at definition time vs. after the fact)
-->

Our first example on access control lists illustrated how F\*'s type
system can be used to automatically check useful security properties for
us. But, before we can do more interesting security-related programming
and verification, we will need to understand more about the basics of
F\*. We'll shift back a gear now and look at programming and proving
properties of simple functions on integers.


## Statically checked assertions

~CH
These example assertions are horrible because
(1) they are in fact lemmas, while that's usually not the case
    for assertions and
How can we fix this?
We could introduce asserts as just a way to check facts in lemmas
or as a way to check facts within (effectful) code.
~

~NS
I don't see the distinction you're making between lemmas and assertions.
~

~CH
(1) In almost about any other language asserts are used to check
_intensional_ properties of (usually stateful) programs by decorating
code. Here you use assert to prove some lemmas (extensional
properties) after the fact. Moreover, this doesn't even seem like the
recommended way to prove lemmas in F\*, given that we have better
support for this described below.

(Generally we should strive to make all the code in this tutorial be valid F\*
code that we can directly load in the editor.)
~

One way to prove properties about your code in F\* is to write assertions.

    let _ = assert (max 0 1 = 1)

Our first assertion is an obvious property about `max`. You may have seen
a construct called `assert` in other languages---typically, it is used to
check a property of your code *at runtime*, raising an assertion-failed
exception if the asserted condition is not `true`.

In F\*, assertions have no runtime significance. Instead, F\* checks
statically (i.e. before compiling your program) that the asserted
condition is always true, and hence could never fail.

In this case, the proof that `max 0 1 = 1` is fairly easy. Under the
covers, F\* translates the definition of `max` and the asserted property
to a theorem prover called [Z3], which then proves the property
automatically.

[Z3]: https://github.com/Z3Prover/z3/wiki

We can assert and automatically check more general properties about `max`. For example:

    let _ = assert (forall x y. max x y >= x
                    && max x y >= y
                    && (max x y = x || max x y = y))

## Factorial and Fibonacci functions

Let's take a look at the factorial function:

    let rec factorial n = 
      if n = 0 then 1 else n * (factorial (n - 1))

~ Remark
By default the `op_Star` operator of F\* (the `*` above) is the type constructor of product types also known as typles. If, as in the example above, we are not doing anything with tuples, it is convenient to open the module `FStar.Mul` to let `*` be multiplication. Otherwise, one has to write `op_Multiply a b` instead of `a * b`.
~

For a recursive function, without further annotation, F\* attempts to
automatically prove that the recursion always terminates. In the case
of `factorial` above, F\* attempts to prove that it has type
`int -> Tot int`, and fails to do so because, factorial is actually
not a total function on arbitrary integers! Think about `factorial -1`.

~ Remark
F\*'s `:int` type represents unbounded mathematical integers.
~

The `factorial` function is, however, a total function on non-negative
integers. We can ask F\* to check that this is indeed the case by
writing:

    val factorial: x:int{x>=0} -> Tot int

The line above says several things:

* `factorial` is a value (that's the '`val factorial`' part)

* It is a *function* (that's the arrow type '`->`')

* It can only be applied to integers `x` that are greater than or
  equal to `0` (that's the refinement type '`x:int{x>=0}`')

* When applied to `x` it always terminates, has no effects (the
'`:Tot`' part), and returns an integer (the result type '`:int`')

Once we write this down, F\* automatically proves that `factorial`
satisfies all these properties. The main interesting bit to prove, of
course, is that `factorial x` actually does terminate when `x` is
non-negative. We'll get into the details of how this is done in
section [#sec-termination], but, for now, it suffices to know that in
this case F\* is able to prove that every recursive call is
on a non-negative number that is strictly smaller than the argument of
the previous call. Since this cannot go on forever (we'll eventually hit
the base case `0`), F\* agrees with our claim about the type of
`factorial`.

The type of non-negative integers (or natural numbers) is common enough
that you can define an abbreviation for it (in fact, F\* already does
this for you in its standard prelude).

    type nat = x:int{x>=0}

~ Exercise
What are some other correct types you can give to `factorial`? Try
writing them and see if F\* agrees with you.
Try describing in words what each of those types mean.
~~ ExerciseFragment {exname=Ex03a}
[INCLUDE=code/exercises/Ex03a.fst:FactorialTypes]
~~
~~ Answer
There are many possible correct types for `factorial`. Here are a few:
~~~ SolutionFragment {exname=Ex03a}
val factorial: int -> Dv int                  (* factorial may loop forever, but may return an integer *)
val factorial: nat -> Tot int                 (* total function on nat inputs *)
val factorial: nat -> Tot nat                 (* stronger result type *)
val factorial: nat -> Tot (y:int{y>=1})       (* even stronger result type *)
~~~
~~
~

<!-- CH: This is causing too much unnecessary pain too early on
  The problem is that to give the following incomparable type for factorial:
val factorial: x:int{x>=2} -> Tot (y:int{y>=2}) (* incomparable to prev type *)
  One needs to unroll it as follows:
let rec factorial n =
  if n = 0 then 1
  else if n = 1 then 1
  else if n = 2 then 2
  else n * factorial (n - 1)

Rank the types
in order of increasing precision. Is it a total order?
~~ Answer
The types in the solution above are listed in increasing order of
precision, with the exception of the last two ones which are
incomparable.  So no, subtyping is not a total order.
~~
-->

~ Exercise
Give several types for the fibonacci function.
~~ ExerciseFragment {exname=Ex03b}
let rec fibonacci n =
  if n <= 1 then 1 else fibonacci (n - 1) + fibonacci (n - 2)
~~
~~ AnswerFragment {exname=Ex03b}
[INCLUDE=code/solutions/Ex03b.fst]
~~
~

## Lemmas

Let's say you wrote the `factorial` function and gave it the type
`:(nat -> Tot nat)`. Later, you care about some other property about
`factorial`, e.g., that if `x > 2` then `factorial x > x`. One option is
to revise the type you wrote for `factorial` and get F\* to reprove that
it has this type. But this isn't always feasible. What if you also wanted
to prove that if `x > 3` then `factorial x > 2 * x`: clearly, polluting
the type of `factorial` with all these properties that you may or may not
care about is impractical.

However, F\* will allow you to write other functions (we call them
lemmas) to prove more properties about `factorial`, after you've defined
`factorial` and given it some generally useful type like
`:nat -> Tot nat`.

A lemma is a _ghost_ total function that always returns the single `:unit` value
`()`. As such, computationally, lemmas are useless---they have no effect
and always return the same value, so what's the point? The point is that
the type of the value returned by the lemma carries some useful
properties about your program.

As a first lemma about `factorial`, let's prove that it always
returns a positive number.

    val factorial_is_positive: x:nat -> GTot (u:unit{factorial x > 0})
    let rec factorial_is_positive x =
      match x with
      | 0 -> ()
      | _ -> factorial_is_positive (x - 1)

The statement of the lemma is the type given to `factorial_is_positive`,
a ghost total function on `x:nat` which returns a unit, but with the refinement
that `factorial x > 0`. The next three lines define
`factorial_is_positive` and proves the lemma using a proof by induction
on `x`. The basic concept here is that by programming total functions, we
can write proofs about other pure expressions. We'll discuss such proofs
in detail in the remainder of this section.

~ Remark
In F\* an expression being ghost (computationally irrelevant) is an effect. The
effect `GTot` in the type of `factorial_is_positive` indicates that it is a ghost, total function.
~

### Dependent function types

~CH
Lemmas don't seem like the most intuitive way of introducing dependent
functions.  Can we find a more computationally relevant thing first?
See `max_ref_type` in `code/max.fst`?
~

Let's start by looking a little closer at the statement of the lemma.

    val factorial_is_positive: x:nat -> GTot (u:unit{factorial x > 0})

This is a *dependent function type* in F\*. Why *dependent*?
Well, the type of the result depends on the _value_ of the parameter
`x:nat`. For example, `factorial_is_positive 0` has the type
`:GTot (u:unit{factorial 0 > 0))`, which is different from the type of
`factorial_is_positive 1`.

Now that we've seen dependent functions, we can elaborate a bit more
on the general form of function types:

~ Center
`:x_1:t_1 -> ... -> E_n x_n:t_n[x_1 ... x\(~n-1~\)] -> E t[x_1 ... x_n]`
~
Each of a function's formal parameters are named
`x\(~i~\)` and each of these names is in scope to the right of the first
arrow that follows it. The notation `:t[x_1 ... x_m]` indicates that
the variables `:x_1 ... x_m` may appear free in `:t`.

### Some syntactic sugar for refinement types and lemmas

How shall we specify that `factorial` is strictly greater than
its argument when its argument is greater than 2? Here's a first cut:

    val factorial_is_greater_than_arg_1: x:(y:nat{y > 2}) -> GTot (u:unit{factorial x > x})

This does the job, but the type of the function's argument is a bit
clumsy, since we needed two names: `y` to restrict the domain to natural
numbers greater than `2` and `x` to speak about the argument in the
result type. This pattern is common enough that F\* provides some
syntactic sugar for it---we have seen it already in passing in the first
type we gave to `factorial`. Using it, we can write the following type,
which is equivalent to, but more concise than, the type above.

    val factorial_is_greater_than_arg_2: x:nat{x > 2} -> GTot (u:unit{factorial x > x})

Another bit of clumsiness that you have no doubt noticed is the result
type of the lemmas which include a refinement of the
`:unit` type. To make this lighter, F\* provides the `:Lemma` keyword which
can be used in place of the `:GTot` effect.

For example, the type of `factorial_is_greater_than_arg_2` is equivalent to the
type of `factorial_is_greater_than_arg_3` (below), which is just a little
easier to read and write.

    val factorial_is_greater_than_arg_3: x:nat{x > 2} -> Lemma (factorial x > x)

We can also write it in the following way, when that's more convenient:

    val factorial_is_greater_than_arg_4: x:nat -> Lemma (requires (x > 2))
                                                  (ensures (factorial x > x))

The formula after `requires` is the _pre-condition_ of the
function/lemma, while the one after `ensures` is its _post-condition_.

### A simple lemma proved by induction, in detail

Now, let's look at a proof of `factorial_is_greater_than_arg` in detail. Here it is:

    let rec factorial_is_greater_than_arg x =
      match x with
      | 3 -> ()
      | _ -> factorial_is_greater_than_arg (x - 1)

The proof is a recursive function (as indicated by the `let rec`); the
function is defined by cases on `x`.

In the first case, the argument is `3`; so, we need to prove that
`factorial 3 > 3`. F\* generates a proof obligation corresponding to this
property and asks [Z3] to see if it can prove it, given the definition of
`factorial` that we provided earlier---Z3 figures out that by
`factorial 3 = 6` and, of course, `6 > 3`, and the proof for this case is
done. This is the base case of the induction.

The cases are taken in order, so if we reach the second case, then we
know that the first case is inapplicable, i.e., in the second case, we
know from the type for `factorial_is_greater_than_arg` that `x:nat{x > 2}`, and
from the inapplicability of the previous case that `x <> 3`. In other
words, the second case is the induction step and handles the proof when
`x > 3`.

Informally, the proof in this case works by using the induction
hypothesis, which gives us that
```
forall n, 2 < n < x ==> factorial n > n
```
and our goal is to prove `factorial x > x`. Or, equivalently, that
`x * factorial (x - 1) > x`, knowing that `factorial (x - 1) > x - 1`; or
that `x*x - x > x`---which is true for `x > 3`.
Let's see how this works formally in F\*.

When defining a total recursive function, the function being defined
(`factorial_is_greater_than_arg` in this case) is available for use in the body
of the definition, but only at a type that ensures that the recursive
call will terminate. In this case, the type of `factorial_is_greater_than_arg` in
the body of the definition is:

    n:nat{2 < n && n < x} -> Lemma (factorial n > n)

This is, of course, exactly our induction hypothesis. By calling
`factorial_is_greater_than_arg (x - 1)` in the second case, we, in effect, make
use of the induction hypothesis to prove that
`factorial (x - 1) > x - 1`, provided we can prove that
`2 < x - 1 && x - 1 < x`---we give this to Z3 to prove, which it can,
since in this case we know that `x > 3`. Finally, we have to prove the
goal `factorial x > x` from the `x > 3` and the property we obtained
from our use of the induction hypothesis: again, Z3 proves this easily.

~ Exercise
Prove the following property for the fibonacci function:
~~ ExerciseFragment {exname=Ex03c}
[INCLUDE=code/exercises/Ex03c.fst:FibonacciGreaterThanArg]
~~
~~ AnswerFragment {exname=Ex03c}
[INCLUDE=code/solutions/Ex03c.fst:FibonacciGreaterThanArgProof]
~~
~

<!--
~ Exercise
Try proving that if `x >= 3` then `factorial x >= 2*x`.
~

~ Exercise
Try proving that if `x >= 4` then `factorial x > x*x`.
~

~ Exercise
Try proving that if `x >= 6` then `factorial x > x*x*x`.
~
Without learning a lot more about F\*, it's unlikely that you will
succeed at that last exercise. Proving properties about non-linear
arithmetic is quite difficult and Z3, as most other theorem provers, are
not great at doing it unassisted.
-->

## Admit

When constructing proofs it is often useful to assume parts of proofs,
and focus on other parts. For instance, when doing case analysis we
often want to know which cases are trivial and are automatically solved
by F\*, and which ones need manual work. We can achieve this by admitting
all but one case, if F\* can prove the lemma automatically, then the case
is trivial. For this F\* provides an `admit` function, that can be used
as follows:
```
    let rec factorial_is_greater_than_arg x =
      match x with
      | 3 -> ()
      | _ -> admit()
```
Since type-checking this succeeds we know
that the base case of the induction is trivially proved and need
to focus our effort on the inductive case.

~CH
How about adding (proper) monotonicity?
```
val fact_monotone : n:nat -> m:nat -> Lemma (requires (n <= m))
                                            (ensures (factorial n <= factorial m))
                                            (decreases m)
let rec fact_monotone n m =
  match m - n with
  | 0 -> ()
  | _ -> fact_monotone n (m-1)
```
~


<!--
## Automatic induction { #auto-ind }

Many of these proofs by induction are quite boring. You simply do case
analysis and call the induction hypothesis as needed. Surely, we should
automate this for you---and we do, in many cases using an **experimental**
_automatic induction_ feature.

Here's another proof of `factorial_is_greater_than_arg`:

    let rec factorial_is_greater_than_arg n = by_induction_on n factorial_is_greater_than_arg

Calling `by_induction_on` with `n` and `factorial_is_greater_than_arg` as argument
causes F\* to encode the induction hypothesis as a quantified logical
formula and feed it to Z3, which in simple examples can take care of
the rest.
-->

## Under the hood: `:bool` versus `:Type` { #sec-bool-versus-type }

_You can probably skip this section on a first reading, if the title
sounds mysterious to you. However, as you start to use quantified
formulas in refinement types, you should make sure you understand this
section._

~KK
Depending on the audience of this tutorial, we might want to move this to
further towards the end of the tutorial.
~
~CH
Agreed
~

We mentioned earlier that a refinement type has the form `:x:t{phi(x)}`.
In its primitive form, `phi` is in fact a type, a predicate on the value
`x:t`. We allow you to use a boolean function in place of a type: under
the covers, we implicitly coerce the boolean to a type, by adding the
coercion

    b2t (b:bool) : Type = (b == true)

The type '`==`' is the type constructor for the homogeneous equality
predicate; it is different from '`=`', which is a boolean function
comparing a pair of values (F\* also has a separate constructor for
heterogeneous equality, '`===`').

[equality on F\* wiki]: https://github.com/FStarLang/FStar/wiki/Deriving-hasEq-predicate-for-inductive-types,-and-types-of-equalities-in-F*#different-types-of-equalities-in-f

~ Remark
Equality is more complex than it looks! See also [equality on F\* wiki].
~

The propositional connectives, `:/\`, `:\/`, and `:~`, for conjunction,
disjunction and negations are type constructors. We also include `:==>`
and `:<==>` for single and bidirectional implication. In contrast,
`:&&`, `:||`, and `:not` are boolean functions.

Often, because of the implicit conversion from `:bool` to `:Type`, you
can almost ignore the distinction between the two. However, when you
start to write refinement formulas with quantifiers, the distinction
between the two will become apparent.

<!--
q:Type. True in universes too? 
-->
Both the universal quantifier `forall` and the existential quantifier
`exists`, are only available in `:Type`. When you mix refinements that
include boolean function and the quantifiers, you need to use the
propositional connectives. For example, the following formula
is well-formed:

    canRead e /\ forall x. canWrite x ==> canRead x

It is a shorthand for the following, more elaborate form:

    b2t(canRead e) /\ forall x. b2t(canWrite x) ==> b2t(canRead x)

On the other hand, the following formula is not well-formed; F\* will
reject it:

    canRead e && forall x. canWrite x ==> canRead x

The reason is that the quantifier is a type, whereas the `:&&` operator
expects a boolean. While a boolean can be coerced to a type, there is no
coercion in the other direction.


# Simple inductive types

~CH

We should explain all our syntactic restrictions on naming things:
Constructors have to start uppercase, types must start lower
case, variables
have to start lower-case. In particular every lower case thing
in a pattern is interpreted as a variable, even if it's already
bound somewhere else. We might want to raise a warning in such
cases.
~

Here's the standard definition of (immutable) lists in F\*.

    type list 'a =
      | Nil  : list 'a
      | Cons : hd:'a -> tl:list 'a -> list 'a

We note some important conventions for the constructors of a data type in F\*.

+ Constructor names must begin with a capital letter.

+ When not writing effects in constructor types,
  the default effect on the result is `Tot`.
  So, when we write `Cons: 'a -> list 'a -> list 'a`,  F\* desugars it to
  `Cons: 'a -> Tot (list 'a -> Tot (list 'a))`.

+ Constructors can be curried. So, we usually define our inductive
  types as above so that we write `Cons hd tl` (curried)
  rather than `Cons(hd,tl)` (uncurried).

+ Constructors can be partially applied, so `Cons hd` is legal and has
  type `:list 'a -> Tot (list 'a)` when `hd:'a`.

<!-- XXX -->

The `list` type is defined in F\*'s standard prelude and is available
implicitly in all F\* programs. We provide special (but standard) syntax
for the list constructors:

* `[]` is `Nil`
* `[v1; ...; vn]` is `Cons v1 ... (Cons vn Nil)`
* `hd :: tl` is `Cons hd tl`.

You can always just write out the constructors like `Nil` and `Cons`
explicitly, if you find that useful (e.g., to partially apply `Cons`).

<!--
The prelude also defines the `option` type, shown below:

```
  type option (a:Type) =
      | None : option a
      | Some : v:a -> option a
```
-->

## Proofs about basic functions on lists { #sec-basic-lists }

The usual functions on lists are programmed in F\* just as in any other ML dialect.
Here is the `length` function:

    val length: list 'a -> Tot nat
    let rec length l = match l with
      | [] -> 0
      | _ :: tl -> 1 + length tl

The type of `length` proves it is a total function on lists of `:'a`-typed
values, returning a non-negative integer.

~ Exercise
Here's the `append` function that concatenates two lists.
~~ ExerciseFragment {exname=Ex04a}
let rec append l1 l2 = match l1 with
  | [] -> l2
  | hd :: tl -> hd :: append tl l2
~~
Give `append` a type that proves it always returns a list
whose length is the sum of the lengths of its arguments.

~~ AnswerFragment {exname=Ex04a}
    l1:list 'a -> l2:list 'a -> Tot (l:list 'a{length l = length l1 + length l2})
~~
~

~ Exercise
Give `append` the weak type below,
then prove the same property as in the previous exercise using a lemma.
~~ ExerciseFragment {exname=Ex04b}
val append:list 'a -> list 'a -> Tot (list 'a)
~~
~~ AnswerFragment {exname=Ex04b}
val append_len: l1:list 'a -> l2:list 'a
             -> Lemma (requires True)
                      (ensures (length (append l1 l2) = length l1 + length l2)))
let rec append_len l1 l2 = match l1 with
    | [] -> ()
    | hd::tl -> append_len tl l2
~~
~

We wrote the type of the function `append` above using an ML-like type
variable `:'a` to stand for an arbitrary type. Under the hood F*
expands this out to something like this:
```
val append : #a:Type -> list a -> list a -> Tot (list a)
let rec append #a l1 l2 = match l1 with
  | [] -> l2
  | hd :: tl -> hd :: append #a tl l2
```

This exposes the fact that `a` is really just an implicit argument of
`append`. The implicit argument `a` is preceded by the `#` symbol
both in the type of append, where we say that `a` has type `Type`,
and in the definition of append, which takes an extra `a` argument.
When calling `append` recursively we have a choice, we can either
pass `a` explicitly by preceding it with the `#` sign, or we can
leave it out and the F* type-checker will infer it automatically.
(Implicit arguments are further discussed in [#sec-implicit-arguments].)

We would like to define a function `mem: x:'a -> l:list 'a -> Tot bool`
that decides whether `x` is present in a list `l` or
not. However, not all types in F\* have decidable equality, so in
order to write `mem` we cannot quantify over arbitrary types, but only
over those with decidable equality. Thus the following definition of
`mem` has to use `:#a:Type{hasEq a}` instead of `:#a:Type` (`hasEq a`
holds if `a` has decidable equality---that is, if there exists a
function `eq_a: a -> a -> Tot bool` such that `eq_a a_1 a_2` iff.
`a_1 == a_2`).  For convenience, the standard library provides `eqtype`
as an abbreviation of `a: Type{hasEq a}`.
```
[INCLUDE=code/exercises/Ex04c.fst:Mem]
```

~ Exercise
Prove that mem satisfies the
following property:
~~ ExerciseFragment {exname=Ex04c}
[INCLUDE=code/exercises/Ex04c.fst:AppendMem]
~~
~~ AnswerFragment {exname=Ex04c}
[INCLUDE=code/solutions/Ex04c.fst:AppendMemProof]
~~
~

~CH
Introduced implicits and eqtypes here already because they are
used in mem exercise. Alternatively, could get rid of this exercise.
~

## To type intrinsically, or to prove lemmas?

As the previous exercises illustrate, you can prove properties either
by enriching the type of a function or by writing a separate lemma
about it---we call these the 'intrinsic' and 'extrinsic' styles,
respectively. Which style to prefer is a matter of taste and
convenience: generally useful properties are often good candidates for
intrinsic specification (e.g, that `length` returns a `nat`); more
specific properties are better stated and proven as lemmas. However,
in some cases, as in the following example, it may be impossible to
prove a property of a function directly in its type---you must resort
to a lemma.

    val reverse: list 'a -> Tot (list 'a)
    let rec reverse = function
      | [] -> []
      | hd :: tl -> append (reverse tl) [hd]

Let's try proving that reversing a list twice is the identity function.
It's possible to
*specify* this property in the type of `reverse` using a refinement type.

    val reverse: #a:Type -> f:(list a -> Tot (list a)){forall l. l == f (f l)}

However, F\* refuses to accept this as a valid type for `reverse`:
proving this property requires two separate inductions, neither of
which F\* can perform automatically.

Instead, one can use two lemmas to prove the property we care about. Here
it is:

    let snoc l h = append l [h]

    val snoc_cons: l:list 'a -> h:'a -> Lemma (reverse (snoc l h) == h :: reverse l)
    let rec snoc_cons l h = match l with
      | [] -> ()
      | hd :: tl -> snoc_cons tl h

    val rev_involutive: l:list 'a -> Lemma (reverse (reverse l) == l)
    let rec rev_involutive l = match l with
      | [] -> ()
      | hd :: tl ->
        // (1) [reverse (reverse tl) == tl]
        rev_involutive tl;
        // (2) [reverse (append (reverse tl) [hd]) == h :: reverse (reverse tl)]
        snoc_cons (reverse tl) hd
        // These two facts are enough for Z3 to prove the lemma:
        //   reverse (reverse (hd :: tl))
        //   =def= reverse (append (reverse tl) [hd])
        //   =(2)= hd :: reverse (reverse tl)
        //   =(1)= hd :: tl
        //   =def= l

In the `Cons` case of `rev_involutive` we are explicitly applying not just
the induction hypothesis but also the `snoc_cons` lemma we proved above.

~Exercise
Prove that reverse is injective, i.e.,
~~ ExerciseFragment {exname=Ex04d}
[INCLUDE=code/exercises/Ex04d.fst:RevInjective]
~~
~~ Answer
Perhaps you did it like this:
~~~ SolutionFragment {exname=Ex04d}
[INCLUDE=code/solutions/Ex04d.fst:RevInjective1]
~~~
That's quite a tedious proof, isn't it. Here's a simpler proof.
~~~ SolutionFragment {exname=Ex04d}
[INCLUDE=code/solutions/Ex04d.fst:RevInjective2]
~~~
The `rev_injective_2` proof is based on the idea that every involutive
function is injective. We've already proven that `reverse` is
involutive. So, we invoke our lemma, once for `l1` and once for `l2`.
This gives to the SMT solver the information that `reverse (reverse
l1) = l1` and `reverse (reverse l2) = l2`, which suffices to complete
the proof. As usual, when structuring proofs, lemmas are your friends!
~~
~

## Higher-order functions on lists

### Mapping

We can write higher-order functions on lists,
i.e., functions taking other functions as arguments.
Here is the familiar `map`:

    let rec map f l = match l with
      | [] -> []
      | hd::tl -> f hd :: map f tl

It takes a function `f` and a list `l` and it applies `f` to each element in `l`
producing a new list. More precisely `map f [a1; ...; a_n]` produces the list
`[f a1; ...; f a_n]`. For example:

    map (fun x -> x + 1) [0; 1; 2] = [1; 2; 3]

As usual, without any specification, F\* will infer for `map` type
`:('a -> 'b) -> list 'a -> list 'b`. Written explicitly, the type of
`map` is `:('a -> Tot 'b) -> Tot (list 'a -> Tot (list 'b))`.  Notice
that the type of `f` also defaults to a function with `:Tot` effect.

By adding a type annotation we can give a _different_ type to `map`:

    val map: ('a -> ML 'b) -> list 'a -> ML (list 'b)

This type is neither a subtype nor a supertype of the type F\* inferred
on its own (for example, the manual annotation above allows you to write
`map (fun x -> FStar.IO.print_any x) l`, whereas the automatically inferred
type does not.

>**No parametric polymorphism on effects**: You might hope to give
`map` a very general type, one that accepts a function with any effect
`E` and returns a function with that same effect. This would require a
kind of effect polymorphism, which *F\* does not support*. This may
lead to some code duplication. We plan to address this by allowing you
to write more than one type for a value, an ad hoc overloading
mechanism.


### Finding

~ Exercise

We define a `find` function that given a boolean function `f` and a list
`l` returns the first element in `l` for which `f` holds. If no element
is found `find` returns `None`; for this we use the usual `:option` type (shown
below and available in F\*'s standard prelude).

~~ ExerciseFragment {exname=Ex04e}
    type option 'a =
       | None : option 'a
       | Some : v:'a -> option 'a

    let rec find f l = match l with
      | [] -> None
      | hd :: tl -> if f hd then Some hd else find f tl
~~

Prove that if `find` returns `Some x` then `f x = true`.
Is it better to do this intrinsically or extrinsically? Do it both ways.

~~ AnswerFragment {exname=Ex04e}
[INCLUDE=code/solutions/Ex04e.fst]
~~
~

~Exercise
Define the `fold_left` function on lists, so that

    fold_left f [b1; ...; bn] a = f (bn, ... (f b2 (f b1 a)))

Prove that `(fold_left Cons l [] == reverse l)`.
(The proof is a level harder from what we've done so far.  You will need to
strengthen the induction hypothesis, and possibly to prove that `append` is
associative)
~~ ExerciseFragment {exname=Ex04f}
val fold_left_cons_is_reverse: l:list 'a -> l':list 'a ->
      Lemma (fold_left Cons l l' = append (reverse l) l')
~~
~~ AnswerFragment {exname=Ex04f}
[INCLUDE=code/solutions/Ex04f.fst:FoldLeftInteresting]
~~
~

<!-- too many similar exercises
~Exercise
Define the `fold_right` function on lists, so that
`fold_right f [a1; ...; an] b` computes `f a1 (f a2 (... (f an b) ...))`.
Prove that `(fold_right Cons [] l = l)`.
~
-->

~KK
This exercise seems to be commented out in the Github version of the tutorial as
opposed to the version actually available on the tutorial page. I don't mind
having multiple exercises of a certain type.
~

## Implicitly defined functions on inductive types

For each definition of an inductive type, F\* automatically introduces a
number of utility functions: for each constructor, we have a
*discriminator*; and for each argument of each constructor, we have a
*projector*. We describe these next, using the list type (repeated below)
as a running example.

    type list 'a =
      | Nil  : list 'a
      | Cons : hd:'a -> tl:list 'a -> list 'a

### Discriminators

A discriminator is a boolean-valued total function that tests whether a
term is the application of a particular constructor. For the `list` type,
we have two discriminators that are automatically generated.

    val Nil?: list 'a -> Tot bool
    let Nil? xs = match xs with | [] -> true | _ -> false

    val Cons?: list 'a -> Tot bool
    let Cons? xs = match xs with | _ :: _ -> true | _ -> false

Identifiers like `Nil?` and `Cons?` cannot be defined by the user
directly. They are reserved to be automatically generated by the
compiler. So, the above code will not typecheck as written---it's only
for illustration purposes.

### Projectors

From the type of the `Cons` constructor, F\* generates the following
signatures:

    val Cons?.hd : l:list 'a{Cons? l} -> Tot 'a
    val Cons?.tl : l:list 'a{Cons? l} -> Tot (list 'a)

This indicates why the names `hd` and `tl` in the types of `Cons` are
significant: they correspond to the names of the auto-generated
projectors. If you don't name the arguments of a data constructor, F\*
will generate names automatically (although these names are derived from
the position of the arguments and their form is unspecified; so, it's
good practice to name the arguments yourself).

Again, names like `Cons?.hd` are reserved for internal use.

~Exercise
Write functions (call them `hd` and `tl`) whose types match
those of `Cons?.hd` and `Cons?.tl`.
~~ ExerciseFragment {exname=Ex04g}
[INCLUDE=code/exercises/Ex04g.fst]
~~
~~Answer
~~~ SolutionFragment {exname=Ex04g}
[INCLUDE=code/solutions/Ex04g.fst]
~~~
~~
~

~Exercise
Write a function that returns the `n`th element of a list. Give a
type that ensures it is total.
~~ ExerciseFragment {exname=Ex04h}
(* Write your code here *)
~~
~~ AnswerFragment {exname=Ex04h}
val nth : l:list 'a -> n:nat{n < length l} -> 'a
let rec nth l n =
  match l with
  | h :: t -> if n = 0 then h else nth t (n - 1)

(* Note how the solution above omits the [] case: that's because whenever you
   pattern match a term (to analyze it by cases), F\* insists on proving that you
   have handled all the cases. We call this the 'exhaustiveness check', and unlike
   in F\# or OCaml, it is implemented semantically. *)
~~
~

## Tuples and records

F\* provides syntactic sugar for tuples. Under the covers, tuples are
just inductive types: tuples of size 2--8 are defined in the standard
prelude. (You can add to this, if you like.) Here's the definition of
pairs from the prelude:

    type tuple2 'a 'b =
      | MkTuple2: _1:'a -> _2:'b -> tuple2 'a 'b

You can write `(5, 7)` instead of `MkTuple2 5 7`, and give it the type
`(int * int)` as a shorthand for `tuple2 int int`.

This notation generalizes to tuples of arbitrary size, so long as the
corresponding `tupleN` data type is defined in the prelude.

F\* also provides syntax for records, although internally records (like
tuples) boil down to data types with a single constructor. Here's an
example:

    type triple 'a 'b 'c = { fst:'a; snd:'b; third:'c }
    let f x = x.fst + x.snd + x.third

F\* accepts this program, giving `f` the type `:(triple int int int -> Tot int)`.

This is equivalent to the following, more verbose, source program:


    type triple 'a 'b 'c =
      | MkTriple: fst:'a -> snd:'b -> third:'c -> triple 'a 'b 'c

    let f x = MkTriple?.fst x + MkTriple?.snd x + MkTriple?.third x

# Proving termination { #sec-termination }

In F\* every pure function is proved to terminate. The termination
check used by F\* is based on a semantic criterion, not syntactic
conditions. We quickly sketch the basic structure of the F\*
termination check---you'll need to understand a bit of this in order
to write more interesting programs.

## A well-founded partial order on values

In order to prove a function terminating in F\* one provides a
measure: a pure expression depending on the function's arguments. F\*
checks that this measure strictly decreases on each recursive call.
The measure for the arguments of the call is compared to the measure
for the previous call according to a well-founded partial order on F\*
values. We write `v1 << v2` when `v1` _precedes_ `v2` in this order.

> A relation `R` is a well-founded partial order on a set `S` if, and
> only if, `R` is a partial order on `S` and there are no infinite
> descending chains in `S` related by `R`. For example, taking `S` to
> be `nat`, the set of natural numbers, the integer ordering `<` is a
> well-founded partial order (in fact, it is a total order).

Since the measure strictly decreases on each recursive call, and there
are no infinite descending chains, this guarantees that the function
eventually stops making recursive calls, i.e., it terminates.

### The precedes relation

1. **The ordering on integers**: Given `i, j : nat`,
   we have `i << j <==> i < j`.

   _Note_: Negative integers are not related by the `<<` relation,
   which is only a _partial_ order.

2. **The subterm ordering on inductive types**: For any value
    `v = D v1 ... vn` of an inductive type (where `D` is a constructor applied
    to arguments `v1` to `vn`) we include `vi << v`, for all `i`.
    That is, the sub-terms of a well-typed constructed term
    precede the constructed term. (The exception to this rule is `lex_t` below).
~CH
I guess there is also an exception for types with negative occurrences, right?
~

3. **The lexicographic ordering on lex_t**, as described below.


### Lexically ordered pairs and lists

Take a look at the classic function, `ackermann`:

    val ackermann: m:nat -> n:nat -> Tot nat
    let rec ackermann m n =
      if m=0 then n + 1
      else if n = 0 then ackermann (m - 1) 1
      else ackermann (m - 1) (ackermann m (n - 1))

Why does it terminate? At each recursive call, it is *not* the case that
all the arguments are strictly less than the arguments at the previous
call, e.g, in the second branch, `n` increases from `0` to `1`. However,
this function does in fact terminate, and F\* proves that it does.

The reason is that although each argument does not decrease in every
recursive call, when taken together, the ordered pair of arguments `(m,n)`
does decrease according to a *lexical ordering* on pairs.

In its standard prelude, F\* defines the following inductive type:

    type lex_t =
      | LexTop  : lex_t
      | LexCons : #a:Type -> a -> lex_t -> lex_t

(The `#` sign marks the first argument as implicit, see [#sec-implicit-arguments].)
This is a list of heterogenously typed elements.

**The lexicographic ordering on `lex_t`** is the following:

* `LexCons v1 v2 << LexCons v1' v2'`, if and only if, either
  `v1 << v1'`; or `v1===v1'` and `v2 << v2'`.

* If `v:lex_t` and `v <> LexTop`, then `v << LexTop`.

~MK
We could explain what that means, that longer `lex_t` are smaller?

I changed v1=v2 to heterogenous equality?
~

**We include the following syntactic sugar**:

>`%[v1;...;v_n]` is shorthand for `(LexCons v_1 ... (LexCons v_n LexTop))`

## A termination proof in detail

Suppose we are defining a function by recursion, using the following form:


    val f: y1:t_1 -> .. -> yn:t_n -> Tot t
    let rec f x1 .. xn = e

Usually, in ML, when type-checking `e`, the recursively bound function
`f` is available for use in `e` at the type
`y1:t_1 -> .. -> yn:t_n -> Tot t`, exactly the type written by the
programmer. This type is, however, not sufficient to guarantee that an
invocation of `f` does not trigger an infinite chain of recursive calls.

To prove termination (i.e., to rule out infinitely deep recursive calls), F\*
uses a different rule for type-checking recursive functions. The recursively bound name `f`
is available for use in `e`, but only at the more restrictive type shown below:

       y1:t1
    -> ...
    -> yn:t_n{%[y_1;...;y_n] << %[x_1;...;x_n]}
    -> Tot t

That is, by default, F\* requires the function's parameters to decrease
according to the lexical ordering of the parameters (in the order in
which they appear, excluding all function-typed parameters). (There is a
way to override this default---see Section [#sec-decreases-clause].)

Let's see how this works on a couple of examples.

### Ackermann

F* can show automatically that the `ackermann` function

    (* 1 *) let rec ackermann m n =
    (* 2 *)   if m=0 then n + 1
    (* 3 *)   else if n = 0 then ackermann (m - 1) 1
    (* 4 *)   else ackermann (m - 1)
    (* 5 *)                  (ackermann m (n - 1))

is terminating and has type:

    ackermann : nat -> nat -> Tot nat

F* gives this function a default termination metric, so this expands out to:

    ackermann : m:nat -> n:nat -> Tot nat (decreases %[m;n])

The `decreases` clause will be discussed in the next section. In this
example it leads to the F\* type-checker using the following more
restrictive type for the recursive calls in the body of `ackermann`:

    ackermann: m':nat
            -> n':nat{%[m';n'] << %[m;n]}
            -> Tot nat

At each of three recursive calls to `ackermann`, we need to prove not
only that the arguments are non-negative, but that they precede the
previous arguments according to the refinement formula above.


* For the call at line 3, we have to show that `%[(m - 1);1] << %[m;n]`.
This works out, since `m - 1 << m` according to the integer ordering, and
we can disregard the rest of the terms.

* The call at line 4 verifies for the same reason---the first argument
decreases.

* For the call at line 5, we have to show that `%[m; (n-1)] << %[m; n]`.
Here, the first elements of each pair are equal, so we look at the second
element, and there `n - 1 << n`, and we're done.

### Decreases clauses {#sec-decreases-clause}

Of course, sometimes you will need to override the default measure for
recursive functions. If we just swap the arguments of the `ackermann`
function the default measure no longer works and we need to add a
`decreases` clause explicitly choosing a lexicographic ordering that
first compares `m` and then `n`:

    val ackermann_swap: n:nat -> m:nat -> Tot nat (decreases %[m;n])
    let rec ackermann_swap n m =
       if m=0 then n + 1
       else if n = 0 then ackermann_swap 1 (m - 1)
       else ackermann_swap (ackermann_swap (n - 1) m)
                           (m - 1)

The optional `decreases` clause is a second argument to the `Tot` effect.
It is, in general, a total expression over the arguments of the function.

<!-- this is messed up because of mult_assoc
~Exercise
Program a tail-recursive version of `factorial` and prove it
total. Also, prove that it is functionally equivalent to `factorial`.

~~Answer
    val fact: a:nat -> n:nat -> Tot nat (decreases n)
    let rec fact a n =
      if n = 0 then a
      else fact (a * n) (n - 1)

    val fact_is_ok: a:nat
                 -> n:nat
                 -> Lemma (ensures (fact a n = a * factorial n))
                          (decreases n)
    let rec fact_is_ok a = function
      | 0 -> ()
      | n -> fact_is_ok (a * n) (n - 1)
~~
~
-->

~Exercise
Here is a more efficient (tail-recursive) variant of the list
reverse function.
~~ ExerciseFragment {exname=Ex05a}
val rev : l1:list 'a -> l2:list 'a -> Tot (list 'a) (decreases l2)
let rec rev l1 l2 =
  match l2 with
  | []     -> l1
  | hd :: tl -> rev (hd :: l1) tl
~~
Prove the following lemma showing the correctness of this
efficient implementation with respect to the previous simple
implementation:
~~ ExerciseFragment {exname=Ex05a}
val rev_is_ok : l:list 'a -> Lemma (rev [] l = reverse l)
~~
~~ AnswerFragment {exname=Ex05a}
val append_assoc : xs:list 'a -> ys:list 'a -> zs:list 'a -> Lemma
      (ensures (append (append xs ys) zs = append xs (append ys zs)))
let rec append_assoc xs ys zs =
  match xs with
  | [] -> ()
  | x :: xs' -> append_assoc xs' ys zs

val rev_is_ok_aux : l1:list 'a -> l2:list 'a -> Lemma
      (ensures (rev l1 l2 = append (reverse l2) l1)) (decreases l2)
let rec rev_is_ok_aux l1 l2 =
  match l2 with
  | [] -> ()
  | hd :: tl  -> rev_is_ok_aux (hd :: l1) tl; append_assoc (reverse tl) [hd] l1

val append_nil : xs:list 'a -> Lemma
      (ensures (append xs [] = xs))
let rec append_nil xs =
  match xs with
  | [] -> ()
  | _ :: tl  -> append_nil tl

val rev_is_ok : l:list 'a -> Lemma (rev [] l = reverse l)
let rev_is_ok l = rev_is_ok_aux [] l; append_nil (reverse l)
~~
~

~Exercise
(**Challenge!**) Here is a more efficient (linear-time)
variant of the Fibonacci function.
~~ ExerciseFragment {exname=Ex05b}
val fib : nat -> nat -> n:nat -> Tot nat (decreases n)
let rec fib a b n =
  match n with
  | 0 -> a
  | _ -> fib b (a+b) (n-1)
~~
Prove the following lemma showing the correctness of this
efficient implementation with respect to the previous simple
implementation:
~~ ExerciseFragment {exname=Ex05b}
val fib_is_ok : n:nat -> Lemma (fib 1 1 n = fibonacci n)
~~
~~ AnswerFragment {exname=Ex05b}
val fib_is_ok_aux : (n: nat) -> (k: nat) ->
  Lemma (fib (fibonacci k) (fibonacci (k + 1)) n == fibonacci (k + n))
let rec fib_is_ok_aux n k =
  if n = 0 then () else fib_is_ok_aux (n - 1) (k + 1)

val fib_is_ok : n:nat -> Lemma (fib 1 1 n = fibonacci n)
let fib_is_ok n = fib_is_ok_aux n 0
~~
~

## Mutually recursive functions

The same strategy for proving termination also works for mutually
recursive functions. In the case of mutually recursive functions, the
F\* termination checker requires that all directly or mutually
recursive calls decrease the termination metric of the called
function. This is a quite strong requirement. Consider for instance
the following example:

```
val foo : l:list int -> Tot int
val bar : l:list int -> Tot int
let rec foo l = match l with
    | [] -> 0
    | x :: xs -> bar xs
and bar l = foo l
```

This function is terminating because `foo` is always called (via
`bar`) on a sub-list. The call from `bar` to `foo` is not on a smaller
list though, and with the default metrics this example is rejected.

We can use a custom metric to convince F\* that the two functions are
terminating:

```
val foo : l:list int -> Tot int (decreases %[l;0])
val bar : l:list int -> Tot int (decreases %[l;1])
```

The metrics of `foo` and `bar` are both lexicographic tuples with the
argument `l` in the first position and `0` or `1` in the second. The
call from `foo` to `bar` decreases `l`, while the call from `bar` to
`foo` keeps `l` the same, but decreases the second component from `1`
to `0`.


# Putting the pieces together

We now look at two complete programs that exercise the various
features we have learned about so far. In each case, there are some
exercises to modify or generalize the programs.

## Quicksort on lists

A canonical example for program verification is proving various
sorting algorithms correct.

We'll start with lists of integers and describe some properties that
we'd like to hold true of a sorting algorithm, starting with a
function `sorted`, which decides when a list of integers is sorted in
increasing order.

```
  val sorted: list int -> Tot bool
  let rec sorted l = match l with
      | [] -> true
      | [x] -> true
      | x :: y :: xs -> x <= y && sorted (y :: xs)
```

Given a sorting algorithm `sort`, we would like to prove the following
property, where `mem` is the list membership function from the earlier
exercise on lists (Section [#sec-basic-lists]).

    sorted (sort l) /\ ( forall i. mem i l <==> mem i (sort l) )

We will see below that this specification can still be improved.

### Implementing Quicksort

Here's a simple implementation of the quicksort algorithm. It always
picks the first element of the list as the pivot; partitions the rest
of the list into those elements greater than or equal to the
pivot, and the rest; recursive sorts the partitions; and slots the
pivot in the middle before returning.

    let rec sort l = match l with
      | [] -> []
      | pivot :: tl ->
        let hi, lo  = partition (fun x -> pivot <= x) tl in
        append (sort lo) (pivot :: sort hi)


~Exercise
Write the partition function and prove it total.
~~ExerciseFragment {exname=Ex06a}
val partition: ('a -> Tot bool) -> list 'a -> Tot (list 'a * list 'a)
~~
~~Answer

The following definition is included in F\*'s list library.
~~~ SolutionFragment {exname=Ex06a}
[INCLUDE=code/solutions/Ex06a.fst:Partition]
~~~

<!-- ~~~ SolutionFragment {exname=Ex06a} -->
<!-- val partition: ('a -> Tot bool) -> list 'a -> Tot (list 'a * list 'a) -->
<!-- let rec partition f = function  -->
<!--   | [] -> [], [] -->
<!--   | hd :: tl ->  -->
<!--     let l1, l2 = partition f tl in -->
<!--     if f hd  -->
<!--     then hd :: l1, l2 -->
<!--     else l1, hd :: l2 -->
<!-- ~~~ -->
~~
~

### Specifying `sort`


We'll verify `sort` intrinsically, although we'll have to use some
additional lemmas. Here's a first-cut at a specification. Try asking
F\* to prove it.

    val sort_0: l:list int -> Tot (m:list int{sorted m /\ (forall i. mem i l <==> mem i m) })

~CH
Is this an exercise or not? :)
~

~KK
This was also my thought. To make the online-editor verify it, definitions of
sorted, mem, etc would be needed.
~

Unsurprisingly, it fails, on several counts:

1. We fail to prove the function total. On what grounds can we claim
   that each recursive call to `sort` is on a smaller argument?
   Intuitively, at each recursive call, the length of the list passed
   as an argument is strictly smaller, since it is a partition of
   `tl`. To express this, we'll need to add a decreases clause.


2. We fail to prove both the sortedness property and the membership
   property---we need some lemmas about `partition` and also about the
   `sorted` function.

Let's try again:

    val sort: l:list int -> Tot (m:list int{sorted m
                                         /\ (forall i. mem i l <==> mem i m)})
                                (decreases (length l))

And here are a couple of lemmas about `partition` and `sorted`:


     val partition_lemma: f:('a -> Tot bool)
        -> l:list 'a
        -> Lemma (requires True)
                 (ensures ((length (fst (partition f l))
                          + length (snd (partition f l)) = length l
                      /\ (forall x. mem x (fst (partition f l)) ==> f x)
                      /\ (forall x. mem x (snd (partition f l)) ==> not (f x))
                      /\ (forall x. mem x l = (mem x (fst (partition f l))
                                            || mem x (snd (partition f l)))))))
     let rec partition_lemma f l = match l with
       | [] -> ()
       | hd :: tl -> partition_lemma f tl

~Remark
The repeated sub-terms like `fst (partition f l)` in specifications
call for a `let`-binding construct in types. We will see how
to do that in a later section---once we have introduced some additional
machinery.
~

     val sorted_concat_lemma: l1:list int{sorted l1}
                           -> l2:list int{sorted l2}
                           -> pivot:int
                           -> Lemma (requires ((forall y. mem y l1 ==> not (pivot <= y))
                                            /\ (forall y. mem y l2 ==> pivot <= y)))
                                    (ensures (sorted (append l1 (pivot :: l2))))
     let rec sorted_concat_lemma l1 l2 pivot = match l1 with
      | [] -> ()
      | hd :: tl -> sorted_concat_lemma tl l2 pivot

Finally, we also need a lemma about `append` and `mem`:

     val append_mem:  l1:list 'a
                   -> l2:list 'a
                   -> Lemma (requires True)
                            (ensures (forall a. mem a (append l1 l2) = (mem a l1 || mem a l2)))
     let rec append_mem l1 l2 = match l1 with
       | [] -> ()
       | hd :: tl -> append_mem tl l2


Armed with these lemmas, let's return to our implementation of
quicksort. Unfortunately, with the lemmas as they are, we still can't
prove our original implementation of quicksort correct. As we saw with
our proofs of `rev_injective` (Section [#sec-basic-lists]), we need to
explicitly invoke our lemmas in order to use their properties. We'll
be able to do better in a minute, but lets see how to modify `sort` by
calling the lemmas to make the proof go through.

    let cmp i j = i <= j
    val sort_tweaked: l:list int -> Tot (m:list int{sorted m /\ (forall i. mem i l = mem i m)})
                                        (decreases (length l))
    let rec sort_tweaked l = match l with
      | [] -> []
      | pivot :: tl ->
        let hi', lo' = partition (cmp pivot) tl in
        partition_lemma (cmp pivot) tl;
        let hi = sort_tweaked hi' in
        let lo = sort_tweaked lo' in
        append_mem lo (pivot :: hi);
        sorted_concat_lemma lo hi pivot;
        append lo (pivot :: hi)

That works, but it's pretty ugly. Not only did we have to pollute our
implementation with calls to the lemmas, in order to make those calls,
we had to further rewrite our code so that we could bind names for
the arguments to those lemmas. We can do much better.


### SMT Lemmas: Bridging the gap between intrinsic and extrinsic proofs

If only we had given `partition` and `sorted` richer intrinsic types,
types that captured the properties that we proved using lemmas, we
wouldn't have had to pollute our implementation of quicksort. This is
a significant advantage of using the intrinsic proof style---every
call of a function like `partition (fun x -> pivot <= x) tl`
immediately carries all the properties that were proven intrinsically
about the function.

However, polluting the type of a general-purpose function with a very
application-specific type would also be awful. Consider enriching the
type of `sorted` with the property proven by
`sorted_concat_lemma`. The lemma is highly specialized for the
quicksort algorithm, whereas `sorted` is a nice, general specification
of sortedness that would work for insertion sort or merge sort, just
as well. Polluting its type is a non-starter. So, it seems we are
caught in a bind.

Wouldn't it be nice if there were some way to retroactively associate
a property proven about a function `f` by a lemma directly with `f`,
so that every application `f x` immediately carries the lemma
properties? F\* provides exactly such a mechanism, which gives a
way to bridge the gap between extrinsic and intrinsic proofs.

Let's say we wrote the type of `partition_lemma` as below---the only
difference is in the very last line, the addition of `[SMTPat
(partition f l)]`. That line instructs F\* and the SMT solver to
associate with every occurrence of the term `partition f l` the
property proven by the lemma.

    val partition_lemma: f:('a -> Tot bool)
      -> l:list 'a
      -> Lemma (requires True)
               (ensures ((length (fst (partition f l))
                        + length (snd (partition f l)) = length l
                    /\ (forall x. mem x (fst (partition f l)) ==> f x)
                    /\ (forall x. mem x (snd (partition f l)) ==> not (f x))
                    /\ (forall x. mem x l = (mem x (fst (partition f l))
                                          || mem x (snd (partition f l)))))))
                         [SMTPat (partition f l)]

Likewise, here's a revised type for `sorted_concat_lemma`. This time,
we instruct F\* and the SMT solver to associate the lemma property
only with very specific terms: just those that have the shape
```
sorted (append 11 (pivot :: l2))
```
The more specific a pattern you can provide,
the more discriminating the SMT solver will be in applying lemmas,
keeping your verification times acceptable.

    val sorted_concat_lemma: l1:list int{sorted l1}
                          -> l2:list int{sorted l2}
                          -> pivot:int
                          -> Lemma (requires ((forall y. mem y l1 ==> not (pivot <= y))
                                           /\ (forall y. mem y l2 ==> pivot <= y)))
                                   (ensures (sorted (append l1 (pivot :: l2))))
                             [SMTPat (sorted (append l1 (pivot :: l2)))]

Finally, we also revise the type for `append_mem` in the same way:

    val append_mem:  l1:list 'a
                  -> l2:list 'a
                  -> Lemma (requires True)
                           (ensures (forall a. mem a (append l1 l2) = (mem a l1 || mem a l2)))
                           [SMTPat (append l1 l2)]
    let rec append_mem l1 l2 = match l1 with
      | [] -> ()
      | hd :: tl -> append_mem tl l2


~Remark
 Use `SMTPat` with care! If you specify a bad pattern, you will get
 very unpredictable performance from the verification engine. To
 understand the details behind `SMTPat`, you should read up a bit on
 patterns (aka triggers) for quantifiers in SMT solvers.

~

With these revised signatures, we can write `sort` just as we
originally did, and give it the type we want. The SMT solver implicitly
derives the properties it needs from the lemmas that we have provided.

    let cmp i j = i <= j
    val sort: l:list int -> Tot (m:list int{sorted m /\ (forall i. mem i l = mem i m)})
                                (decreases (length l))
    let rec sort l = match l with
      | [] -> []
      | pivot :: tl ->
        let hi, lo = partition (cmp pivot) tl in
        append (sort lo) (pivot :: sort hi)


### Exercises

~CH
New solution files don't yet compile.
~

~CK
Now solved.
~

~CH
Exercise files still missing.
~

~CK
I do not write an exercise file Exercises 6c and 6d since it is exactly
the same as for Exercise 6b, and would thus duplicate code.
~

~Exercise
Starting from this implementation (given below), generalize `sort` so
that it works on lists with an arbitrary element type, instead of just
lists of integers. Prove it correct.
~~ ExerciseFragment {exname=Ex06b}
(* Start from the code in the "Load in editor" link *)
~~

~~ AnswerFragment {exname=Ex06b}
[INCLUDE=code/solutions/Ex06b.fst]
~~
~

~Exercise {exname=Ex06c}
Our specification for `sort` is incomplete. Why? Can you write a
variation of `sort` that has the same specification as the one above,
but discards some elements of the list?
~~ ExerciseFragment {exname=Ex06b}
(* Start from the code in the "Load in editor" link *)
~~

~~ Answer
The specification does not ensure that the resulting list is a
permutation of the initial list: it could discard or repeat some
elements.
~~~ SolutionFragment {exname=Ex06c}
[INCLUDE=code/solutions/Ex06c.fst]
~~~
~~
~

~ Exercise
Fix the specification to ensure that `sort` cannot discard any
elements in the list. Prove that `sort` meets the specification.
~~ ExerciseFragment {exname=Ex06b}
(* Start from either the problem or the solution of exercise 6b *)
~~
~~ AnswerFragment {exname=Ex06d}
[INCLUDE=code/solutions/Ex06d.fst]
~~
~

~ Exercise
Implement insertion sort and prove the same properties.
~~ ExerciseFragment {exname=Ex06e}
val sort : l:list int -> Tot (m:list int{sorted m /\ (forall x. mem x l == mem x m)})
~~
~~ AnswerFragment {exname=Ex06e}
[INCLUDE=code/solutions/Ex06e.fst]
~~
~


# Case study: simply-typed lambda-calculus

<!-- (CH: Any way I could have imported the existing code here without duplicating it?
      Without a way to do this maintaining this document and keeping it consistent
      will be a nightmare.) -->

[sf-stlc]: http://www.cis.upenn.edu/~bcpierce/sf/current/Stlc.html
[sf-stlc-prop]: http://www.cis.upenn.edu/~bcpierce/sf/current/StlcProp.html

We now look at a larger case study: proving the soundness of a type-checker
for the simply-typed $\lambda$-calculus (STLC). If you're not familiar with
STLC, you can have a look at the [Software Foundations book][sf-stlc] for
a gentle introduction given by the textual explanations (you can ignore
the Coq parts there). The formalization and proof here closely follows
the one in [Software Foundations][sf-stlc-prop]. Our proofs are, however,
shorter and much more readable than Coq proofs.

## Syntax

We represent STLC types by the F* inductive datatype `ty`.
```
type ty =
  | TBool  : ty
  | TArrow : tin:ty -> tout:ty -> ty
```
We consider Booleans as the only base type (`TBool`).
Function types are represented by the `TArrow` constructor taking two
type arguments. For instance we write `TArrow TBool TBool` for the type
of functions taking a Boolean argument and returning a Boolean result.
This would be written as `bool -> bool` in F* syntax, and
$\mathsf{bool} \to \mathsf{bool}$ in paper notation.

We represent the expressions of STLC by the datatype `exp`.
```
type exp =
  | EVar   : v:var -> exp
  | EApp   : fn:exp -> arg:exp -> exp
  | EAbs   : v:var -> vty:ty -> body:exp -> exp
  | ETrue  : exp
  | EFalse : exp
  | EIf    : test:exp -> btrue:exp -> bfalse:exp -> exp
```
Variables are represented as integer "names" decorated by the constructor `EVar`.
Variables are "bound" by lambda abstractions (`EAbs`).
For instance the identity function on Booleans is written `EAbs 0 TBool (EVar 0)`.
In paper notation one would write this function as $(\lambda x:\mathsf{bool}.~x)$.
The type annotation on the argument (`TBool`) allows for very simple type-checking.
We are not considering type inference here, to keep things simple.
The expression that applies the identity function to the `ETrue` constant is written
```
let stlc_app_id_to_true = EApp (EAbs 0 TBool (EVar 0)) ETrue
```
(in paper notation $(\lambda x:\mathsf{bool}.~x)~\mathsf{true}$).

The language also has a conditional construct (if-then-else).
For instance, the Boolean "not"
function can be written as
```
let stlc_not = EAbs 0 TBool (EIf (EVar 0) EFalse ETrue)
```
(in paper notation
$\lambda x:\mathsf{bool}.~\mathsf{if }~x~\mathsf{ then~false~else~true}$).

## Operational semantics

We define a standard small-step call-by-value interpreter for STLC.
The final result of successfully evaluating an expression is
called a _value_. We postulate that functions and the Boolean
constants are values by defining `is_value`,
a boolean predicate on expressions (a total function):
```
val is_value : exp -> Tot bool
let is_value e =
  match e with
  | EAbs _ _ _
  | ETrue
  | EFalse     -> true
  | _          -> false
```
The `EAbs`, `ETrue`, and `EFalse` cases share the same right-hand-side (`true`),
which is a way to prevent duplication in definitions.

In order to give a semantics to function applications we define a
function `subst x e e'` that substitutes `x` with `e` in `e'`:
```
val subst : int -> exp -> exp -> Tot exp
let rec subst x e e' =
  match e' with
  | EVar x' -> if x = x' then e else e'
  | EAbs x' t e1 ->
      EAbs x' t (if x = x' then e1 else (subst x e e1))
  | EApp e1 e2 -> EApp (subst x e e1) (subst x e e2)
  | ETrue -> ETrue
  | EFalse -> EFalse
  | EIf e1 e2 e3 -> EIf (subst x e e1) (subst x e e2) (subst x e e3)
```
We traverse the expression and
when we reach a variable `(EVar x')` we check whether this is the variable
`x` we want to substitute, and if it is we replace it by `e`.
For lambda abstractions `(EAbs x' t e1)` we only substitute inside the body
`e1` if `x` and `x'` are different; if they are the same we leave the body unchanged.
The reason for this is that the `x` in `e1` is bound by the abstraction:
it is a new, local name that just happens to be spelled the same as some
global name `x`. The global `x` is no longer accessible in this scope, since
it is shadowed by the local `x`. The other cases are straightforward.

> _Note for experts: Because we will only reduce closed expressions,
  where all variables
  are bound by previous lambdas, we will only ever substitute closed
  expressions `e`, and this naive definition of
  substitution is good enough. Substitution would become trickier to define
  or the representation of variables would have to change if we were considering
  the case where `e`, the expression replacing a variable in
  some other expression, may itself contain free variables._

Given the definition of values and of substitution we can now define
a small-step interpreter, as a function `step` that takes an expression `e`
and it either returns the expression to which `e` reduces in
a single step, or it returns `None` in case of an error
(all errors in this language are typing errors, and will be prevented
 statically by the type system).
```
val step : exp -> Tot (option exp)
  let rec step e =
  match e with
  | EApp e1 e2 ->
      if is_value e1 then
        if is_value e2 then
          match e1 with
          | EAbs x t e' -> Some (subst x e2 e')
          | _           -> None
        else
          match (step e2) with
          | Some e2' -> Some (EApp e1 e2')
          | None     -> None
      else
        (match (step e1) with
        | Some e1' -> Some (EApp e1' e2)
        | None     -> None)
  | EIf e1 e2 e3 ->
      if is_value e1 then
        match e1 with
        | ETrue   -> Some e2
        | EFalse  -> Some e3
        | _       -> None
      else
        (match (step e1) with
        | Some e1' -> Some (EIf e1' e2 e3)
        | None     -> None)
  | _ -> None

```
We execute an application expression `EApp e1 e2` in multiple steps by first reducing
`e1` to a value, then reducing `e2` to a value (following a *call-by-value*
evaluation order), and if additionally `e1`
is an abstraction `EAbs x t e' `we continue by substituting the formal
argument `x` by the actual argument `e2`. If not we signal a dynamic typing error
(a non-functional value is applied to arguments) by returning `None`.
For `EIf e1 e2 e3` we first reduce the guard `e1`: if the guard reduces to `true`
then we continue with `e2`, if the guard reduces to `false` we continue with `e3`,
and if the guard reduces to something else (e.g. a function) we report a
dynamic typing error. The `None -> None` cases simply propagate errors to
the top level.
```
let _ = assert (step (EApp (EAbs 0 TBool (EVar 0)) ETrue) = Some ETrue)
let _ = assert (step (EApp ETrue ETrue) = None)
```

## Type-checker

In order to assign a type to a term, we need to know what assumptions
we should make about the types of its free variables. So typing
happens with respect to a typing environment---a mapping from the
variables in scope to their types. We represent such partial maps as
functions taking an integer variable name and returning an optional type:
```
type env = int -> Tot (option ty)
```
We start type-checking closed terms in the empty environment,
i.e. initially no variables are in scope.
```
val empty : env
let empty = fun _ -> None
```
When we move under a binder we extend the typing environment.
```
val extend : env -> int -> ty -> Tot env
let extend g x t = fun x' -> if x = x' then Some t else g x'
```
For instance we type-check `EAbs x t e` in typing environment `g` by
first type-checking the body `e` in the environment `extend g x t`.

The type-checker is a total function taking an environment `g` and an expression `e`
and producing either the type of `e` or `None` if `e` is not well-typed.

```
val typing : env -> exp -> Tot (option ty)
let rec typing g e =
  match e with
  | EVar x -> g x
  | EAbs x t e1 ->
      (match typing (extend g x t) e1 with
      | Some t' -> Some (TArrow t t')
      | None    -> None)
  | EApp e1 e2 ->
      (match typing g e1, typing g e2 with
      | Some (TArrow t11 t12), Some t2 -> if t11 = t2 then Some t12 else None
      | _                    , _       -> None)
  | ETrue  -> Some TBool
  | EFalse -> Some TBool
  | EIf e1 e2 e3 ->
      (match typing g e1, typing g e2, typing g e3 with
      | Some TBool, Some t2, Some t3 -> if t2 = t3 then Some t2 else None
      | _         , _      , _       -> None)
```
Variables are simply looked up in the environment.
For abstractions `EAbs x t e1` we type-check the body `e1` under the
environment `extend g x t`, as explained above. If that succeeds and
produces a type `t'`, then the whole abstraction is given type `TArrow t t'`.
For applications `EApp e1 e2` we type-check `e1` and `e2` separately, and
if `e1` has a function type `TArrow t11 t12` and
`e2` has type `t11`, then the whole application has type `t12`.
`ETrue` and `EFalse` have type `TBool`.
For conditionals, we require that the guard has type `TBool` and
the two branches have the same type, which is also the type of the conditional.

## Soundness proof

<!-- (CH: I generally find giving people finished proofs uninformative and unhelpful.
      This seems like something to do from scratch in an emacs buffer. Trying to capture
      that kind of interaction in the current format seems very hard.) -->

We prove progress and preservation for STLC.
The **progress** theorem tells us that closed, well-typed terms do not produce
(immediate) dynamic typing errors: either a well-typed term is a value,
or it can take an evaluation step.
The proof is a relatively straightforward induction.
```
val progress : e:exp -> Lemma
      (requires (Some? (typing empty e)))
      (ensures (is_value e \/ (Some? (step e))))
let rec progress e =
  match e with
  | EApp e1 e2 -> progress e1; progress e2
  | EIf e1 e2 e3 -> progress e1; progress e2; progress e3
  | _ -> ()
```
Variables are not well-typed in the empty environment,
so the theorem holds vacuously for variables.
Boolean constants and abstractions are values,
so the theorem holds trivially for these.
All these simple cases are proved automatically by F\*.
For the remaining cases we need to use the induction hypothesis,
but otherwise the proofs are fully automated.
Under the hood F\* and Z3 are doing quite a bit of work though.

In case `e = (EApp e1 e2)` F\* and Z3 automate the following intuitive argument:
We case split on the first instance of the induction
hypothesis `(is_value e1 \/ (Some? (step e1)))`.

* If `e1` steps to `e1'` then, by the definition of `step`,
`(EApp e1 e2)` steps to `(EApp e1' e2)`.
* If `e1` is a value, we case split on the second induction hypothesis instance,
``(is_value e2 \/ (Some? (step e2)))``.
  * If `e2` steps to `e2'` then `(EApp e1 e2)` steps to `(EApp e1 e2')`,
  since `e1` is a value.
  * If `e2` is also a value, then we need to obtain that `e1` has
    a function type and from this that it is an abstraction.
    Expression `e1` has a function type because, by the definition of `typing`,
    an application is well-typed only when the first expression is a function.
    The remaining step is usually done as a separate "canonical forms" lemma, stating
    that any value that has a function type is actually an abstraction.
    Z3 can prove this fact automatically from the definitions of `typing`
    and `is_value`.

The intuitive proof of the `EIf` case is similar.

<!--
Using the _experimental_ support for automatic induction ([#auto-ind]),
the proof above can be written as just:
```
let rec progress e = by_induction_on e progress
```
-->

The **preservation** theorem (sometimes also called "subject reduction") states that
when a well-typed expression takes a step, the result is a well-typed expression
of the same type.
In order to show preservation we need to prove a couple of auxiliary results
for reasoning about variables and substitution.

The case for function application has to reason about "beta reduction" steps,
i.e. substituting the formal argument of a function with an actual value.
To see that this step preserves typing, we need to know that the
substitution itself does. So we prove a **substitution** lemma, stating that
substituting a (closed) term `v` for a variable `x` in an expression `e` preserves
the type of `e`. The tricky cases in the substitution proof
are the ones for variables and for function abstractions.
In both cases, we discover that we need to take an expression `e` that has been shown
to be well-typed in some context `g` and consider the same expression `e` in a
slightly different context `g'`. For this we prove a **context invariance** lemma,
showing that typing is preserved under "inessential changes" to the context `g`---in
particular, changes that do not affect any of the free variables of the expression.
For this, we need a definition of the free variables of an expression---i.e.,
the variables occurring in the expression that are not bound by an abstraction.

A variable `x` appears free in `e` if `e` contains some occurrence of `x`
that is not under an abstraction labeled `x`:
```
val appears_free_in : x:int -> e:exp -> Tot bool
let rec appears_free_in x e =
  match e with
  | EVar y -> x = y
  | EApp e1 e2 -> appears_free_in x e1 || appears_free_in x e2
  | EAbs y _ e1 -> x <> y && appears_free_in x e1
  | EIf e1 e2 e3 ->
      appears_free_in x e1 || appears_free_in x e2 || appears_free_in x e3
  | ETrue
  | EFalse -> false
```

We also need a technical lemma connecting free variables and typing contexts.
If a variable `x` appears free in an expression `e`,
and if we know that `e` is well typed in context `g`,
then it must be the case that `g` assigns some type to `x`.
```
val free_in_context : x:int -> e:exp -> g:env -> Lemma
      (requires (Some? (typing g e)))
      (ensures (appears_free_in x e ==> Some? (g x)))
let rec free_in_context x e g =
  match e with
  | EVar _
  | ETrue
  | EFalse -> ()
  | EAbs y t e1 -> free_in_context x e1 (extend g y t)
  | EApp e1 e2 -> free_in_context x e1 g; free_in_context x e2 g
  | EIf e1 e2 e3 -> free_in_context x e1 g;
                    free_in_context x e2 g; free_in_context x e3 g
```
The proof is a straightforward induction.
As a corollary for `g == empty` we obtain that expressions typable
in the empty environment have no free variables.
```
val typable_empty_closed : x:int -> e:exp -> Lemma
      (requires (Some? (typing empty e)))
      (ensures (not(appears_free_in x e)))
      [SMTPat (appears_free_in x e)]
let typable_empty_closed x e = free_in_context x e empty
```
The quantifier pattern `[SMTPat (appears_free_in x e)]` signals to Z3
that it should consider applying this lemma when its context contains a
term of the form `appears_free_in`

Sometimes, we know that `typing g e = Some t`,
and we will need to replace `g` by an "equivalent" context `g'`.
We still need to define formally when two environments are equivalent.
A natural definition is extensional equivalence of functions:
```
logic type equal (g1:env) (g2:env) = forall (x:int). g1 x = g2 x
```
According to this definition two environments are equivalent if have
the same domain and they map all variables in the domain to the same type.
We remark `equal` in particular and logical formulas in general
are *types* in F\*, thus the different syntax for this definition.
~CH
What does opaque do exactly?
What does logic do?
Hints for the SMT solver / logical encoding only?
~

The context invariance lemma uses in fact a weaker variant of this equivalence
in which the two environments only need to agree on the variables that appear free
in an expression `e`:
```
logic type equalE (e:exp) (g1:env) (g2:env) =
  forall (x:int). appears_free_in x e ==> g1 x = g2 x
```
The context invariance lemma is then easily proved by induction:
```
val context_invariance : e:exp -> g:env -> g':env -> Lemma
  (requires (equalE e g g'))
  (ensures (typing g e == typing g' e))
let rec context_invariance e g g' =
  match e with
  | EAbs x t e1 ->
     context_invariance e1 (extend g x t) (extend g' x t)

  | EApp e1 e2 ->
     context_invariance e1 g g';
     context_invariance e2 g g'

  | EIf e1 e2 e3 ->
     context_invariance e1 g g';
     context_invariance e2 g g';
     context_invariance e3 g g'

  | _ -> ()
```
Because `equal` is a stronger relation than `equalE` we obtain the same property
for `equal`:
```
val typing_extensional : g:env -> g':env -> e:exp -> Lemma
  (requires (equal g g'))
  (ensures (typing g e == typing g' e))
let typing_extensional g g' e = context_invariance e g g'
```
We can use these results to show the following substitution lemma:
```
val substitution_preserves_typing : x:int -> e:exp -> v:exp -> g:env -> Lemma
  (requires (Some? (typing empty v) /\
             Some? (typing (extend g x (Some?.v (typing empty v))) e)))
  (ensures (Some? (typing empty v) /\
            typing g (subst x v e) ==
            typing (extend g x (Some?.v (typing empty v))) e))
let rec substitution_preserves_typing x e v g =
  let Some t_x = typing empty v in
  let gx = extend g x t_x in
  match e with
  | ETrue -> ()
  | EFalse -> ()
  | EVar y ->
     if x=y
     then context_invariance v empty g (* uses lemma typable_empty_closed *)
     else context_invariance e gx g

  | EApp e1 e2 ->
     substitution_preserves_typing x e1 v g;
     substitution_preserves_typing x e2 v g

  | EIf e1 e2 e3 ->
     substitution_preserves_typing x e1 v g;
     substitution_preserves_typing x e2 v g;
     substitution_preserves_typing x e3 v g

  | EAbs y t_y e1 ->
     let gxy = extend gx y t_y in
     let gy = extend g y t_y in
     if x=y
     then typing_extensional gxy gy e1
     else
       (let gyx = extend gy x t_x in
        typing_extensional gxy gyx e1;
        substitution_preserves_typing x e1 v gy)
```
The proof proceeds by induction on the expression `e`;
we give the intuition of the two most interesting cases:

* Case `e = EVar y`
  * Case `x = y`: We have `subst x v e = v` and we already know that
    `typing empty v == Some t_x`. However, what we need to show is
    `typing g v == Some t_x` for some arbitrary environment `g`.
    From the `typable_empty_closed` lemma we obtain that `v` contains
    no free variables, so we have `equalE v empty g`. This allows us
    to apply the `context_invariance` lemma to obtain that
    `typing empty v = typing g v` and complete the proof of this case.
  * Case `x <> y`: We have `subst x v e = e` and
    `typing gx e = Some t_e` and need to show that
```
typing g e == Some t_e
```
    We have that `EquivE (EVar y) gx g` since `x <> y` so we can apply
    the `context_invariance` lemma to conclude.
* Case `e = EAbs y t_y e1`:
  We have that `typing gxy e1 = Some t_e1`,
  and need to show that
```
typing gy e1 == Some t_e1
```
  * Case `x = y`: We have `subst x v e = EAbs y t_y e1`.
    Since `x = y` the `x` binder in `gxy` is spurious (we have `equal gy gxy`)
    and can apply the `typing_extensional` lemma.
  * Case `x <> y`: We have `subst x v e = EAbs y t_y (subst x v e1)`.
    Since `x <> y` (and since we are in a simply typed calculus, not
    a dependently typed one) we can swap the `x` and `y` binding to show
    that `equal gxy xyx`. By the `typing_extensional` lemma we obtain that
    `typing gxy e1 == typing gyx e1`. By the induction hypothesis we thus
    obtain that
```
typing gy (subst x v e1) == Some t_e1
```
    and by the definition
    of `typing` we conclude that `typing g (EAbs y t_y (subst x v e)) = t_e`.

We now have the tools we need to prove preservation:
if a closed expression `e` has type `t` and takes an evaluation step to `e'`,
then `e'` is also a closed expression with type `t`.
In other words, the small-step evaluation relation preserves types.
```
val preservation : e:exp -> Lemma
  (requires (Some? (typing empty e) /\ Some? (step e) ))
  (ensures (Some? (step e) /\
            typing empty (Some?.v (step e)) == typing empty e))
let rec preservation e =
  match e with
  | EApp e1 e2 ->
     if is_value e1
     then (if is_value e2
           then let EAbs x _ ebody = e1 in
                substitution_preserves_typing x ebody e2 empty
           else preservation e2)
     else preservation e1

  | EIf e1 _ _ ->
      if not (is_value e1) then preservation e1
```
We only have two cases to consider, since only applications and conditionals
can take successful execution steps. The case for `e = EIf e1 e2 e3` is simple:
either `e1` is a value and thus the conditional reduces to `e2` or `e3` which
by the typing hypothesis also have type `t`, or `e1` takes a successful step and
we can apply the induction hypothesis. We use the `Some?.v` projector,
which requires F\* to prove that indeed `e1` can take a step;
this is immediate since we know that the whole conditional takes a step
and we know that `e1` is not a value.

The case for `e = EApp e1 e2` is a bit more complex. If `e1` steps or if `e1` is
a value and `e2` steps then we just apply the induction hypothesis.
If both `e1` and `e2` are values it needs to be the case that `e1` is
an abstraction `EAbs x targ ebody` and `step e = subst x e2 ebody`.
From the typing assumption we have `typing (extend empty x tags) ebody = Some t`
and `typing empty e2 = Some targ`, so we can use the substitution lemma
to obtain that `typing empty (subst x e2 ebody) = Some t`, which concludes the proof.

## Exercises for STLC

~ Exercise
Define a `typed_step` step function that takes a
well-typed expression `e` that is not a value and
produces the expression to which `e` steps. Give `typed_step`
the following strong type
(basically this type captures both progress and preservation):
~~ ExerciseFragment {exname=Ex07a}
  val typed_step : e:exp{Some? (typing empty e) /\ not(is_value e)} ->
                   Tot (e':exp{typing empty e' = typing empty e})
~~
(Hint: the most direct solution to this exercise fits on one line)
~~ AnswerFragment {exname=Ex07a}
    let typed_step e = progress e; preservation e; Some?.v (step e)
~~
~

~ Exercise
To add pairs to this formal development we add the following to the
definition of types, expressions, values, substitution, and step:
~~ ExerciseFragment {exname=Ex07b}
type ty =
  ...
  | TPair : ty -> ty -> ty

type exp =
  ...
  | EPair  : exp -> exp -> exp
  | EFst   : exp -> exp
  | ESnd   : exp -> exp
~~
(note the extra rec in `is_value` below!)
~~ ExerciseFragment {exname=Ex07b}
let rec is_value e =
  match e with
  ...
  | EPair e1 e2 -> is_value e1 && is_value e2

let rec subst x e e' =
  match e' with
  ...
  | EPair e1 e2 -> EPair (subst x e e1) (subst x e e2)
  | EFst e1 -> EFst (subst x e e1)
  | ESnd e1 -> ESnd (subst x e e1)

let rec step e =
  ...
  | EPair e1 e2 ->
      if is_value e1 then
        if is_value e2 then None
        else
          (match (step e2) with
          | Some e2' -> Some (EPair e1 e2')
          | None     -> None)
      else
        (match (step e1) with
        | Some e1' -> Some (EPair e1' e2)
        | None     -> None)
  | EFst e1 ->
      if is_value e1 then
        (match e1 with
        | EPair v1 v2 -> Some v1
        | _           -> None)
      else
        (match (step e1) with
        | Some e1' -> Some (EFst e1')
        | None     -> None)
  | ESnd e1 ->
      if is_value e1 then
        (match e1 with
        | EPair v1 v2 -> Some v2
        | _           -> None)
      else
        (match (step e1) with
        | Some e1' -> Some (ESnd e1')
        | None     -> None)
~~
Add cases to `typing` for the new constructs and fix all the proofs.

~~ Answer
We extend the `typing` and `appears_free_in` functions with cases
for `EPair`, `EFst`, and `ESnd`:
~~~ SolutionFragment {exname=Ex07b}
val typing : env -> exp -> Tot (option ty)
let rec typing g e =
...
  | EPair e1 e2 ->
      (match typing g e1, typing g e2 with
      | Some t1, Some t2 -> Some (TPair t1 t2)
      | _      , _       -> None)
  | EFst e1 ->
      (match typing g e1 with
      | Some (TPair t1 t2) -> Some t1
      | _                  -> None)
  | ESnd e1 ->
      (match typing g e1 with
      | Some (TPair t1 t2) -> Some t2
      | _                  -> None)

val appears_free_in : x:int -> e:exp -> Tot bool
...
  | EPair e1 e2 -> appears_free_in x e1 || appears_free_in x e2
  | EFst e1 -> appears_free_in x e1
  | ESnd e1 -> appears_free_in x e1
~~~
The proofs of the lemmas are also easy to extend by just calling
the induction hypothesis:
~~~ SolutionFragment {exname=Ex07b}
val free_in_context : x:int -> e:exp -> g:env -> Lemma
      (requires (Some? (typing g e)))
      (ensures (appears_free_in x e ==> Some? (g x)))
let rec free_in_context x e g =
...
  | EPair e1 e2 -> free_in_context x e1 g; free_in_context x e2 g
  | EFst e1
  | ESnd e1 -> free_in_context x e1 g

val context_invariance : e:exp -> g:env -> g':env
                     -> Lemma
                          (requires (equalE e g g'))
                          (ensures (typing g e == typing g' e))
let rec context_invariance e g g' =
...
| EPair e1 e2 ->
     context_invariance e1 g g';
     context_invariance e2 g g'

  | EFst e1
  | ESnd e1 -> context_invariance e1 g g'

val substitution_preserves_typing : x:int -> e:exp -> v:exp ->
      g:env{Some? (typing empty v) &&
            Some? (typing (extend g x (Some?.v (typing empty v))) e)} ->
      Tot (u:unit{typing g (subst x v e) ==
                  typing (extend g x (Some?.v (typing empty v))) e})
let rec substitution_preserves_typing x e v g =
...
  | EPair e1 e2 ->
     (substitution_preserves_typing x e1 v g;
      substitution_preserves_typing x e2 v g)

  | EFst e1
  | ESnd e1 ->
      substitution_preserves_typing x e1 v g
~~~
As for the other cases, the preservation proof when `e = EPair e1 e2`
follows the structure of the `step` function. If `e1` is not a value
then it further evaluates, so we apply the induction hypothesis to
`e1`.  If `e1` is a value, then since we know that the pair evaluates,
it must be the case that `e2` is not a value and further evaluates, so
we apply the induction hypothesis to it. The cases for `EFst` and
`ESnd` are similar.
~~~ SolutionFragment {exname=Ex07b}
val preservation : e:exp{Some? (typing empty e) /\ Some? (step e)} ->
      Tot (u:unit{typing empty (Some?.v (step e)) == typing empty e})
let rec preservation e =
...
  | EPair e1 e2 ->
      (match is_value e1, is_value e2 with
      | false, _     -> preservation e1
      | true , false -> preservation e2)

  | EFst e1
  | ESnd e1 ->
      if not (is_value e1) then preservation e1
~~~
~~
~

<!--
(CH: Quite a lot of stuff to add to the definitions, maybe just
 give them all that code, or at least the step extension.
 Otherwise they can get it wrong (I did) and have a hard time in the proofs.)
-->

<!--
_Exercise: Add arithmetic expressions to this formal development.
Copy the stlc.fst file and add the following to the definition of
types and expressions (`EIf0` tests if a number is zero or not):_
```
type ty =
  ...
  | TNat : ty

type exp =
  ...
  | ENat  : nat -> exp
  | EPlus : exp -> exp -> exp
  | EIf0  : exp -> exp -> exp -> exp
```
(CH: The pairs and arithmetic exercises are rather similar, so just do one of them.
None of them is very interesting from a proof point of view.)
-->

~ Exercise
We want to add a let construct to this formal development.
We add the following to the definition of expressions:
~~ ExerciseFragment {exname=Ex07c}
type exp =
  ...
  | ELet  : int -> exp -> exp -> exp
~~
Add cases for `ELet` to all definitions and proofs.

[Load answer in editor](code/solutions/Ex07c.fst)
~~ CH
Make the prev kind of link a macro of some sorts. It doesn't have a
proper body so it's not an environment, but what is it? Entity?
~~
~

~ Exercise
Define a big-step interpreter for STLC as a recursive
function `eval` that given a well-typed expression `e`
either produces the well-typed value `v` to which `e` evaluates
or it diverges if the evaluation of `e` loops. Give `eval` the following
strong type ensuring that `v` has the same type as `e`
(basically this type captures both progress and preservation):
~~ ExerciseFragment {exname=Ex07d}
  val eval : e:exp{Some? (typing empty e)} ->
             Dv (v:exp{is_value v && typing empty v = typing empty e})
~~
The `Dv` effect is that of potentially divergent computations.
We cannot mark this as `Tot` since _a priori_ STLC computations could loop,
and it is hard to prove that well-typed ones don't.
~~Answer
Here is a solution that only uses `typed_step` (suggested by Santiago Zanella):
~~~ SolutionFragment {exname=Ex07d}
val eval : e:exp{Some? (typing empty e)} ->
           Dv (v:exp{is_value v && typing empty v = typing empty e})
let rec eval e =
  if is_value e then e
  else eval (typed_step e)
~~~
or using the `progress` and `preservation` lemmas instead of `typed_step`
(suggested by Guido Martinez):
```
val eval : e:exp{Some? (typing empty e)} ->
  Dv (v:exp{is_value v && typing empty v = typing empty e})
let rec eval e = match step e with
  | None -> progress e; e
  | Some e' -> preservation e; eval e'
```
Here is another solution that only uses the substitution lemma:
~~~ SolutionFragment {exname=Ex07d}
val eval' : e:exp{Some? (typing empty e)} ->
            Dv (v:exp{is_value v && typing empty v = typing empty e})
let rec eval' e =
  let Some t = typing empty e in
  match e with
  | EApp e1 e2 ->
     (let EAbs x _ e' = eval' e1 in
      let v = eval' e2 in
      substitution_preserves_typing x e' v empty;
      eval' (subst x v e'))
  | EAbs _ _ _
  | ETrue
  | EFalse     -> e
  | EIf e1 e2 e3 ->
     (match eval' e1 with
      | ETrue  -> eval' e2
      | EFalse -> eval' e3)
  | EPair e1 e2 -> EPair (eval' e1) (eval' e2)
  | EFst e1 ->
     let EPair v1 _ = eval' e1 in v1
  | ESnd e1 ->
     let EPair _ v2 = eval' e1 in v2
  | ELet x e1 e2 ->
     (let v = eval' e1 in
      substitution_preserves_typing x e2 v empty;
      eval' (subst x v e2))
~~~
~~
~

<!-- Relating small step and big step is hard and boring
val steps_n : nat -> exp -> Tot (option exp)
let rec steps_n n e =
  match n with
  | 0 -> Some e
  | _ -> (match step e with
          | Some e' -> steps_n (n-1) e'
          | _       -> None)

opaque logic type Steps (e:exp) (e':exp) =
  (exists (n:nat). steps_n n e == Some e')

assume val step_last : e:exp -> e':exp -> e'':exp -> Lemma
             (requires (Steps e e' /\ step e' = Some e''))
             (ensures (Steps e e''))

val eval : e:exp{Some? (typing empty e)} ->
           Dv (v:exp{is_value v && typing empty v = typing empty e
                     /\ Steps e v}) --- added this
-->
<!--
_Exercise: Extend the type-checker so that it produces error messages in case
of ill-typed terms, instead of just `None`_
(CH: boring programming exercise? nothing to do with proving/verifying)

_Exercise: Change the step and typing functions to use primitive exceptions
instead of options_ (CH: Not sure this is solvable with the current F\*,
but it seems quite interesting)
-->

# Higher kinds, indexed types, implicit arguments, and type-level functions

So far, we have mostly programmed total functions and given them
specifications using rich types. Under the covers, F\* has a second
level of checking at work to ensure that the specifications that you
write are also meaningful---this is F\*'s kind system. Before moving
on to more advanced F\* features, you'll have to understand the kind
system a little. In short, just as types describe properties of
programs, kinds describe properties of types.

## `Type`: The type of types

The basic construct in the kind system is the constant `Type`, which
is the basic type of a type. For example,
just as we say that `0 : int` (zero has type `int`), we also say
`int : Type` (`int` has kind `Type`), `bool : Type`, `nat : Type` etc.

## Arrow kinds

Beyond the base types, consider types like `list int`, `list bool`
etc.  These are also types, of course, and in F\* `list int : Type`
and `list bool : Type`. But, what about the unapplied type
constructor `list`---it's not a `Type` on its own; it only produces a
`Type` when applied to a `Type`. In that sense, `list` is type
constructor. To describe this, just as we use the `->` type
constructor to describe functions at the level of programs, we use the
`->` kind constructor to describe functions at the level of types. As
such, we write `list : Type -> Type`, meaning that it produces a
`Type` when applied to a `Type`.

## Indexed types and implicit arguments

Beyond inductive types like `list`, F\* also supports inductive type
families, or [GADTs]. For example, here is one definition of a
`vector`, a length-indexed list.

```
type vector (a:Type) : nat -> Type =
   | Nil :  vector a 0
   | Cons : hd:a -> n:nat -> tl:vector a n -> vector a (n + 1)
```

Here, we define a type constructor `vector (a:Type) : nat -> Type`,
which takes two arguments: it produces a `Type` when applied first to
a `Type` and then to a `nat`.

The point is to illustrate that just as functions at the level of
programs may take either type- or term-arguments, types themselves can
take either types or pure expressions as arguments.

[GADTs]: https://en.wikipedia.org/wiki/Generalized_algebraic_data_type

### Implicit arguments

When writing functions over terms with indexed types, it is convenient
to mark some arguments as implicit, asking the type-checker to infer
them rather than having to provide them manually.

For example, the type of a function that reverses a vector can be
written as follows:

```
val reverse_vector: #a:Type -> #n:nat -> vector a n -> Tot (vector a n)
```

This indicates that `reverse_vector` expects three arguments, but the
`#` sign mark the first two arguments as implicit. Given a term
`v : vector t m`, simply writing `reverse_vector v` will type-check as
`vector t m`---internally, F\* elaborates the function call to
`reverse_vector #t #m v`, instantiating the two implicit arguments
automatically. To view what F\* inferred, you can provide the
`--print_implicits` argument to the compiler, which will cause all
implicitly inferred arguments to be shown in any message from the
compiler. Alternatively, you can write `reverse_vector #t #m v`
yourself to force the choice of implicit arguments manually.
Applying this to the `Cons` constructor's `n` argument we get:

```
type vector (a:Type) : nat -> Type =
   | Nil :  vector a 0
   | Cons : hd:a -> #n:nat -> tl:vector a n -> vector a (n + 1)
```

With this, given `v: vector int m`, we can write `Cons 0 v` to build a
`vector int (m + 1)`.

When you write a type like `'a -> M t wp` (note the "ticked" variable
`'a`), this is simply syntactic sugar for `#a:Type -> a -> M t wp`,
with every occurrence of `'a` replaced by `a` in `M t wp`---in other
words, the "ticked" type variable is an implicit type argument.
So the signature below is equivalent to the previous one.

```
val reverse_vector' : #n:nat -> vector 'a n -> Tot (vector 'a n)
```

## Dependent kind arrows and type function

Even some of the built-in type constructors can be given
kinds. Consider the refinement type constructor: `_:_{_}`, whose
instances include types we've seen before, like `x:int{x >= 0}`, which
is the type `nat`, of course.  With less syntactic sugar, you can view
the type `x:int{phi(x)}` as the application of a type constructor
`refinement` to two arguments `int` and a refinement formula
`fun x -> phi(x)`. As discussed in Section [#sec-bool-versus-type], the formula
`phi(x)` is itself a `Type`. Thus, the kind of the `refinement`
constructor is `a:Type -> (a -> Type) -> Type`, meaning it takes two
arguments; first, a type `a`; then a second argument, a predicate on
`a` (whose kind depends on `a`); and then builds a `Type`.

The type `fun x -> phi(x)` is type function, a lambda abstraction at
the level of types, which when applied to a particular value, say,
`0`, reduces as usual to `phi(0)`.

# Specifying effects

~CH
This seems like super advanced stuff that should come a lot
later in the tutorial. How about swapping 2 references first?
(have a couple of such simple examples in the Saarbrucken course)
~

F\* provides a mechanism to define new effects. As mentioned earlier,
the system is already configured with several basic effects, including
non-termination, state and exceptions.  In this section, we look
briefly at how effects are defined, using the `PURE` and `STATE`
effects for illustration.

On a first reading, you may wish to skip this section and proceed
directly to the next chapter.


## Background: Generating compact verification conditions

Type inference in F\* is based on a weakest pre-condition calculus for
higher order programs. For every term `e`, F\* computes `WP e`, which
captures the semantics of `e` as a _predicate transformer_. If you want
to prove some property `P` about the result of `e` (i.e., a
post-condition), it suffices to prove `WP e P` of the initial
configuration in which `e` is executed (i.e., a pre-condition).

Let's start by considering a very simple, purely functional
programming language with only the following forms:

```
b :: = x | true | false
e :: = b | let x = e1 in e2 | assert b | if b then e1 else e2
```

For this tiny language, the post-conditions `P` that you may want to
prove are predicates on the boolean results of a computation, while
the pre-conditions are just propositions. Defining `WP e` for this
language is fairly straightforward:

```
WP b P                      = P b
WP (let x = e1 in e2) P     = WP e1 (fun x -> WP e2 P)
WP (assert b) P             = b /\ P b
WP (if b then e1 else e2) P = (b ==> WP e1 P) /\ ((not b) ==> WP e2 P)
```

This is nice and simple, but it has a significant drawback,
particularly in the fourth rule: the post-condition formula `P` is
duplicated across the branches. As several branching expressions are
sequenced, this leads to an exponential blowup in the size of the
verification condition.

This problem with WP computations is well known---[Flanagan and Saxe]
and [Leino] both study this problem and provide equivalent
solutions. In essence, they show that for a first-order,
assignment-free guarded command language the weakest pre-condition can
be computed in a size nearly linear in the size of the program. Then,
they show that the first-order imperative WHILE language can be converted
into SSA form and then into a guarded command language.

[Flanagan and Saxe]: http://dl.acm.org/citation.cfm?id=360220
[Leino]: http://dl.acm.org/citation.cfm?id=1710882

In the context of F\*, we generalize the results of Flanagan and Saxe,
and Leino, to the setting of a higher-order language. The main
technical idea is a new "Double [Dijkstra Monad]". Given a program, F\*
infers a monadic computation type indexed by (1) a WP, the weakest
pre-condition predicate transformer; and (2) a WLP, the weakest
liberal pre-condition predicate transformer. Intuitively, `WLP e Q` is
sufficient to guarantee that if `e` contains no internal assertion
failures, then its result satisfies `P`; in contrast, the `WP e P` is
sufficient to guarantee both that `e`'s internal assertions succeed and
that its result satisfies `P`.

Here's the definition of `WLP` for our small example language.

```
WLP b P                      = P b
WLP (let x = e1 in e2) P     = WLP e1 (fun x -> WLP e2 P)
WLP (assert b) P             = b ==> P b
WLP (if b then e1 else e2) P = (b ==> WLP e1 P) /\ ((not b) ==> WLP e2 P)
```

The only difference with the `WP` is in the case of assertions. For a
WLP, rather than requiring a proof of the assertion, we try to prove
the post-condition assuming the assertion succeeds.

How does this help us? Let's see.

First, you can prove the following property about `WLP`.

```
(Identity 1)
           WLP e P
      <==> forall x. P x \/ WLP e (fun y -> y <> x)
```

Here's one way to read formula (1): Proving that `P` is true of the
typed result of a computation (the LHS) is equivalent to proving that
for every `x`, we either have `P x` or we can prove that the result of
the computation is not `x` (the RHS).

Armed with identity (1) above, we can also prove the following
identity:

```
(Identity 2)
          WP e P
     <==> WP e (fun x -> True) /\ WLP e P
     <==> WP e (fun x -> True) /\ (forall x. P x \/ WLP e (fun y -> y <> x)) (by (1))
```

That is, in order to prove that a computation `e` has no internal
assertion failures and produces a result satisfying `P`, one can
equivalently prove (a) that `e` has no internal assertion failures
(i.e., `WP e (fun x -> True)`) and then prove (b) that `e`'s result
satisfies `P` (i.e, `WLP e P`).

Finally, we come back to our rule for conditional expressions, which
we can restate as follows (using identities (1) and (2)):

```
WP (if b then e1 else e2) P
=    (b ==> WP e1 (fun x -> True))
  /\ ((not b) ==> WP e2 (fun x -> True))
  /\ (forall y. P y \/ ((b ==> WLP e1 (fun x -> x<>y)
                     /\ (not b) ==> WLP e2 (fun x -> x<>y)))
```

In this form, the post-condition `P` is no longer duplicated across
the branches of the computation and we avoid the exponential blowup
(at the cost of computing both the `WP` and the `WLP`).

WPs and WLPs are convenient for type inference---for loop- and
recursion-free code, we can simply compute the most precise
verification condition for a program without further
annotation. However, programmers are often more comfortable writing
specification in terms of pre- and post-conditions, rather than
predicate transformers. As we will see shortly, computing both the WP
and the WLP allows F\* to move between the two styles without a loss
in precision (computing just the WP alone would not allow this).


## Computation types

Expressions in F\* are given _computation types_ `C`.  In its most
primitive form, each effect in F\* introduces a computation type of
the form `M t_1..t_n t wp wlp`. The `M t_1..t_n` indicates the name of the
effect `M` and several user-defined indexes `t_1..t_n`.  The type `t`
indicates the type of the result of the computation; `wp` is a
predicate transformer that is not weaker than the weakest
pre-condition of `e`; and `wlp` is a predicate transformer not weaker
than the weakest liberal pre-condition of `e`. For the sake of
brevity, we call `wp` the weakest pre-condition and `wlp` the weakest
liberal pre-condition.

For the purposes of the presentation here, we start by discussing a
simplified form of these `C` types, i.e, we will consider computation
types of the form `M t wp wlp`, those that have a non-parameterized
effect constructor `M` (and generalize to the full form towards the
end of this chapter).

A computation `e` has type `M t wp wlp` if when run in an initial
configuration satisfying `wp post`, it (either runs forever, if
allowed, or) produces a result of type `t` in a final configuration
`f` such that `post f` is true, all the while exhibiting no more
effects than are allowed by `M`. In other words, `M` is an upper bound
on the effects of `e`; `t` is its result type; and `wp` is a predicate
transformer that for any post-condition of the final configuration of
`e`, produces a sufficient condition on the initial configuration of
`e`. The `wlp` is similar, except it is liberal with respect to the
internal assertions of `e` in the sense described in the previous
section.

These computation types (specifically their predicate transformers)
form what is known as a [Dijkstra monad]. Some concrete examples
should provide better intuition.

~Remark
We follow an informal syntactic convention that the non-abbreviated
all-caps names for computation types are the primitive forms---other
forms are derived from these.

For example, the computation type constructor `PURE` is a more
primitive version of `Pure`, which we have used earlier in this
tutorial. By the end of this chapter, you will see how `Pure`, `Tot`,
etc. are expressed in terms of `PURE`. Likewise `STATE` is the
primitive form of `ST`; `EXN` is the primitive form of `Exn`; etc.

~

[Dijkstra monad]: http://research.microsoft.com/en-us/um/people/nswamy/papers/paper-pldi13.pdf



## The `PURE` effect

The `PURE` effect is primitive in F\*. No user should ever have to
(re-)define it. Its definition is provided once and for all in
`lib/prims.fst`. It's instructive to look at its definition inasmuch
as it provides some insight into how F\* works under the covers. It
also provides some guidance on how to define new effects. We discuss a
fragment of it here.

The type of pure computations is `PURE t wp wlp` where
`wp, wlp : (t -> Type) -> Type`. This means that pure computations produce
`t`-typed results and are described by predicate transformers that
transform result predicates (post-conditions of kind `t -> Type`) to
pre-conditions, which are propositions of kind `Type`.

Seen another way, the signatures of the `wp` and `wlp` have the
following form---they transform pure post-conditions to pure
pre-conditions.

```
let pure_post (a:Type) = a -> Type
let pure_pre = Type
let pure_wp (a:Type) = pure_post a -> pure_pre
```

For example, one computation type for `0` is
`PURE int return_zero return_zero` where
```
return_zero = fun (p:(int -> Type)) -> p 0
```
This means that in order to
prove any property `p` about the result of the computation `0` is
suffices to prove `p 0`---which is of course what you would
expect. The type `return_zero` is an instance of the more general form
below:

```
type return_PURE (#a:Type) (x:a) = fun (post: pure_post a) -> post x
```

When sequentially composing two pure computations using
`let x = e1 in e2`, if
`e1: PURE t1 wp1 wlp1` and
`e2: (x:t1 -> PURE t2 (wp2 x) (wlp2 x))`,
we type the composed computation as
```
PURE t2 (bind_PURE wp1 wp2) (bind_PURE wlp1 wlp2)
```
where:
```
type bind_PURE (#a:Type) (#b:Type)
               (wp1: pure_wp a)
               (wp2: a -> pure_wp b)
 = fun (post:pure_post b) -> wp1 (fun (x:a) -> wp2 x post)
```

One can show that `return_PURE` and `bind_PURE` together form a monad over
`PureWP`.

### `Pure` and `Tot`

The `Pure` effect is an abbreviation for `PURE` that allows you to
write specifications with pre- and post-conditions instead of
predicate transformers. It is defined as follows:

```
effect Pure (a:Type) (pre:Type) (post: a -> Type) =
       PURE a (fun (p:pure_post a) -> pre /\ forall (x:a). post x ==> p x)
              (fun (p:pure_post a) -> forall (x:a). pre /\ post x ==> p x)
```

That is `Pure a pre post` is a computation which when `pre` is true
produces a result `v:a` such that `post v` is true.

The `Tot` effect is defined below:

```
effect Tot (a:Type) =
       PURE a (fun (p:pure_post a) -> forall (x:a). p x)
              (fun (p:pure_post a) -> forall (x:a). p x)
```

This means that a computation type `Tot a` only reveals that the
computation always terminates with an `a`-typed result.

### From predicate transformers to pre- and post-conditions

We have just seen how the definition of `Pure a pre post` unfolds into
the `PURE` effect. It is also possible to go in the other direction,
turning a specification written in the `PURE a wp wlp` style into a
`Pure a pre post`. However, whereas F\* will always automatically
unfold the definition of `Pure` in terms of `PURE` as needed, it will
not automatically transform the more primitive `PURE` effect into the
derived form `Pure`. Instead, one uses an explicit coercion for this
purpose, with the signature shown below.

```
type as_requires (#a:Type) (wp:pure_wp a)  = wp (fun x -> True)
type as_ensures  (#a:Type) (wlp:pure_wp a) (x:a) = ~ (wlp (fun y -> ~(y==x)))
assume val as_Pure: #a:Type -> #b:(a -> Type)
          -> #wp:(x:a -> pure_wp (b x))
          -> #wlp:(x:a -> pure_wp (b x))
          -> $f:(x:a -> PURE (b x) (wp x) (wlp x))
          -> x:a -> Pure (b x) (as_requires (wp x))
                               (as_ensures (wlp x))
```

~Remark

 The second-to-last argument to `as_Pure` is named `f` and is tagged with
 an equality constraint, written by preceding the bound variable name
 with the equality symbol, i.e., `#f`. Given an application
 `as_Pure g`, the equality constraint on the `f` argument of `as_Pure`
 instructs F\*'s type inference engine to unify the type of `g` with
 the expected type of `f`. Without this constraint, by default, F\*
 will try to prove that the type of `g` is a subtype of the expected
 type of `f`. In cases like this, where an argument mentions several
 implicit arguments that need to be inferred (`a, b, wp, wlp`, in this
 case), the equality constraint produces better inference results.

~

One way to understand this type is as follows: `as_Pure f` coerces the
type of `f` which has a `PURE (b x) wp wlp` effect to an equivalent
function type written in terms of `Pure`.  The pre-condition of the
resulting function type is easily computed from the `wp`, using
`as_requires wp`, which is just the weakest pre-condition of the
trivial post-condition.

Computing the post-condition is a bit more subtle, and is accomplished
with the `as_ensures wlp` function. Perhaps the easiest way to
understand it is by studying the roundtrip transformation of `PURE` to
`Pure` and back to `PURE`. The key step below is in the transformation
from (b) to (c), where we see that by ensuring
`~wlp (fun y -> y <> x)`, we make use precisely of Identity 1
from the previous section. Given only a `wp` without a `wlp`, we would be
stuck not being able to construct `as_ensures`.

```
(a) Pure t (as_requires wp)
           (as_ensures wlp)

= (definition)

(b) PURE t (fun post -> wp (fun x -> True) /\ forall y. ~(wlp (fun y' -> y<>y')) ==> post y)
           (fun post -> forall y. wp (fun x -> True) /\  ~(wlp (fun y’ -> y<>y')) ==> post y)

= (unfolding ==> and double negation)

(c) PURE t (fun post -> wp (fun x -> True) /\ forall y.  wlp (fun y' -> y<>y’) \/ post y)
           (fun post -> forall y. not (wp (fun x -> True)) \/ wlp (fun y’ -> y<>y’) \/ post y)

= (folding ==>)

(d) PURE t (fun post -> wp (fun x -> True) /\ forall y.  wlp (fun y’ -> y<>y’) \/ post y)
           (fun post -> forall y. wp (fun x -> True) ==> (wlp (fun y’ -> y<>y’) \/ post y))

= (rearranging quantifier)

(e) PURE t (fun post -> wp (fun x -> True) /\ forall y.  wlp (fun y’ -> y<>y’) \/ post y)
           (fun post -> wp (fun x -> True) ==> forall y. (wlp (fun y’ -> y<>y’) \/ post y))

= (identity 1)

(f) PURE t (fun post -> wp (fun x -> True) /\ wlp (fun y’ -> post y’))
           (fun post -> wp (fun x -> True) ==> wlp (fun y’ -> post y’))

<: (strengthening wlp, eta reduction)

(g) PURE t (fun post -> wp (fun x -> True) /\ wlp post)
            wlp

= (identity 2, eta)

(h) PURE t wp wlp
```

Here are a few small examples using this coercion to move from `PURE`
to `Pure`.

```
val f : x:int -> PURE int (fun 'p -> x > 0 /\ 'p (x + 1)) (fun 'p -> x > 0 ==> 'p (x + 1))
let f x = assert (x > 0); x + 1

val h : #req:(int -> Type)
     -> #ens:(int -> int -> Type)
     -> $f:(x:int -> Pure int (req x) (ens x))
     -> y:int -> Pure int (req y) (ens y)
let h f x = f x

val g : x:int -> Pure int (x > 0) (fun y -> y == x + 1)
let g = h (as_Pure f)
```

~Exercise

Work out the roundtrip unfolding a `Pure t pre post` into a `PURE`
effect and then back to a `Pure` using the `as_Pure` coercion. Prove
that after this round trip, the resulting type is logically equivalent
to the type you started with.

~~ Answer
```
Pure t p q

= (definition)

PURE t wp wlp
  where wp  = fun post -> p /\ forall (y:t). q y ==> post y
    and wlp = fun post -> forall (y:t). p /\ q y ==> post y

<:  (definition)

  Pure t (p /\ forall (y:t). q y ==> True)
         (fun y -> not (forall (z:t). p /\ q z ==> (y<>z)))

=  (simplification/de Morgan)

Pure t (p)
       (fun y -> exists (z:t). p /\ q z /\ y=z)

= (replacing existential by equality)

   Pure t p
          (fun y -> p /\ q y)

<: (weakening post-condition, eta)

   Pure t p q
```
~~
~


## The `STATE` effect

Stateful programs operate on an input heap producing a result and an
output heap. The computation type `STATE t wp wlp` describes such a
computation, where `wp, wlp : state_wp t` has the signature below.

```
let state_post t = t -> heap -> Type
let state_pre    = heap -> Type
let state_wp t   = state_post t -> state_pre
```

In other words, WPs and WLPs for stateful programs transform stateful
post-conditions (relating the t-typed result of a computation to the
final heap) into a pre-condition, a predicate on the input heap.

The type `heap` is axiomatized in `lib/heap.fst` and is one model of
the primitive heap provided by the F\* runtime system. Specifically,
we have functions to select, update, and test the presence of a
reference in a heap.

```
module Heap
assume type heap
assume val sel : #a:Type -> heap -> ref a -> Tot a
assume val upd : #a:Type -> heap -> ref a -> a -> Tot heap
assume val contains : #a:Type -> heap -> ref a -> a -> Tot bool
```

These functions are interpreted using standard axioms, which allows
the SMT solver to reason about heaps, for instance:

```
assume SelUpd1:       forall (a:Type) (h:heap) (r:ref a) (v:a).
                      {:pattern (sel (upd h r v) r)}
                      sel (upd h r v) r = v

assume SelUpd2:       forall (a:Type) (b:Type) (h:heap) (k1:ref a) (k2:ref b) (v:b).
                      {:pattern (sel (upd h k2 v) k1)}
                      k2=!=k1 ==> sel (upd h k2 v) k1 = sel h k1
```

The `Heap` library axiomatizes several other functions---the
interested reader refer to `lib/heap.fst` for more details.

~Remark
Note, although `sel` is marked as a total function, the axioms
underspecify its behavior, e.g., `sel h r` has no further
interpretation unless `h` can be proven to be an `upd`.

Also note the use of the `{:pattern ...}` form in the axioms above.
This provides the SMT solver with a trigger for the quantifiers, in
effect orienting the equalities in `SelUpd1` and `SelUpd2` as
left-to-right rewritings.
~

The functions `sel` and `upd` provide a logical theory of heaps. At
the programmatic level, we require stateful operations to allocate,
read and write references in the current heap of the program. The
signatures of these stateful primitives are provided in `lib/st.fst`,
as illustrated (in slightly simplified form) below.

Let's start with the signature of `read`:

```
let wp_read r = fun (post:state_post a) (h0:heap) ->  post (sel h0 r) h0

assume val read: #a:Type -> r:ref a -> STATE a (wp_read r) (wp_read r)
```

This says that `read r` returns a result `v:a` when `r:ref a`;
and, to prove for any post-condition `post:state_post a` relating the
`v` to the resulting heap, it suffices to prove `post (sel h0 r) h0`
when `read r` is run in the initial heap `h0`.

Next, here's the signature of `write`:

```
let wp_write r = fun (post:state_post a) (h0:heap) -> post () (upd h0 r v)

assume val write: #a:Type -> r:ref a -> v:a -> STATE unit (wp_write r) (wp_write r)
```

This says that `write r v` returns a a `unit`-typed result when
`r:ref a` and `v:a`; and, to prove any post-condition `post:state_post a`
relating the `()` to the resulting heap, it suffices to prove
`post () (upd h0 r v)` when `write r v` is run in the initial heap `h0`.

~Remark
F\* provides support for user-defined custom operators. To allow you to
write `!r` for `read r` and `r := v` instead of `ST.write r v`, we define
```
let op_Bang x = ST.read x
```
and
```
let op_Colon_Equals x v = ST.write x v
```
~

As with the `pure_wp`, the predicate transformers `state_wp t` form a
monad, as shown by the combinators below.

```
type return_STATE (#a:Type) (v:a) = fun (post:state_post a) (h0:heap) -> post v h0

type bind_STATE (#a:Type) (wp1:state_wp a) (wp2:a -> state_wp b)
     = fun (post:state_post b) (h0:heap) -> wp1 (fun (x:a) -> wp2 x post) h0
```

### The `ST` effect

`ST` is to `STATE` what `Pure` is to `PURE`: it provides a way to
write stateful specifications using pre- and post-conditions instead
of predicate transformers. It is defined as shown below (in
`lib/st.fst`):

```
effect ST (a:Type) (pre:state_pre) (post: (heap -> state_post a)) = STATE a
  (fun (p:state_post a) (h:heap) ->
     pre h /\ (forall a h1. (pre h /\ post h a h1) ==> p a h1)) (* WP *)
  (fun (p:state_post a) (h:heap) ->
     (forall a h1. (pre h /\ post h a h1) ==> p a h1))          (* WLP *)
```

As before, we can also define a coercion to move to `ST` from `STATE`.


```
type as_requires (#a:Type) (wp:STWP_h heap a)  = wp (fun x h -> True)
type as_ensures  (#a:Type) (wlp:STWP_h heap a) (h0:heap) (x:a) (h1:heap)
  = ~ (wlp (fun y h1' -> y<>x \/ h1<>h1') h0)
val as_ST: #a:Type -> #b:(a -> Type)
          -> #wp:(x:a -> STWP_h heap (b x))
          -> #wlp:(x:a -> STWP_h heap (b x))
          -> $f:(x:a -> STATE (b x) (wp x) (wlp x))
          -> x:a -> ST (b x) (as_requires (wp x))
                             (as_ensures (wlp x))
```

And here's a small program that uses this coercion.

```
let f x = !x * !x

val h : #req:(ref int -> heap -> Type)
     -> #ens:(ref int -> heap -> int -> heap -> Type)
     -> $f:(x:ref int -> ST int (req x) (ens x))
     -> y:ref int -> ST int (req y) (ens y)
let h f x = f x

val g : x:ref int -> ST int (fun h -> True) (fun h0 y h1 -> h0=h1 /\ y >= 0)
let g = h (as_ST f)
```

### The `St` effect

We define another abbreviation on top of `ST` for stateful programs with
trivial pre- and post-conditions.

```
  effect St (a:Type) = ST a (fun h -> True) (fun _ _ _ -> True)
```

## Lifting effects

When programming with multiple effects, you can instruct F\* to
automatically infer a specification for your program with a predicate
transformer that captures the semantics of all the effects you
use. The way this works is that F\* computes the least effect for each
sub-term of your program, and then lifts the specification of each
sub-term into some larger effect, as may be needed by the context.

For example, say you write:

```
let y = !x in y + 1
```

The sub-term `!x` has the `STATE` effect; whereas the sub-term `y+1`
is `PURE`. Since the whole term has at least `STATE` effect, we'd like
to lift the pure sub-computation to `STATE`. To do this, you can
specify that `PURE` is a sub-effect of `STATE`, as follows (adapted
from `lib/prims.fst`):

```
sub_effect
  PURE   ~> STATE = fun (a:Type) (wp:pure_wp a) (p:state_post a) (h:heap) ->
                    wp (fun a -> p a h)
```

This says that in order to lift a `PURE` computation to `STATE`, we
lift its `wp:pure_wp a` (equivalently for its `wlp`) to a `state_wp a`
that says that that pure computation leaves the state unmodified.

Note that the type of `PURE ~> STATE` is
`(a:Type) -> pure_wp a -> state_post a -> heap -> Type` which corresponds
to `(a:Type) -> pure_wp a -> state_wp a` since:
```
let state_wp a   = state_post a -> state_pre
let state_pre    = heap -> Type
```

## Indexed computation types

Finally, as mentioned earlier, in its full form a computation type has
the shape `M t_1..t_n t wp wlp`, where `t_1..t_n` are some user-chosen
indices to the effect constructor `M`.

This is convenient in that it allows effects to be defined
parametrically, and specific effects derived just by instantiation.

For example, the `STATE` effect is actually defined as an instance of
a more general parameterized effect `STATE_h (mem:Type)`, which is
parametric in the type of the memory used by the state monad.

Specifically, we have in `lib/prims.fst`

```
new_effect STATE = STATE_h heap
```

However, as we will see in subsequent sections, it is often convenient
to define other variants of the state monad using different memory
structures.

# Verifying Stateful Programs

In the previous chapter, we saw how F\*'s `ST` monad was specified. We
now look at several programs illustrating its use. If you skipped the
previous chapter, you should be ok---we'll explain what's needed about
the `ST` monad as we go along, although, eventually, you'll probably
want to refer to the previous chapter for more details.

## Stateful access control { #sec-stateful-access-control }

We'll start with a simple example, based on the access control example
from [#sec-access-control]. A restriction of that example was that the
access control policy was fixed by the two functions `canWrite` and
`canRead`. What if we want to administer access rights using an access
control list (ACL)? Here's one simple way to model it---of course, a
full implementation of stateful access control would have to pay
attention to many more details.

The main idea is to maintain a reference that holds the current access
control list. In order to access a resource, we need to prove that the
current state contains the appropriate permission.

Here's how it goes:

```
[INCLUDE=code/exercises/Ex10a.fst]
```

~Exercise
Write down some types for `grant` and `revoke` that are sufficiently
precise to allow the program to continue to type-check.

~~ ExerciseFragment {exname=Ex10a}
val grant : e:entry -> ST unit (requires (fun h -> True))
                               (ensures (fun h x h' -> True))
let grant e = ST.write acls (e :: ST.read acls)

val revoke: e:entry -> ST unit (requires (fun h -> True))
                               (ensures (fun h x h' -> True))
let revoke e =
  let db = List.filterT (fun e' -> e<>e') (ST.read acls) in
  ST.write acls db
~~

~~Answer
~~~ SolutionFragment {exname=Ex10a}
val grant : e:entry -> ST unit (requires (fun h -> True))
                               (ensures (fun h x h' -> sel h' acls = e :: sel h acls))
let grant e = ST.write acls (e :: ST.read acls)

val revoke: e:entry -> ST unit (requires (fun h -> True))
                               (ensures (fun h x h' -> not(List.mem e (sel h' acls))))
let revoke e =
  let db = List.filter (fun e' -> e<>e') (ST.read acls) in
  ST.write acls db
~~~
~~
~

~KK
The notation to reason about heaps was introduced in the previous section, which
was marked as "optional" on the first read. A primer on how it works on a
more superficial level (without the theoretical background) at the start of this
section might be helpful.
~

## Reasoning about (anti-)aliased heap structures

A central problem in reasoning about heap-manipulating programs has to do
with reasoning about updates to references in the presence of aliasing. In general,
any two references (with compatible types) can alias each other (i.e, they may refer
to the same piece of memory), so updating one can also change the contents of the other.
In this section, we'll look at two small but representative samples that illustrate
how this form of reasoning can be done in F\*.

### Dynamic frames

For our first example, we define the type of a 2-dimensional
mutable point. The refinement on `y` ensures that the references
used to store each coordinate are distinct.

```
type point =
  | Point : x:ref int -> y:ref int{y<>x} -> point
```

Allocating a new point is straightforward:

```
[INCLUDE=code/solutions/Ex10b.fst:NewPoint]
```

As is moving a point by a unit along the `x`-axis.
```
let shift_x p = Point?.x p := !(Point?.x p) + 1
```

Now, things get a bit more interesting. Let's say we have
a pair of points, `p1` and `p2`, we want to shift `p1`, and
reason that `p2` is unchanged. For example, we'd like
to identify some conditions that are sufficient to prove that
the assertion in the program below always succeeds.

```
[INCLUDE=code/solutions/Ex10b.fst:ShiftXP1]
```

If you give this program (reproduced in its entirety below) to F\*, it
will report no errors. This may surprise you: after all, if you call
`shift_x_p1 p p` the assertion will indeed fail. What's going on here is that
for functions in the `ST` effect, F\* infers a most precise type for it,
and if the programmer did not write down any other specification, this
precise inferred type is what F\* will use. This means that without an
annotation on `shift_x_p1`, F\* does not check that the assertion will succeed
on every invocation of `shift_x_p1`; instead, using the inferred type it will
aim to prove that the assertion will succeed whenever the function is
called. Let's try:

```
[INCLUDE=code/solutions/Ex10b.fst:Test0]
```

Here, we wrote a specification for `test0`, asking it to have trivial
pre-condition. When we ask F\* to check this, it complains at the call to
`shift_x_p1 x x`, saying that the assertion failed.

If we try calling `shift_x_p1` with two distinct points, as below, then we
can prove that the assertion succeeds.

```
[INCLUDE=code/solutions/Ex10b.fst:Test1]
```

In simple code like our example above, it is a reasonable trade-off to let F\* infer
the most precise type of a function, and to have it check later, at each call site, that
those calls are safe.

As you write larger programs, it's a good idea to write down the types
you expect, at least for tricky top-level functions. To see what these
types look like for stateful programs, let's decorate each of the
top-level functions in our example with the types---for these tiny
functions, the types can often be larger than the code itself; but this
is just an exercise. Typically, one wouldn't write down the types of such
simple functions.

### A type for `shift_x` and the `Heap.modifies` predicate

Let's start with `shift_x`, since it is the simplest:

Here's one very precise type for it:

```
val shift_x : p:point -> ST unit
  (requires (fun h -> True))
  (ensures (fun h0 _ h1 -> h1=upd h0 (Point?.x p) (sel h0 (Point?.x p) + 1)))
```

Another, less precise but more idiomatic type for it is:

```
[INCLUDE=code/solutions/Ex10b.fst:ShiftXType]
```

Informally, `ensures`-clause above says that the heap after calling this
function (`h1`) differs from the initial heap `h0` only at `Point?.x` (and
possibly some newly allocated locations). Let's look at it in more detail.

The predicate `modifies` is defined in the module `Heap`
(`lib/heap.fst`). First, let's look at its signature, which says that it
relates a `set aref` to a pair of heaps. The type `aref` is the type of a
reference, but with the type of referent abstracted. As such, the first
argument to `modifies` is a set of heterogenously typed references.

```
  type aref =
    | Ref : #a:Type -> ref a -> aref
  let only x = Set.singleton (Ref x)
  type modifies : set aref -> heap -> heap -> Type
```

The definition of `modifies` follows:

```
type modifies mods h0 h1 =
  Heap.equal h1 (Heap.concat h1 (Heap.restrict (Set.complement mods) h0))

```
**Heap.equal**: `Heap.equal` is an equivalence relation on `heap` capturing the notion of
_extensional equality_; i.e., two heaps are extensionally equal if they map each
reference to the same value.

**Heap.concat**: Each reference `r` in the heap `concat h h'` contains a value that is the
same as in `h`, except if the `r` is in `h'`, in which case it's value is
whatever value `h'` contains at `r`. More formally:

```
   sel r (concat h h')
   = if contains h' r then sel h' r else sel h r
```

**Heap.restrict** The function `restrict s h` provides a way to shrink
the domain of the heap `h` to be at most `s`. Specifically:

```
  contains (restrict s h) r = Set.mem (Ref r) s && contains h r
```

Altogether, the definition of `modifies` says that for every reference `r`,
if `r` is not in the set `mods` and `r` if the heap `h0` contains `r`,
then `sel h1 r = sel h0 r`, i.e., it hasn't changed. Otherwise,
`sel h1 r = sel h1 r`---meaning we know nothing else about it.

~Remark
We define the following infix operators to build sets of heterogenous references.

```
let op_Hat_Plus_Plus (#a:Type) (r:ref a) (s:set aref) = // ^++
    Set.union (Set.singleton (Ref r)) s

let op_Plus_Plus_Hat (#a:Type) (s:set aref) (r:ref a) = // ++^
    Set.union s (Set.singleton (Ref r))

let op_Hat_Plus_Hat (#a:Type) (#b:Type) (r1:ref a) (r2:ref b) = // ^+^
    Set.union (Set.singleton (Ref r1)) (Set.singleton (Ref r2))
```
~

### A type for `new_point` and freshness of references

Here's a precise type for `new_point`, with its definition reproduced
for convenience.

```
[INCLUDE=code/solutions/Ex10b.fst:NewPointType]

[INCLUDE=code/solutions/Ex10b.fst:NewPoint]
```

The `modifies` clause says the function modifies no existing
reference---that's easy to see, it only allocates new references.

In the body of the definition, we build `Point x y` and the type of
`Point` requires us to prove that `x <> y`. The only way to prove that is
if we know that `ref` returns a distinct reference each time, i.e., we
need freshness.

The type of `ST.alloc`
gives us the freshness we need to prove that `x <> y`. The type
below says that the returned reference does not exists in the initial heap
and does exist in the final heap (initialized appropriately).

```
val alloc:  #a:Type -> init:a -> ST (ref a)
    (requires (fun h -> True))
    (ensures (fun h0 r h1 -> fresh (only r) h0 h1 /\ h1==upd h0 r init))

type fresh (refs:set aref) (h0:heap) (h1:heap) =
  (forall (a:Type) (r:ref a). mem (Ref r) refs
                         ==> not(contains h0 r) /\ contains h1 r)
```


Coming back to the type of `new_point`: By stating that the new heap
contains both `Point?.x` and `Point?.y`, we provide the caller with enough
information to prove that any references that it might allocate
subsequently will be different from the references in the newly allocated
point. For example, the assertion in the code below succeeds, only
because `new_point` guarantees that the heap prior to the allocation of
`z` contains `Point?.x p`; and `ST.alloc` guarantees that `z` is different
from any reference that exists in the prior heap. Carrying
`Heap.contains` predicates in specifications is important for this
reason.


```
[INCLUDE=code/solutions/Ex10b.fst:Test2]
```

On the other hand, since the `ST` library we are working with provides no
ability to deallocate a reference (other versions of it do; see, for
example, `lib/stperm.fst`), we know that if we have a value `v:ref t`,
that the current heap must contain it. Capturing this invariant, the `ST`
library also provides the following primitive (whose implementation is a
noop).

```
val recall: #a:Type -> r:ref a -> ST unit
  (requires (fun h -> True))
  (ensures (fun h0 _ h1 -> h0=h1 /\ Heap.contains h1 r))
```

Using this, we can write the following code.

```
val f : ref int -> St unit
let f (x:ref int) =
  recall x;         //gives us that the initial heap contains x, for free
  let y = ST.alloc 0 in
  assert (y <> x)
```

As a matter of style, we prefer to keep the use of `recall` to a minimum,
and instead carry `Heap.contains` predicates in our specifications, as
much as possible.

### A type for `shift_x_p1`

Now, we have all the machinery we need to give `shift_x_p1` a sufficiently
strong type to prove that its assertion will never fail (if its pre-condition is met).


```
[INCLUDE=code/exercises/Ex10b.fst:ShiftXP1Spec]
```

In order to prove that the assignment to `Point?.x p1` did not change
`p2`, we need to know that `Point?.x p1` does not alias either component
of `p2`. In general, if `p1` can change arbitrarily, we also need the
other two conjuncts, i.e., we will need that the references of `p1` and `p2`
are pairwise distinct.

~Remark
A note on style: we could have placed the refinement on `p2` within the
requires clause. However, since the refinement is independent of the
state, we prefer to write it in a manner that makes its state independence
explicit.
~

~Exercise
Change `shift_x` to the function `shift` below that moves both the `x` and `y` components
of its argument by some amount. Prove that calling this function in a
function like `shift_p1` does not change `p2`.

~~ ExerciseFragment {exname=Ex10b}
let shift p = Point?.x p := ...; Point?.y p := ...
~~

~~Answer
~~~ SolutionFragment {exname=Ex10b}
[INCLUDE=code/solutions/Ex10b.fst:ShiftP1Solution]
~~~
~~
~


# Lightweight framing with hyper-heaps

The style of the previous section is quite general. But, as programs scale up,
specifying and reasoning about anti-aliasing can get to be quite tedious. In this chapter,
we look at an alternative way of proving stateful programs correct by making use of
a more structured representation of memory that we call a "HyperHeap". First, we illustrate
the problem of scaling up the approach of the previous chapter to even slightly larger examples


## Flying robots

![johnnysokko]

Let's have some fun with flying robots. Maybe you've seen this classic Japanese
TV series from the 60s. <https://en.wikipedia.org/wiki/Giant_Robo_(tokusatsu)>. Let's build
a tiny model of this flying robot.

Our `bot` has a 3d-point representing its position; and two arms, both in polar coordinates.

```
  type bot = {
    pos:point;
    left:arm;
    right:arm
  }
  and point = {
    x:ref int;
    y:ref int;
    z:ref int;
  }
  and arm = {
    polar:ref int;
    azim:ref int
  }
```

Our bot has an invariant: whenever it is flying, it has its arms up.

![flying]

We can write a function to make our robot fly, which sets its arms up and then
shoots the robot into the sky.

```
  let fly b =
    b.left.polar := 0;
    b.right.polar := 0;
    b.z := 100
```

Now, we'd like to prove properties like the following:

```
  //initially, in h0: b1 not flying, b2 not flying
      fly b1
  //in h1: b1 flying, b2 not flying
```

To prove this property, and to prove that the bot's invariant is
maintained, we need many anti-aliasing invariants, including the
following (stated informally).

```
forall r1, r2. r1 in refs_in b1
              /\ r2 in refs_in b2 ==> r1 <> r2
refs in b.left.arm `disjoint` refs in b.pos
refs in b.right.arm `disjoint` refs in b.pos
refs in b.left.arm `disjoint` refs in b.right.arm
```

As you can see, what started out as a bit of fun programming is now a
painful exercise in managing anti-aliasing invariants. Worse, even if we
state all these invariants, proving that they are preserved involves
reasoning about a quadratic number of inequalities between all the
references in each robot, as well as a further quadratic inequalities
among the references of each bot.

F\*'s effect mechanism provides an alternative to this tedium. Instead of
working with the primitive `ST` effect (and its flat model of memory via
the `heap` type) that we have been working with so far, we can
easily define a different, more structured view of memory as a new effect.
One such alternative is the HyperHeap, a model of memory which provides a
convenient built-in notion of disjoint sets of references.

[johnnysokko]: johnnysokko.jpg "Johnny Sokko" { width=300px }
[flying]: flying.gif "Flying Bot" { width=200px }
[heap]: heap.jpg "A flat heap" {width=400px}
[hyperheap]: hyperheap.jpg "A hyper-heap" {width=500px}

## Hyper-heaps

So far, we have been working with the `heap` type, a flat, typed-map from
references to their contents. The `ref` type is abstract---it has no structure
and only supports equality operations. Conceptually, it looks like this:

![heap]

A hyper-heap provides organizes the heap into many disjoint fragments, or
regions. Each region is collectively addressed by a _region identifier_,
and these identifiers are organized in a tree-shaped hierarchy. Each
region is itself a map from typed references to values, just like our
heaps from before. The picture below depicts the structure.

![hyperheap]

At the top-level we have regions `R0 ... Rn`, each associated with a
fragment of heap references. Region identifiers `R0.0 ... R0.m` are
within the hierarchy rooted at `R0`; each has a set of references distinct from
all other regions (including its parent `R0`); and so on.

The `HyperHeap` module in F\*'s standard library defines this structure,
a type `rid` for region ids, and a type `rref (r:rid) (a:Type)`, which is
the type of a reference in region `r` that points to an `a`-typed value.
We prove that values of type `rref r a` are in an injection with values
of type `ref a`---meaning that hyper-heaps are in 1-1 correspondence with
the underlying heap, except, as we will see, the additional structure
provided helps with stating and proving memory invariants.

The module `HyperHeap` defines:

```
type t = Map.t rid heap
new_effect STATE = STATE_h t
effect ST (a:Type) (pre:t -> Type) (post:t -> a -> t -> Type) =
       STATE a (fun 'p m0 -> pre m0 /\ (forall x m1. post m0 x m1 ==> 'p x m1))
               (fun 'p m0 ->  (forall x m1. pre m0 /\ post m0 x m1 ==> 'p x m1))
```

That is `HyperHeap.STATE` and `HyperHeap.ST` are just like the `STATE`
and `ST` we've been working with, except that instead of the type of
memory being a flat `heap`, the memory has type `HyperHeap.t`, a map from
region ids `rid` to `heap`s.

### The type of region identifiers, `rid`

[abstract]: https://github.com/FStarLang/FStar/wiki/Qualifiers-for-definitions-and-declarations#abstract

The type `rid` is [abstract] to clients, with a few operations on it
revealing its hierarchical structure. Specifically, we have three functions:

```
type rid
val root : rid
val includes : rid -> rid -> Tot bool
val extends  : rid -> rid -> Tot bool
let disjoint i j = not (includes i j) && not (includes j i)
```

One way to think of `rid` is as a `list int`, where `includes i j` if and
only if `i` is a prefix of `j`; and `extends i j` is true if `i = _ :: j`.
However, while this may provide you with some intuition, formally, the
only properties that these functions are known to satisfy are revealed by
the following lemmas:

```
val lemma_includes_refl: i:rid
-> Lemma (requires (True))
         (ensures (includes i i))
         [SMTPat (includes i i)]

val lemma_disjoint_includes: i:rid -> j:rid -> k:rid
-> Lemma (requires (disjoint i j /\ includes j k))
         (ensures (disjoint i k))
         [SMTPat (disjoint i j);
          SMTPat (includes j k)]

val lemma_extends_includes: i:rid -> j:rid
-> Lemma (requires (extends j i))
         (ensures (includes i j))
         [SMTPat (extends j i)]

val lemma_extends_disjoint: i:rid -> j:rid -> k:rid
-> Lemma (requires (extends j i /\ extends k i /\ j<>k))
         (ensures (disjoint j k))
         [SMTPat (extends j i);
          SMTPat (extends k i)]
```

### The type of region-indexed references: `rref`

Next, we have a type `rref` for region-indexed
references, as a function `as_ref` that coerces an `rref` to its
underlying heap reference.

~ Remark
The functions `as_ref` and `ref_as_rref` have effect `GTot`, indicating
that they are **ghostly** total functions. A subsequent chapter will
describe ghost effects in more detail. For now, it suffices to know that
computations with effect `GTot` can only be used in specifications.
Concrete executable programs cannot use `GTot`. In this case, the `GTot`
effect is used to ensure that client programs of `HyperHeap` cannot break
the abstraction of the `rref` type, except in their specifications.

~

```
  type rref : rid -> Type -> Type
  val as_ref      : #a:Type -> #id:rid -> r:rref id a -> GTot (ref a)
  val ref_as_rref : #a:Type -> i:rid -> r:ref a -> GTot (rref i a)
  val lemma_as_ref_inj: #a:Type -> #i:rid -> r:rref i a
      -> Lemma (requires (True))
               (ensures ((ref_as_rref i (as_ref r) = r)))
         [SMTPat (as_ref r)]
```

The functions `Heap.sel` and `Heap.upd` are adapted to work with
hyper-heaps instead; in effect, we select first the region `i`
corresponding to `rref i a` and then perform the select/update within the
heap within that region. An important point to note is that although the
`rid` type is structured hierarchically, the map of all regions is itself
flat. Thus, selecting the region corresponding to `i` is constant time
operation (rather than requiring a tree traversal). As we will see below,
this is crucial for good automation in proofs.

```
let sel (#a:Type) (#i:rid) (m:t) (r:rref i a) = Heap.sel (Map.sel m i) (as_ref r)
let upd (#a:Type) (#i:rid) (m:t) (r:rref i a) (v:a)
  = Map.upd m i (Heap.upd (Map.sel m i) (as_ref r) v)
```
### Stateful operations on regions and references

**Creating a new region** `HyperHeap` also provides four main stateful functions. The
first of these, `new_region r0`, allocates a new, empty region rooted at `r0`

```
val new_region: r0:rid -> ST rid
      (requires (fun m -> True))
      (ensures (fun (m0:t) (r1:rid) (m1:t) ->
                           extends r1 r0
                        /\ fresh_region r1 m0 m1
                        /\ m1=Map.upd m0 r1 Heap.emp))
```

**Allocating a reference in a region** The function `ralloc r v` allocates a new reference
 in region `r` and initializes its contents to `v`.

```
val ralloc: #a:Type -> i:rid -> init:a -> ST (rref i a)
    (requires (fun m -> True))
    (ensures (fun m0 x m1 ->
                    Let (Map.sel m0 i) (fun region_i ->
                    not (Heap.contains region_i (as_ref x))
                    /\ m1=Map.upd m0 i (Heap.upd region_i (as_ref x) init))))
```

**Reading and writing references** These next two operations are written `!r`
and `r := v`, for dereference and assignment of `rref`'s, respectively.

```
val op_Bang: #a:Type -> #i:rid -> r:rref i a -> ST a
  (requires (fun m -> True))
  (ensures (fun m0 x m1 -> m1=m0 /\ x=Heap.sel (Map.sel m0 i) (as_ref r)))

val op_Colon_Equals: #a:Type -> #i:rid -> r:rref i a -> v:a -> ST unit
  (requires (fun m -> True))
  (ensures (fun m0 _u m1 -> m1=Map.upd m0 i (Heap.upd (Map.sel m0 i) (as_ref r) v)))
```

### Framing conditions for hyper-heaps: `modifies` revised

Finally, we redefine `modifies` clauses to work with hyper-heaps instead
of heaps. Crucially, given region ids `r1, ..., rn`, we intend to write
`modifies (r1 ^++ ... ++^ rn) h0 h1` to mean that `h1` may differ from
`h0` in all regions _rooted at some region_ in the set `{r1, ..., rn}`
(and, as before, any other regions that are newly allocated in `h1`). As
we will see, generalizing modifies clauses to work with hierarchical
region names is a crucial feature, both for compactness of specifications
and for better automation.


```
type modifies (s:Set.set rid) (m0:t) (m1:t) =
  Map.equal m1 (Map.concat m1 (Map.restrict (Set.complement (mod_set s)) m0))

where
val mod_set : Set.set rid -> GTot (Set.set rid)
and forall (x:rid) (s:Set.set rid).
           Set.mem x (mod_set s) <==> (exists (y:rid). Set.mem y s /\ includes y x)
```

~Remark
The interpretation of `Map.restrict, Map.concat` and `Map.equal` is
analogous to the counterparts for `Heap` discussed in the previous
section.

The definition of `mod_set s` is non-constructive, since deciding
`Set.mem x (mod_set s)` requires finding a witness in `s`, and element
`y` of `s`, such that `includes y x`. As such, we mark it as a ghost
function with the `GTot` effect.
~

## Robot, fly!

We now look at the use of hyper-heaps to solve our problem of
programming our robots.

The main idea is to define our data structures in a way that distinct
components of the robot are allocated in distinct regions, thereby
maintaining an invariant that the components do not interfere. For
example, by allocating the left arm and the right arm of the robot in
distinct regions, we know that the arms do not alias each other---moving
one arm does not disturb the other.

Here's how it goes:

Each type carries an extra implicit parameter, recording the region in
which its mutable references reside. Within a region, we get no free
anti-aliasing properties---so, for those references, we write explicit
inequalities that say, for example, that the `x, y` and `z` fields of a
point are not aliased.

For the `bot` itself, we say that its components are allocated in
disjoint sub-regions of `r`, the region of the bot. Note that the
hierarchical region names allow us to state this conveniently. With just
a flat namespace of regions, we would had to have explicitly specified
the set of regions that are transitively allocated within a bot (possibly
breaking the abstraction of sub-objects, which would have to reveal how
many regions they used.)

```
module Robot
open FStar.Heap
open FStar.HyperHeap
open FStar.Set

[INCLUDE=code/solutions/Ex11a.fst:Types]
```

Next, we define our robot invariant: whenever it is flying, its arms are
up. Additionally, we state that all the regions of the bot are included
in the heap. This is a matter of taste: alternatively, we could have
omitted the `contains` predicates in our invariant, and resorted to
`ST.recall` (its analog for hyper-heaps), to prove that a region exists
whenever we need that property.

```
[INCLUDE=code/solutions/Ex11a.fst:Invariant]
```

Now, we come to the code that builds new points, arms, and robots. In
each case, the caller specifies `r0`, the region within which the object
is to be allocated. We create a new sub-region of `r0`, allocate the
object within, prove that the region is fresh (meaning the initial
heap does not contain that region, while the final heap does), and that
the object is initialized as expected. In the case of `new_robot`, we also
prove that the the returned `bot` satisfies the robot invariant.


```
[INCLUDE=code/solutions/Ex11a.fst:Build]
```

Next, we write our function `fly`, which sets the robots arms up, and
shoots it up to fly. As an illustration, we show that we can also modify
some other reference arbitrarily, and the anti-aliasing properties of our
invariant suffice to prove that this assignment does not mess with the
robot invariant.

```
[INCLUDE=code/solutions/Ex11a.fst:Fly]
```

Finally, if we're given two robots, `b0` and `b1`, we know that they are
allocated in disjoint regions, and satisfy the robot invariant, then we
can fly one, not impact the invariant of the other; then fly the other,
and have both of them flying at the end.

```
[INCLUDE=code/solutions/Ex11a.fst:FlyBoth]
```


~Exercise
Extend this example by adding a function which takes a list of robots,
each in a region disjoint from all others, and fly all of them.

~~ ExerciseFragment {exname=Ex11a}
val fly_robot_army: ...
~~

~~Answer
~~~ SolutionFragment {exname=Ex11a}
val fly_robot_army:  #rs:Set.set rid
        -> bs:bots rs
        -> ST unit
           (requires (fun h -> (forall b.{:pattern (trigger b)}
                                            (trigger b /\ mem b bs ==> robot_inv b h))))
           (ensures  (fun h0 _u h1 ->
                          modifies rs h0 h1
                           /\ (forall b.{:pattern (trigger b)}
                                  (trigger b /\ mem b bs ==> robot_inv b h1 /\ flying b h1))))
~~~
~~
~


## Why hyper-heaps work

The style of dynamic frames and explicit inequalities between objects is
not incompatible with hyper-heaps: as we've also seen, within a region,
we resort to pairwise inequalities, as usual. However, using hyper-heaps
where possible, as we've just seen, makes specifications much more
concise. As it turns out, they also make verifying programs much more
efficient. On some benchmarks, we've noticed a speedup in verification
time of more than a factor of `20x` when using hyper-heaps for
disjointness invariants. In this section, we look a bit more closely at
what's happening under the covers and why hyper-heaps help.

First, without hyper heaps, consider a computation `f ()` run in a heap
`h0` and producing a heap `h1` related by `modifies {x1,...,xn} h0 h1`,
for some set of references `{x1...xn}`. Consider also some predicate
`Q = fun h -> P (sel h y1) ... (sel h yn)`, which is initially true in
`h0`. We would like to prove that if `Q h1` is also true. In other words,
we need to prove the formula:

```
Expanding definitions:
  P (sel h0 y1) ... (sel h0 ym)
  /\ h1 = concat h1 (restrict (complement {x1..xn}) h0)
  ==> P (sel h1 y1) ... (sel h1 ym)
```
In general, for an arbitrary uninterpreted predicate `P`, to prove this,
one must prove an quadratic number of inequalities, e.g., to prove that
`sel h1 y1 = sel h0 y1`, requires proving that `y1` is not equal to any
the `{x1..xn}`.

However, if one can group related references into regions (meaning
related references are generally read and updated together), one can do
much better. For example, moving to hyper-heaps, suppose we place all the
`{x1..xn}` in a region included by `rx`. Suppose `{y1,...,ym}` are all
allocated in some region `ry`. Now, our goal is to prove

```
  P (Heap.sel (Map.sel h0 ry) y1) ... (Heap.sel (Map.sel h0 ry) ym)
  /\ h1 = concat h1 (restrict (mod_set {rx}) h0)
  ==> P (Heap.sel (Map.sel h1 ry) y1) ... (Heap.sel (Map.sel h1 ry) ym)
```

To prove this, we only need to prove that
`Map.sel h0 ry = Map.sel h1 ry`, which involves proving
`not (included rx ry)`. Having proven this fact, the SMT solver simply
unifies the representation of all occurrences of these sub-terms
everywhere in the formula above and concludes the proof immediately.
Thus, in such (arguably common) cases, what initially required quadratic
number of reasoning steps, now only requires a constant number of steps!

In more general scenarios, we may still have to perform a quadratic
number of reasoning steps, but with hyper-heaps, we are quadratic only in
the number of regions involved, which are often _much_ smaller than the
number of references that they may contain. The use of region hierarchies
serves to further reduce the number of region identifiers that one refers
to, making the constants smaller still. Of course, in the degenerate case
where one has just one reference per region, this devolves back to the
performance one would get without regions at all.

<!--

NS: I'm removing this section for the moment. It involves explain
a third model of the heap, augmented with permissions to deal with information
hiding etc. This is getting to be extremely advanced, particularly as it also involves
reasoning about existentially quantified invariant predicates. It was nice to have
as a placeholder when it was the only non-trivial stateful example available in the tutorial.
But, that's no longer the case. We could restore it at some point, with a full explanation
of the permission model etc., if we think that would be useful.

## Local state in two counters

In this simple example with two counters we show how to maintain useful
invariants about the heap.

We define an auxiliary effect `InvST` that maintains the invariant `inv`.

```
effect InvST (t:Type) (inv:(heap -> Type)) (fp:set aref) (post:(heap -> t -> heap -> Type)) =
             ST t (fun h -> On fp inv h)
                  (fun h i h' -> post h i h' /\ On fp inv h')
                  (SomeRefs fp)
```

The two counters will be updated simultaneously, such that their sum is
always even. This requirement is expressed using the type `even` and
`even_post` when lifted to the post condition of a stateful function.

```
type even (x:int) = x%2=0
type even_post (h:heap) (i:int) (h':heap) = even i
```

The following invariant states that the counters are allocated and synchronized.
```
opaque type inv1 (r1:ref int) (r2:ref int) (h:heap) =
           Heap.sel h r1 = Heap.sel h r2
           /\ contains h r1
           /\ contains h r2
```

We will employ two techniques to prove that the invariant is maintained.

 * First, by proving that `r1` and `r2` are incremented by the same amount.
 * Second, using alias analysis by proving that `r1` and `r2` are not changed at all.


```
type t =
  | Evens : inv:(heap -> Type)
          -> fp:set aref
          -> (unit -> InvST int inv fp even_post)
          -> t



val mk_counter: unit
             -> ST t (requires (fun h -> True))
                     (ensures  (fun h v h' ->
                             On  (Evens?.fp v) (Evens?.inv v) h'
                             /\ Heap.fresh h (Evens?.fp v)))
                     (modifies no_refs)
let mk_counter _ =
  let x = ST.alloc 0 in
  let y = ST.alloc 0 in
  let evens () =
    let rx = ST.read x in
    let ry = ST.read y in
    ST.write x (rx + 1);
    ST.write y (ry + 1);
    rx + ry in
  Evens (inv1 x y) (Set.union (Set.singleton (Ref x)) (Set.singleton (Ref y))) evens


val mk_counter_2: unit
               -> ST t (requires (fun h -> True))
                       (ensures  (fun h v h' ->
                         On  (Evens?.fp v) (Evens?.inv v) h'
                         /\ Heap.fresh h (Evens?.fp v)))
                       (modifies no_refs)
let mk_counter_2 _ =
  let x = ST.alloc 0 in
  let evens = fun _ ->
    let rx = ST.read x in
    ST.write x (rx + 1);
    2 * rx in
  Evens (fun h -> b2t(contains h r)) (Set.singleton (Ref x)) evens
```

~Exercise
Fix this example.

~~ ExerciseFragment {exname=Ex10c}
val mk_counter_2: unit
               -> ST t (requires (fun h -> True))
                       (ensures  (fun h v h' ->
                         On  (Evens?.fp v) (Evens?.inv v) h'
                         /\ Heap.fresh h (Evens?.fp v)))
                       (modifies no_refs)
let mk_counter_2 _ =
  let x = ST.alloc 0 in
  let evens = fun _ ->
    let rx = ST.read x in
    ST.write x (rx + 1);
    2 * rx in
  Evens (fun h -> b2t(contains h r)) (Set.singleton (Ref x)) evens
~~

~
-->

# Cryptography examples for authentication

Cryptographic reasoning is inherently modular reasoning. We want to show 
that using some cryptographic primitive we can secure some application goal.
We will formalize both the guarantees afforded by the cryptographic module, 
as well as the goals of the application employing the module using types. 

## Message authentication codes (MAC) { #MAC }

We begin with  a simple cryptographic library for message authentication codes (MAC) in `module MAC`.

MACs provide integrity protection using shared key cryptography, often using
keyed cryptographic hashes.

A MAC scheme typically consists of three algorithms, a `keygen` 
algorithm for generating the shared `key`,
a `mac` function for generating a MAC `tag` on message `text` using `key`, and
a `verify` function for verifying that `tag` is a valid MAC for `text` and `key`.

We say that messages for which `verify key text tag=true` have been authenticated. But what does this mean formally? Does it mean that the owner of `key` endorsed the message `text`. But who is the owner of `key`, which after all is a bitstring that could be learned and copied. And for deployed MAC functionalities,  `verify` is a pure function, so even if the key was kept perfectly secret, a randomly picked `tag` will verify with some probability.

Cryptographers grappled with these problems and specify the properties of a MAC negatively by bounding the probability of realistic adversaries forging a `tag`. This is typically done in the form of a random experiment:

### The message authentication experiment { #MAC-game }

A MAC scheme implementing `keygen`, `mac`, `verify` is unforgeable against chosen message attacks (UF-CMA) 
if the success probability of all 'realistic' adversaries 
`A` (modelled as F\* programs) is small in the following probabilistic experiment:

 * A key `k` is generated by running `keygen`.
 * The adversary `A` is given oracle access to `mac k`. To the adversary, the key used to answer oracle queries is hidden but he can provide messages `text'` to learn tags of the form `mac k text'`. We let the set `log` record all `text'` that `A` asked to the oracle. Eventually, the adversary outputs a text and tag pair: `(text,tag)`.
 
 * The adversary succeeds if and only if (1) `verify k text tag = true` and (2) `not (mem text log)`. That is if he produced a verifying tag on a new message `text`.

We refer to the success probability of `A` in this experiment as its UF-CMA advantage $\epsilon_{ufcma}(A)$.

Does the appearance of *probabilities* in this section make you suspicious? It should, as nowhere in this tutorial have we so far discussed the use of F\* for probabilistic reasoning. The F\* type checker is a deductive reasoning tool, and trades in certainties and not in probabilities. While we will see that there are ways to extend F\* semantics with probabilities to express probabilistic experiments in the language of F\*, we will now provide an abstraction or idealization of the guarantees of the MAC functionality. We will discuss below, how one can formally establish the soundness of this idealization. 

### The MAC type interface 
We define an interface that models perfect authentication guarantees. We will get back to how it can be (approximately) implemented by an F* module later.

~~ SolutionFragment {exname=Ex12.MAC}
[INCLUDE=code/solutions/Ex12.MAC.fst:MacSpec]
~~


The specification of `keygen` attaches an authenticated property `p: text -> Type` to each key using function `key_prop k` and type `pkey p`:

The function `key_prop : key -> p:(text -> Type)` maps each key to an application defined authentication property `p` and the type `pkey p = k:key{key_prop k == p}` denotes keys with property `p`.

The property `(key_prop k) t` is used as a pre-condition for MACing and as a postcondition for verifying a message `t` with key `k`.


When implementing the message authentication experiment using a `key_prop k` that witnesses the monotone property `(mem text log)` before answering a `mac k` query, the postcondition of `verify` guarantees that `A` has UF-CMA advantage 0. This confirms that the interface models perfect authentication guarantees. 

In the RPC example below ([#sec-secure-rpc]) we will define `key_prop k == reqresp` and uses keys of type `pkey reqresp` where
```
[INCLUDE=code/solutions/Ex12b.RPC.fst:MsgProperty]
```
which states that messages that verify with that key are either 
`Request`s or `Respone`s. The formatting functions use tags to disambiguate between requests and responses. 



## Cryptographic Capabilities for Accessing Files

We extend the access control example ([#sec-policy]) with cryptographic capabilities, so
that our trusted `ACLs` library can now issue and accept MAC tags
as evidence that a given file is readable. Recall that this exercise employed 
predicates `ACLS.canRead f` and `ACLS.canWrite f` on filenames that determined whether 
a file could be read or written.

We now attach these predicates to unforgeable cryptographic tokens also called capabilities.

*What is the advantage of capabilities over the dynamic access control based on state of Section [#sec-stateful-access-control]?*

Capabilities need to be kept secret, as everyone knowing the capabilities is granted access to 
the corresponding files, however, parties no longer need to keep a synchronized access control database.
In order to verify capabilities, they only need to store the cryptographic key
used to generate the capability. The tokens generated in this fashion work in a
similar fashion as tokens in the *Kerberos* authentication protocol.

~ Exercise
Relying on `ACLs` and `MAC`, implement a pair of functions with
the following specification, and verify them by typing under the
security assumption that the MAC is UF-CMA. Use `failwith "bad capability"` in
case the verification fails.
~~ ExerciseFragment {exname=Ex12a1.Cap}
[INCLUDE=code/exercises/Ex12a1.Cap.fst:CapType]
~~

To this end, assume given a UTF8 encoding function from strings to
bytes, with the following specification:

```
[INCLUDE=code/exercises/Ex12a1.Cap.fst:UTF8Inj]
```

This says that if two `utf_8` encodings are equal, then so are the values that were encoded. 
The patterns instructs the SMT solver to associate the injectivity axiom only with very specific terms.


~~ Answer
~~~ SolutionFragment {exname=Ex12a1.Cap}
[INCLUDE=code/solutions/Ex12a1.Cap.fst:CapImplementation]
~~~
~~

How would we extend it to also cover write access?

~~ Answer
~~~ SolutionFragment {exname=Ex12a2.Cap}
[INCLUDE=code/solutions/Ex12a2.Cap.fst:CapImplementation2]
~~~
~~

~

~KK
Added Ex12a1.Cap.fst as a proposed answer to this question. It would be nice if
someone could review my code and confirm that it is actually a valid solution.
~

## Secure RPC

We describe a simple secure RPC implementation, which consists of three
modules: `MAC`, our library for MACs, `Format`, a module for message
formatting, and `RPC`, a module for the rest of the protocol code.
(Placing the formatting functions request and response in a separate
module is convenient to illustrate modular programming and verification.)

### Secure RPC protocol

To start with we consider a single client and server. The security goals of
our RPC protocol are that (1) whenever the server accepts a
request message `s` from the client, the client has indeed sent the message to
the server and, conversely,
(2) whenever the client accepts a response message `t` from the server,
the server has indeed sent the
message in response to a matching request from the client.

To this end, the protocol uses message authentication codes (MACs) computed as
keyed hashes, such that each symmetric MAC key kab is associated with (and known
to) the pair of principals a and b. Our protocol may be informally described as follows.
An Authenticated RPC Protocol:

    1. client->server  utf8 s | (mac kab (request s))
    2. server->client  utf8 t | (mac kab (response s t))

In the protocol narration, each line indicates an intended communication of data
from one principal to another.

`utf8` marshalls strings such as `s` and `t` into byte arrays
that can be send over the network. Operator `|` written also `append`
is a function concatenating `bytes` (In F\* we will use the infix operator `@|`). Functions `request` and `response` build
message digests (the authenticated values). These functions may for
instance be implemented as tagged concatenations of their utf8-encoded
arguments. These functions will be detailed in the `Format` module.

Below we give an implementation of the protocol using these functions,
the functions of the 'MAC' module, and networking functions

```
[INCLUDE=code/solutions/Ex12b.RPC.fst:Network]
```

These functions model an untrusted network controlled by the adversary, who decides
through their implementation when to deliver messages.

Two events, record genuine requests and responses
```
[INCLUDE=code/solutions/Ex12b.RPC.fst:RpcPredicates]
```
in the following protocol

~~ SolutionFragment {exname=Ex12b.RPC}
[INCLUDE=code/solutions/Ex12b.RPC.fst:RpcProtocol]
~~

For simplicity we initially elide the client code which is similar, but you can open the module in the online editor to look at the full protocol.

The `assert`s guarantee that messages that are received by a party have been sent by its peer.

~ Remark
One can imagine an extended protocol where we have a
population of principals, represented as concrete strings ranged over
by `a` and `b`, that intend to perform various remote procedure calls
(RPCs) over a public network.
The RPCs consist of requests and responses, both also represented as strings.
The security goals of our RPC protocol are that (1) whenever a principal `b` accepts a
request message `s` from `a`, principal `a` has indeed sent the message to `b` and, conversely,
(2) whenever `a` accepts a response message `t` from `b`, principal `b` has indeed sent the
message in response to a matching request from `a`.
~


### Formatting code

The `Format` module used by the protocol implementation
exports among others the following types and functions.

```
[INCLUDE=code/solutions/Ex12b2.Format.fst:FormatMsg]

[INCLUDE=code/solutions/Ex12b2.Format.fst:FormatReqRes]
```

We require 3 lemmas on message formats:

* requests and responses are distinct
* requests are injective on their argument
* responses are injective on both their arguments

For authentication properties, we require both functions to be
injective, so that authenticating the bytes unambiguously
authenticate the text before marshalling.

~Exercise
Try to state this formally as three lemmas.
~~ ExerciseFragment {exname=Ex12b1.Format}
[INCLUDE=code/exercises/Ex12b1.Format.fst:FormatLemmasEx]
~~

~~Answer
~~~ SolutionFragment {exname=Ex12b1.Format}
[INCLUDE=code/solutions/Ex12b2.Format.fst:FormatLemmas]
~~~
~~

Prove these three lemmas.
~~ ExerciseFragment {exname=Ex12b2.Format}
[INCLUDE=code/exercises/Ex12b2.Format.fst:FormatProofsEx]
~~

~~ Answer
~~~ SolutionFragment {exname=Ex12b2.Format}
[INCLUDE=code/solutions/Ex12b2.Format.fst:FormatProofs]
~~~
~~
~

These lemmas rely on a lemma about the
injectivity of append:

```
[INCLUDE=code/solutions/Ex12b2.Format.fst:FormatAppend]
```

~ Exercise
Complete the proof of the lemma.

~~ ExerciseFragment {exname=Ex12c.Format}
[INCLUDE=code/exercises/Ex12c.Format.fst:FormatAppendEx]
~~
~


<!--

~~ Answer
```
let rec append_inj_lemma b1 b2 c1 c2 =
  let l = length b1 in
  match l with
  | 0 -> ()
  | _ -> append_inj_lemma (slice b1 1 l) b2 (slice c1 1 l) c2
```
~~
-->


<!--
## Signing Email

The purpose of this exercise is to program and verify functions for
signing email messages and verifying their signatures.
-->

## Verified Padding

We intend to MAC and/or encrypt plaintexts of variable sizes using
fixed-sized algorithms, such as those that operate on 256-bit blocks, such as AES.

To this end, we use the following classic PKCS#7 padding scheme (used for instance in TLS).

```
[INCLUDE=code/exercises/Ex12.Pad.fst:CreatePadding]

[INCLUDE=code/exercises/Ex12.Pad.fst:EncodePadding]

[INCLUDE=code/exercises/Ex12.Pad.fst:DecodePadding]
```

The function `n2b` converts numbers less than 256 into a byte; `pad` 
creates `bytes` consisting of `n` bytes encoding the number `n`;
`encode` fills up `bytes` smaller than a block to a full block using such pads; and
`decode` removes pads.

~ Exercise
Write a typed specifications for these functions. In particular
show that decoding cancels out encoding.
~~ ExerciseFragment {exname=Ex12d.Pad}
[INCLUDE=code/exercises/Ex12d.Pad.fst:PadEx]
~~

~~ Answer

~~~ SolutionFragment {exname=Ex12d.Pad}
[INCLUDE=code/solutions/Ex12d.Pad.fst:CreatePaddingT]
[INCLUDE=code/solutions/Ex12d.Pad.fst:EncodePaddingT]
[INCLUDE=code/solutions/Ex12d.Pad.fst:DecodePaddingT]
~~~
~~
~


~ Exercise
Is it a good padding scheme? What are its main properties?

Prove a lemma that expresses its injectivity.

Provide examples of other schemes, both good and bad.

~~ Answer

A padding scheme should be invertible. For authentication properties,
we require it to be injective, so that authenticating a padded text
unambiguously authenticate the text before padding. For instance:

~~~ SolutionFragment {exname=Ex12e.Pad}
[INCLUDE=code/solutions/Ex12e.Pad.fst:PaddingInj]
~~~

In this context, another good padding scheme is to append a 1 then
enough zeros to fill the block. One could also format the message by
explicitly including its length.
A bad scheme is to fill the rest of the block with zeros (or random
values), as the same padded text may then be parsed into several texts
ending with 0s.
~~
~

~ Exercise
Relying on that specification of `encode`, implement and verify a MAC interface
for messages of type `text` of less than `blocksize` bytes,
using the MAC interface below, which only supports messages of type
`block`.

To this end, we assume given an implementation of the following module
(adapted from the `MAC` module)

```
[INCLUDE=code/exercises/Ex12.BMAC.fst]
```

Write a similar module that accepts texts shorter than `blocksize` instead of blocks:

Write the implementations for `keygen`, `mac`, and `verify`, first
then devise type annotations to derive their authentication property
(on `texts`) from the one of `BMAC` (on `blocks`).

~~ ExerciseFragment {exname=Ex12f.TMAC}
[INCLUDE=code/exercises/Ex12f.TMAC.fst:TMACEx]
~~

~~ Answer
~~~ SolutionFragment {exname=Ex12f.TMAC}
[INCLUDE=code/solutions/Ex12f.TMAC.fst:TMAC]
~~~
~~
~

~ Exercise
1. Can we similarly construct a MAC scheme for texts
   with type `text:bytes {length text <= n}`?
   
2. Implement, then verify this construction.

~~ Answer
1. Yes, using two keys: one for `text:bytes {length b = n}`
   and another for `text:bytes {length b}`.

2. Code:

~~~ SolutionFragment {exname=Ex12g.TMAC2}
[INCLUDE=code/solutions/Ex12g.TMAC2.fst:TMAC2]
~~~
~~

~

~ Exercise
What about MAC schemes for texts of arbitrary lengths?
Hint: have a look e.g. at the HMAC construction.
~

# Cryptography examples for confidentiality

##  Theoretical background on cryptographic verification in F\*
Those unfamiliar with functional programming may appreciate the
following [security centered look](https://arxiv.org/abs/1201.5728) at the differences between a functional
and an imperative programming style.

We use a [cryptographic verification by typing approach](http://research.microsoft.com/en-us/um/people/fournet/comp-f7/)
adapted to make use of new F\* features. We follow their notation
and recall them briefly.

*a) F\* Types:* A program is a sequential composition of modules,
written $A_1 \cdot A_2 \cdot \dots \cdot A_n$. A module $A$ depends on types, values,
and functions from its environment and exports types,
values, and functions in its own interface $I$.
A module is well typed, written $I_1 ,\dots ,I_\ell \vdash A \leadsto I$ if it correctly
implements $I$ using modules with interfaces $I_1 ,\dots,I_\ell$. The simplest
well typed program is an expression $A$ that has type $t$ in typing
environment $I$, written $I \vdash A : t$. A program is closed if it is well
typed in the empty environment $\emptyset$.
A program is *safe* if, in every run of the program, all runtime
values satisfy their type. A key property of a type system is that
well-typed programs are always safe.

~KK
Are modules and expressions the same in this notation?
~
[link]: http://link.com "link title"

**Theorem 1 (Safety by Typing):** If $\emptyset \vdash A : t$, then $A$ is safe.

In F* randomness originates from calls to effectful external functions
and at the meta level we model these functions probabilistically via a
`sample` expression, which reduces
to either true or false with probability 1/2. This expression
models a perfect source of random bits. In this probabilistic interpretation,
two expressions $A_0$ and $A_1$ are equivalent, written $A_0 \approx A_1$,
when they return the same probability distribution of values.

An interface or module can declare a type as [abstract], e.g. `abstract type key = bytes`, 
and keep its representation hidden. The 
[qualified](https://github.com/FStarLang/FStar/wiki/Qualifiers-for-definitions-and-declarations) name is bound outside the scope of 
the module, but its definition is hidden. 
This guarantees
that modules using this interface will treat key values as opaque and
guarantees their integrity and secrecy.

As in the example above, one use of abstract types is to protect
keys from being leaked by programming mistakes. Another use of
abstract types is to verify simple equivalences for programs $P \cdot A$
written such that operations on secrets are performed by a pure
(side-effect free) module $P$ that exports a restricted interface $I_\alpha$
with an abstract type $\alpha$ for secrets. The rest of the program can
be passed secrets and can pass them back to $P$ but cannot access the
representation of secrets. For instance, it can never branch on secrets.

**Theorem 2 (Equivalence by Typing):** If $\emptyset \vdash P_b \leadsto I_\alpha$ for $b = 0,1$
and $I_α \vdash A : bool$, then $P_0 \cdot A \approx P_1 \cdot A$.

*b) $\epsilon$-Safety and $\epsilon$-Equivalence:* Instead of relying on 
the asymptotic safety and equivalence notions of the original [cryptographic verification by typing approach](http://research.microsoft.com/en-us/um/people/fournet/comp-f7/)
we use a concrete $\epsilon$ loss
term. A module A is $\epsilon$-safe if the probability of an assertion failing
in $A$ is $\epsilon$.
Two closed boolean expressions $A_0$ and $A_1$ are $\epsilon$-equivalent when
the statistical distance
$\frac{1}{2} \sum_{M=true,false} |Pr[A_0 \Downarrow M]-Pr[A_1 \Downarrow M]|$
is $\epsilon$. If $A$ is not closed, then the value of $\epsilon$ may depend on the context in which 
it is executed, in this case we model $\epsilon(\cdot)$ as a function that depends on the context 
and call it an advantage function.

~KK
I dit not quite understand what M is at this point and what the $\Downarrow$
notation means.
~

*c) Keyed module:* Keyed modules export `gen`, `leak`, and `coerce`
functions and provide some cryptographic functionality modeled
using idealization for honest keys and strong algorithms. To this
end keys can be associated with a unique index upon generation. Keyed
modules enforce that `gen` is called at most once per index. The index
describes the context the key was created in and the algorithms it is
to be used with.

~KK
Many buzzwords in this paragraph. What is "idealization", "honest" and "strong"
in this context?
~

*d) Plain module:* To define confidentiality using types, consider a
module $P$ that is typeable using an interface $IP$ of the form

```
[INCLUDE=code/solutions/EtM.Plain.fst:EtMPlain]
```

<!--
We call $IP$ a secrecy interface for $P$.
The representation of plaintexts is `bytes`, whereas the
type plain is `abstract`, with functions `repr` and `coerce` converting between
them. If `conf` is `true`, the function `repr` cannot be called, intuitively
making the interface parametric in type `plain`, so that we can apply *Theorem 2*
-->

If `conf` is `true`, the function `repr` cannot be called, intuitively
making the interface parametric in type `plain` except for the information leaked by `length`.

When `auth` is true, the function `coerce` cannot be called, guaranteeing authenticity.
The module $P$ does not hide the length of `plain`.
as modelled by the function `length` returning its length.

~MK
The following is not true because of the `length` function. Removing it until we have a better answer. Indexing by length might be it.
~

<!--
**Definition 1 (Interface-based secrecy):** Let $IP$ be the secrecy interface
of $P$ and $IC$ be the interface of module $C$. $C$ protects the secrecy
of `plain` , if `conf=true`, $IP \vdash C \leadsto IC$ and for all modules $P$ with
`conf=false`, $P \leadsto IP$ and `conf=true`, $P \leadsto IP$; and $A$ with $IP,IC \vdash A$ 
we have `conf=false`, $P \cdot C \cdot A \approx_{\epsilon(P,A)}$ `conf=true`, 
$P \cdot C \cdot A$

~KK
I was not able to parse this definition. It might make sense to add a verbatim
explaination of the formalism? 
~


The function $\epsilon(.,.)$ is called the advantage function of $P,A$.


When `conf=true`, $C\cdot A$ is well typed with respect to interface "`conf=true`, $IP$" 
which can be interpreted as
the $I_\alpha$ interface of *Theorem 2*. Consequently for any module $P_0,P_1$ satisfying 
this interface, we have 
`conf=true`$\cdot P_0 \cdot C \cdot A \approx$ `conf=true`$\cdot P_1 \cdot C \cdot A$.

Using *interface-based secrecy* twice we get that 
`conf=false`$\cdot P_0 \cdot C \cdot A \approx_{\epsilon_0(A,P_0)+\epsilon_1(A,P_1)}$ `conf=false`$\cdot P_1 \cdot C \cdot A$.
-->

To achieve its strong confidentiality strong property `conf=true`$\cdot C$ must not be able to access the 
representation of `plain` and thus 
needs to be idealized. For encryption modules this is typically done by encrypting a 
value that is independent of the plaintext, such as a string of zeros. To make also the 
decryption of such encryptions of zero indistinguishable from decryptions of the acutal 
plaintext, the idealization of $C$ decrypts using a table lookup.

~KK
At this point, maybe it would be possible to reuse some of the material of the
intro to the Record-Layer paper? It might be helpful to provide the reader with
a more high-lever overview over what we are actually trying to achieve here and
how F\* is used in the process. For the encrypt-then-mac example in the next
section, it might be nice to have an introduction to game-based proofs.
~

Lets make the theory more concrete by looking at Encrypt-then-MAC 
based authenticated encryption. 

## Encrypt-then-MAC 

At the high level, we  define 5 modules:

*1. the `Ideal` module* defines `auth` and `conf` flags and relates 
them to assumptions that allow idealization of the underlying cryptographic building blocks. 
Flag `ind_cpa` controls the idealization for an encryption module that is indistinguishable under chosen plaintext attack (IND-CPA), 
and flag `uf_cma` controls the idealization for a UF-CMA secure MAC module. UF-CMA security was defined above using a cryptographic game ([#MAC-game]), we will se a similar game for IND-CPA below.

*2. the `Plain` module* is a secrecy module defining a `plain` type. 
Its representation is abstract and can only be accessed using the `repr` function 
when `conf=false`.

*3. the `CPA` module* models an IND-CPA secure encryption scheme. Intuitively, when the adversary is 
restricted to only query `decrypt` on ciphertexts resulting from `encrypt` 
(under the same key), then the idealization of `CPA` can encrypt zeros and `CPA` 
protects the secrecy of `plain`. This is modelled by setting `conf=ind_cpa`.

*4. the `MAC` module* models an UF-CMA secure message authentication scheme. The idealization
of `MAC` guarantees that only message and tag pairs that were `mac`ed can `verify`.

*5. the `AE` module* composes `CPA` and `MAC`. An `AE` key `k` consists of a key `k.ke` for a 
CPA secure encryption scheme, and  a `MAC` key `k.km`. The `AE` ciphertext is computed by 
`encrypt`ing the plaintext and then `mac`ing the ciphertext: 

~ SolutionFragment {exname=EtM.AE}
[INCLUDE=code/solutions/EtM.AE.fst:EtMAEEncrypt]
~
The monotonic log `k.log` is only used to model security properties and does not exist in
concrete schemes.

Before `decrypt`ing the `CPA` ciphertext, `AE` verifies the corresponding MAC tag:

~ SolutionFragment {exname=EtM.AE}
[INCLUDE=code/solutions/EtM.AE.fst:EtMAEDecrypt]
~

`AE` treats `CPA` and `MAC` keys abstractly, and never reveals them to an adversary $A$.
This together with the fact that `AE` only ever `mac`s after `encrypt`ing, guarantees 
that `AE`$\cdot A$ is a restricted `CPA` adversary.

Let us now look in more detail at the `CPA` and `MAC` modules and how they model the security 
of the underlying primitives. 

### Chosen Plaintext Attack (CPA) secure encryption

Our concrete implementation of CPA secure encryption uses `AES_128_CBC`. 
It first samples a random initialization vector `iv`;
obtains a representation of the key using `k.raw` and the plaintext using `repr m`;
and calls a library function implementing `AES_128_CBC`:

```
[INCLUDE=code/solutions/EtM.CPA.fst:EtMCPAEncrypt]
```

Calling the library function which expects bytes as input, requires breaking the 
abstraction barriers. This is not a problem for the AES key, as its type is 
declared in the same module. It is however forbidden for the `Plain` module in which 
`plain` is defined as calls to `repr` are only allowed when `priv=false`.

Instead, when `priv=true` module `CPA` computes `text` in the following way:
```
  let text = if ind_cpa then createBytes (length m) 0z else repr m in
```

Here we make use of the fact that we defined `priv=ind_cpa` in the `Ideal` module. 
This change is justified by plaintext indistinguishability, as adversaries cannot distinguish encryptions 
of `m` from encryptions of `(length m)` `0z` bytes. 

For IND-CPA secure schemes this indistinguishability is only guaranteed to hold for adversaries 
that can provide plaintext to `encrypt`, but cannot provide ciphertexts to `decrypt`. 

To see this consider the following classical definition.

#### The CPA indistinguishability experiment { - }

A CPA scheme encryption scheme $\Pi=($`keygen`,`encrypt`,`decrypt`$)$ is 
IND-CPA secure if the advantage of 'realistic' adversaries 
$A$ in the following probabilistic experiment is small:

 * A key `key` is generated by running `keygen`.
 * The adversary $A$ is given oracle access to `encrypt key`. The key is hidden but he can provide messages to learn ciphertexts. 
   The adversary eventually outputs a pair of messages `m_0` and `m_1` of the same length. 
 * A random bit `b` is chosen and a ciphertext `c = encrypt key m_b` is 
   computed and handed to $A$.
 * The adversary $A$ continues to have oracle access to `encrypt key` and outputs a bit `b'`.
 * The output of the experiment is defined to be 1 if `b=b'` and 0 otherwise. (When the output is 1,
   we say that the adversary succeeded.)

The advantage of $A$ is defined as $\epsilon(A) = 2 Pr[b=b']-1$.

#### The CPA encryption type interface { - }

Without loss of generality, we can allow 'decryption' of ciphertexts resulting 
from `encrypt`. To model this we change the last line of `encrypt` 
to be '`write_at_end k.log (m,c); c`' to update the `log` 
of ciphertext. By `decrypt`ing using table lookup as follows we still 
hide whether it was `m` or a string of `0z`s that was encrypted:
```
[INCLUDE=code/solutions/EtM.CPA.fst:EtMCPADecrypt]
```

Note that in the `match` statement, we did not define a '`| None`' branche. This is because the 
precondition of `decrypt` requires for IND-CPA restricted adversaries (`ind_cpa_rest_adv=true`)
that table lookup in `log` always succeeds.

This can be expressed as a stateful precondition for `decrypt`:

~ SolutionFragment {exname=EtM.CPA}
[INCLUDE=code/solutions/EtM.CPA.fst:EtMCPADecryptRequires]
~
where function `m_sel h0 k.log` retrives the content of `k.log` as stored in heap `h0`.

~ Exercise
Play with the type of `EtM.CPA.decrypt` above. What fails if you remove the precondition. What is the connection with chosen plaintext attacks?

~~ Answer
The type checker fails with a "Patterns are incomplete" error as it can no longer guarantee that it will find the ciphertext in the `log`. To preserve functionality one would have to use the `block_decrypt` functionality as in the else branch which could leak information to the attacker which might make the scheme insecure. The stronger security notion of indistinguishability against chosen ciphertext attacks (IND-CCA) guarantees security in such a setting.
~~

~

### Using authentication to achieve stronger confidentiality

We already introduced the MAC unforgeabiltiy property in ([#MAC]). Here we model 
MAC security using an idealization involving a log of `mac`ed messages and their tags:
```
[INCLUDE=code/solutions/EtM.MAC.fst:EtMMACMac]
```

Instead of modelling unforgeability using a predicate that is required upon 
`mac`ing and guaranteed upon `verify`ing, we now express the unforgeability guarantee 
directly on the log. In the idealization, only `(m,t)` pairs that are in the log verify.


```
[INCLUDE=code/solutions/EtM.MAC.fst:EtMMACVerifyT]
```

As for encryption, we also give an ideal functionality that is well typed 
with respect to this specification:

~ SolutionFragment {exname=EtM.MAC}
[INCLUDE=code/solutions/EtM.MAC.fst:EtMMACVerify]
~

As any adversary that causes a difference between `verified` and `found` is also a successful UF-CMA adversary we have that
`uf_cma=false`, `MAC`⋅$A$ $\approx_{\epsilon_{ufcma}}(A)$
`uf_cma=true`, `MAC`⋅$A$. 

Said otherwise, as long as the MAC construction is UF-CMA secure, the adversaries behaviour cannot differ substantially between the *ideal world* and the *real world*. 

 - In the *ideal world* in which `uf_cma=true` all MAC forgeries are rejected, but we make the unrealistic assumption of maintaining the global `log`. 
 
 - In the *real world*, in which `uf_cma=false` forgeries are in principle always possible but we use cryptography instead of the unrealistic global log.

~KK
At this point maybe another verbatim explaination of the formal statement?
~

~MK
Is this what you had in mind?
~

For our Encrypt-then-MAC scheme we establish the invariant that all `mac`ed messages correspond to 
`encrypt`ed ciphertexts. This allows us to establish the precondition of `decrypt` after MAC `verify`.

When the MAC is idealized, adversaries cannot compute ciphertexts that pass MAC verification. 

The invariant is expressed over logs and at first sight looks quite daunting:

~ SolutionFragment {exname=EtM.AE}
[INCLUDE=code/solutions/EtM.AE.fst:EtMAEInvariant]
~

Crucially all messages in the MAC log correspond to ciphertexts that resulted from previous encryptions.
This allows us to prove that `uf_cma=true` $\cdot$ `MAC` $\cdot$ `AE`$\cdot A$ is a restricted `CPA` adversary.
We can thus set `ind_cpa_rest_adv=uf_cma`.

### The computational security proof

We want to prove that:
For all `𝐴` that are well typed with respect to an unrestricted `Plain` and `AE`
interface, that is `conf=false, auth=false`, $IP$, $IC \vdash A$,
the following computational equivalence holds:

> (1) `conf=auth=ind_cpa_rest_adv=uf_cma=ind_cpa=false, Plain` ⋅ `CPA` ⋅ `MAC` ⋅ `AE` ⋅ `A`

>&emsp;&emsp;$\approx_{\epsilon_{indcpa}(B_1)+\epsilon_{ufcma}(B_2)}$

> (2) `conf=auth=ind_cpa_rest_adv=uf_cma=ind_cpa=true, Plain` ⋅ `CPA` ⋅ `MAC`⋅ `AE` ⋅ `A`.

In other words, the distinguishing advantage of an adversary `A` between the concrete implementation
and the idealized implementation of Encrypt-then-MAC is bounded by $\epsilon_{indcpa}(B_1)+\epsilon_{ufcma}(B_2)$.

To see this we reason about several game transformations.

*Game 1* is the same as program (1), except that we swap the order of `MAC` and `Plain`⋅`CPA` with 
the goal of moving all modules to the right of the `MAC` module. This change does not affect the 
distribution output of `A` and we thus have

(1) ≈ `conf=false, auth=false`, `MAC`⋅`Plain`⋅`CPA`⋅`AE`⋅ `A`

*Game 2* is the same as *Game 1*, except that we activate the idealizations of the `MAC` module by
setting `uf_cma=true`. 

> `conf=false, auth=false`, `MAC`⋅`Plain`⋅`CPA`⋅`AE`⋅ `A` 

> &emsp;&emsp; $\approx_{\epsilon_{ufcma}}(B_1)$

> `conf=false, auth=false, uf_cma=true`, `MAC`⋅`Plain`⋅`CPA`⋅`AE`⋅ `A`

$B_1$ corresponds to `Plain`⋅`CPA`⋅`AE`⋅ `A`, and can be interpreted as a reduction to UF-CMA security.

*Game 3* is the same as *Game 2*, except that we swap the order of `MAC` and `Plain`⋅`CPA` back and
also set `auth` and `ind_cpa_rest_adv` to `true`. These changes have no effect on the distribution of the program.

> ≈ `conf=false, auth=ind_cpa_rest_adv=uf_cma=true`, `Plain`⋅`CPA`⋅`MAC`⋅`AE`⋅ `A`

Because of the MAC guarantees we can establish the main invariant 
which is sufficient to prove that we never call `CPA.decrypt` on ciphertexts that are not in the log.
This allows us type the program with `auth` and `ind_cpa_rest_adv` set to true.

*Game 4* is the same as *Game 3*, except that we activate the idealizations of the `CPA` module by
setting `ind_cpa=true`.

> `conf=false, auth=ind_cpa_rest_adv=uf_cma=true`, `Plain`⋅`CPA`⋅`MAC`⋅`AE`⋅ `A`

> &emsp;&emsp; $\approx_{\epsilon_{indcpa}}(B_2)$ 

> `conf=false, auth=ind_cpa_rest_adv=uf_cma=ind_cpa=true`, `Plain`⋅`CPA`⋅`MAC`⋅`AE`⋅ `A` ≈ (2)

$B_2$ corresponds to `MAC`⋅`AE`⋅ `A`, and can be interpreted as a reduction to IND-CPA security.

Finally, `CPA`⋅`MAC`⋅`AE`⋅`A` does not make use of `Plain.repr` and we can thus type it as

>`conf=auth=ind_cpa_rest_adv=uf_cma=ind_cpa=true`, `Plain`⋅`CPA`⋅`MAC`⋅`AE`⋅`A` = (2) 

~ Align-Right
$\qed$
~

~Exercise
Explain the behaviour of the type checker when you try to prove that `AE` is 
confidential (`conf=true`), without the MAC being `uf_cma` secure.

Set the flags explicitly to see what happens:

```
let ind_cpa = true
let uf_cma = false
let ind_cpa_rest_adv = uf_cma
let conf = true
let auth = false
```

~~ Answer
The type checker fails to prove that plaintexts are `not conf`: 
```
.\EtM.CPA.fst(70,85-70,86): Subtyping check failed; 
expected type (p#25:EtM.Plain.plain{Prims.op_Negation EtM.Ideal.conf}); 
got type EtM.Plain.plain
```

This is needed to call the `repr` function in the `encrypt` function:

```
let text = if ind_cpa && ind_cpa_rest_adv then createBytes (length m) 0z else repr m 
```

This is because we cannot encrypt `0z`s when we do not have a restricted 
CPA adversary (`ind_cpa_rest_adv=false`). 

If we set `ind_cpa_rest_adv=true` even though `uf_cma=false` we get a type error

```
.\EtM.AE.fst(139,11-139,31): failed to prove a pre-condition
Error: 1 errors were reported (see above)
```

This results from failing to prove the pre-condition of `CPA.decrypt` in `AE.decrypt`.


~~
~




<!-- deactivating this example for now until we can replace it with Encrypt-then-MAC

# Advanced cryptographic examples

## Stateful Authenticated Encryption

A typical guarantee provided by a secure channel protocol is that
messages are received in the same order in which they were sent. To
achieve this, we construct a stateful, authenticated encryption scheme from
a (stateless) “authenticated encryption with additional data” (AEAD)
scheme (Rogaway 2002). Two counters are maintained, one each for the
sender and receiver. When a message is to be sent, the counter value is
authenticated using the AEAD scheme along with the rest of the message
payload and the counter is incremented. The receiver, in turn, checks
that the sender’s counter in the message matches hers each time a message
is received and increments her counter.

Cryptographically, the ideal functionality behind this scheme involves
associating a stateful log with each instance of a encryptor/decryptor
key pair. At the level of the stateless functionality, the guarantee
is that every message sent is in the log and the receiver only accepts
messages that are in the log---no guarantee is provided regarding
injectivity or the order in which messages are received. At the
stateful level, we again associate a log with each key pair and here
we can guarantee that the sends and receives are in injective,
order-preserving correspondence. Proving this requires relating the
contents of the logs at various levels, and, importantly, proving that
the logs associated with different instances of keys do not
interfere. We give this proof in F*.

We start with a few types provided by the `AEAD` functionality.

```
module AEAD
type encryptor = Enc : #r:rid -> log:rref r (seq entry) -> key -> encryptor
and decryptor =  Dec : #r:rid -> log:rref r (seq entry) -> key -> decryptor
and entry =  Entry : ad:nat -> c:cipher -> p:plain -> basicEntry
```

An `encryptor` encapsulates a `key` (an abstract type whose
hidden representation is the raw bytes of a key) with a log of entries
stored in the heap for modeling the ideal
functionality. Each `entry` associates a `plain` text `p`,
with its `cipher` `c` and some additional data `ad:nat`. The
log is stored in a region `r`, which we maintain as an additional
(erasable) field of `Enc`. The `decryptor` is similar. It is
worth pointing out that although `AEAD` is a stateless
functionality, its cryptographic modeling involves the introduction of
a stateful log. Based on a cryptographic assumption, one can view this log as
ghost.

On top of `AEAD`, we add a `Stateful` layer, providing stateful
encryptors and decryptors. `StEnc` encapsulates an encryption key
provided by `AEAD` together with the sender's counter, `ctr`,
and its own log of stateful entries, associates `plain`-texts
with `cipher`s. The `log` and the counter are stored in a
region `r` associated with the stateful encryptor. `StDec` is
analogous.

```
module Stateful
type st_enc = StEnc : #r:rid -> log: rref r (seq st_entry) -> ctr: rref r nat
           -> key:encryptor{extends (Enc?.r key) r} -> st_enc
and st_dec =  StDec : #r:rid -> log: rref r (seq st_entry) -> ctr: rref r nat
           -> key:decryptor{extends (Dec?.r key) r} -> st_dec
and st_entry = StEntry : c:cipher -> p:plain -> st_entry
```

Exploiting the hierarchical structure of hyper-heaps, we store
the `AEAD.encryptor` in a distinct sub-region of `r`---this is
the meaning of the `extends` relation. By doing this, we ensure
that the state associated with the `AEAD` encryptor is distinct
from both `log` and `ctr`. By allocating distinct instances
`k1` and `k2` in disjoint regions, we can prove that using
`k1` (say `decrypt k1 c`) does not alter the state
associated with `k2`. In this simplified setting with just three
references, the separation provided is minimal; when manipulating
objects with sub-objects that contain many more references (as in our
full development), partitioning them into separate regions provides
disequalities between their references for free.


~ Exercise
Look at the following code, what happens in case of decryption failure?

~~ CodeFragment {filename=exercises/stateful-encrypt.fst name=ex13a-stateful-encrypt}
type statefulEntry =
  | StEntry : c:cipher -> p:plain -> statefulEntry

type st_encryptor (i:rid) =
  | StEnc : log: rref i (seq statefulEntry) ->
            ctr: rref i nat ->
            key:encryptor i ->
            st_encryptor i

type st_decryptor (i:rid) =
  | StDec : log: rref i (seq statefulEntry) ->
            ctr: rref i nat ->
            key:decryptor i ->
            st_decryptor i

  ...

val stateful_enc : #i:rid -> e:st_encryptor i -> p:plain -> ST cipher
  (requires (fun h -> st_enc_inv e h))
  (ensures (fun h0 c h1 ->
            st_enc_inv e h1
         /\ HyperHeap.modifies (Set.singleton i) h0 h1
         /\ Heap.modifies (refs_in_e e) (Map.sel h0 i) (Map.sel h1 i)
         /\ sel h1 (StEnc?.log e) = snoc (sel h0 (StEnc?.log e)) (StEntry c p)))
let stateful_enc i (StEnc log ctr e) p =
    let c = enc e (op_Bang ctr) p in
    op_Colon_Equals log (snoc (op_Bang log) (StEntry c p));
    op_Colon_Equals ctr (op_Bang ctr + 1);
    c

val stateful_dec: #i:rid -> d:st_decryptor i -> c:cipher -> ST (option plain)
  (requires (fun h -> st_dec_inv d h))
  (ensures (fun h0 p h1 ->
        st_dec_inv d h0    //repeating pre
        /\ st_dec_inv d h1
        /\ HyperHeap.modifies (Set.singleton i) h0 h1
        /\ Heap.modifies !{as_ref (StDec?.ctr d)} (Map.sel h0 i) (Map.sel h1 i)
        /\ Let (sel h0 (StDec?.ctr d)) (fun (r:nat{r=sel h0 (StDec?.ctr d)}) ->
           Let (sel h0 (StDec?.log d))
           (fun (log:seq statefulEntry{log=sel h0 (StDec?.log d)}) ->
           (None? p ==> (r = Seq.length log           //nothing encrypted yet
                          || StEntry?.c (Seq.index log r) <> c //wrong cipher
                            ) /\ sel h1 (StDec?.ctr d) = r)
        /\ (Some? p ==>
                  ((sel h1 (StDec?.ctr d) = r + 1)
                   /\ StEntry?.p (Seq.index log r) = Some?.v p))))))

let stateful_dec _id (StDec _ ctr d) c =
  let i = op_Bang ctr in
  cut(trigger i);
  match dec d (op_Bang ctr) c with
    | None -> None
    | Some p -> op_Colon_Equals ctr (op_Bang ctr + 1); Some p

~~

~

## Encryption

Next we will look into constructing basic encryption schemes from even lower
level cryptographic primitives.

The basis of our first encryption example is the interface of a symmetric
encryption scheme based on a block cipher, say AES.
It uses a one block initialization vector (IV) that is XORed to the plaintext.
The masked value is then enciphered. The ciphertext consists of the IV and this
enciphered block.

```
module AES
open Array

type bytes = seq byte
type nbytes (n:nat) = b:bytes{length b == n}

let blocksize = 32 (* 256 bits *)

type plain  = nbytes blocksize
type cipher = nbytes (2 * blocksize)  (* including IV *)

let keysize = 16 (* 128 bits *)
type key = nbytes keysize

assume val gen: unit -> key
assume val dec: key -> cipher -> Tot plain
(* this function is pure & complete *)
assume val enc: k:key -> p:plain -> c:cipher
```

~ Exercise
Refine the type of the encryption function to model the *correctness* property:
A correctly generated ciphertext, decrypts to its plaintext.
~~ CodeFragment {filename=exercises/encrypt.fst name=ex13a-encrypt}
assume val enc: k:key -> p:plain -> c:cipher
~~

~~ Answer

```
assume val enc: k:key -> p:plain -> c:cipher{ dec k c = p }
(* this function is not pure (samples IV);
   the refinement captures functional correctness *)
```
~~

~

We model *perfect secrecy* by typing using type abstraction. As concrete cryptographic
algorithms such as the one above talk about bytes, we now introduce the notion of a
symmetric encryption scheme that enforces type abstraction for `safe` keys.

A scheme is parameterized by a key type `k` for keys and a plain type `p` that abstractly defines plaintexts.

Keys are abstract to enforce a well typed key usage regime on well typed users of the module.

```
module SymEnc (* a multi-key symmetric variant; for simplicity:
                 (1) only using AES above; and (2) parsing is complete *)

type r = AES.plain

(* CPA variant: *)
opaque type Encrypted: #k: Type -> p:Type -> k -> AES.bytes -> Type
(* an opaque predicate, keeping track of honest encryptions *)

type cipher (p:Type) (k:Type) (key:k) = c:AES.cipher { Encrypted p key c }
```
The type of `cipher` records the plain type, key type, and key it was generated for.

```
type keyT: Type -> Type =
  | Ideal    : p:Type -> AES.key -> i:int -> keyT p
  | Concrete : p:Type -> AES.key -> i:int -> keyT p
```
Keys record whether they were correctly generated `Ideal` in which
case their usage will provide security by type abstraction, or not, in
which they are just concrete keys that can be leaked to the adversary.

```
type scheme: Type -> Type =
  | Scheme:  p:Type -> k:Type ->
             keyrepr: (k -> AES.key) ->
             keygen:  (bool -> k) ->
             encrypt: (key:k -> p -> cipher p k key) ->
             decrypt: (key:k -> cipher p k key -> p) ->
             scheme p
```

A scheme records a plain type `p`, a function `keyrepr` for breaking
key abstraction, a key generation function `keygen`, and encryption
and decryption functions `encrypt` and `decrypt`.

```
type entry (p:Type) (k:Type) =
  | Entry : key:k -> c:cipher p k key -> plain:p -> entry p k

```


To create a cryptographic scheme one needs to provide a `plain` function
for turning `bytes` into the plain type values
and a `repr` function for turning plain type values into bytes.
```
val create: p: Type -> plain: (r -> p) -> repr: (p -> r) -> scheme p
```

The `create` function instantiates the scheme using the `AES` module.

```
let create (p:Type) plain repr =
  let c = ST.alloc 0 in
  let log : ref (list (entry p (keyT p))) = ST.alloc [] in

  let keygen : bool -> keyT p = fun safe ->
    let i = !c in
    c := !c + 1;
    let kv = AES.gen() in
    if safe
    then Ideal    p kv i
    else Concrete p kv i in

  let keyrepr k : AES.key =
    match k with
    | Ideal kv _ -> failwith "no way!"
    | Concrete kv _ -> kv in

  let encrypt: (k:keyT p -> p -> cipher p (keyT p) k) = fun k text ->
    match k with
    | Ideal kv _ ->
       let c = AES.enc kv AES.dummy in
       assume (Encrypted p k c);
       log := Entry k c text :: !log;
       c

    | Concrete kv _ ->
       let rep = repr text in
       (* NS: need the let-binding because repr is impure and we can't
          substitute it in the type of 'enc' *)
       let c = AES.enc kv rep in
       assume (Encrypted p k c);
       c in

  let decrypt: key:keyT p -> cipher p (keyT p) key -> p = fun k c ->
    match k with
    | Ideal kv _ -> (match List.find (fun (Entry k' c' _) -> k=k' && c= c') !log with
                     | Some e -> Entry?.plain e
                     | _ -> failwith "never")
    | Concrete kv _ -> plain (AES.dec kv c)  in

  Scheme p (keyT p) keyrepr keygen encrypt decrypt
```

Relying on basic cryptographic assumptions (IND-CPA), the
 ideal implementation never accesses plaintexts when using `Ideal` keys!

Formally, the `Ideal` branch can be typed using an abstract plaintype:
`encrypt` encrypts a constant AES block `c` and logs `Entry k c text`
`decrypt` returns `text` when `Entry k c text` is in the log.
As IND-CPA requires that only ciphertext that have previously
been encrypted are decrypted, this is guaranteed by typing to be always the case.

Here is how such a symmetric encryption scheme would be used in an example:
```
module SampleEncrypt
open SymEnc
let plain (x:AES.plain) = x
let repr (x:AES.plain) = x

let test() =
  let p = failwith "nice bytes" in
  let Scheme keyrepr keygen encrypt decrypt = SymEnc.create (AES.plain) plain repr in
  let k0 = keygen true in
  let k1 = keygen true in
  let c = encrypt k0 p in
  let p' = decrypt k0 c in
  assert( p == p');                // this succeeds, as we enforce functional correctness
  (* let p'' = decrypt k1 c in     // this rightfully triggers an error *)
  ()
```

-->

# A note on this document 

This document was created using Dan Leijen's <https://madoko.net> markdown processor. 

If` you find any errors in it, or would like to improve it in any way,
this is how you do it: 

1. Point your browser at <https://madoko.net>
2. Select the folder icon in the top-left corner, then "Open ... (Alt-O)"
3. Select the "GitHub" icon
4. Authorize the Madoko application to access your GitHub repositories
5. Open <github>/FStarLang/FStar/doc/tutorial/tutorial.mdk, or your fork of the FStar repo
   if you don't have access, and start editing.
6. Ctrl+S (or Synchronize from the folder tab) will bring up a dialog to
   commit and push this file to GitHub after merging with the latest changes there.

For included code snippets:

The snippets must be derived from files that are stored in the same
subtree of the git repo i.e, all the samples should be in
sub-directories of FStarLang/FStar/doc/tutorial/code

Note: in this mode (i.e., github instead of dropbox), the document
will not live update with the changes that anyone else may be making
to the document concurrently. You will only see others' changes when
you synchronize with github.

If you use Emacs to edit this you can use the
markdown-mode <http://jblevins.org/projects/markdown-mode/> for syntax
highlighting.

If you experience problems with the online editor, e.g., many examples failing with strange error messages, please ring the bells at the [mailing list] or raise a [github issue] to investigate the problem further. Someone else might have filed one before, or an old bug crawled back in.


[Title          A]: http://Title_A.com "Title          A title"


<script>
// show answers when clicked upon.
[].forEach.call( document.getElementsByClassName("answer-link"), function(answerLink) {
  answerLink.addEventListener("click", function(ev) {
    var answer = answerLink.parentNode;
    var cls = answer.className;
    if (/\bview\b/.test(cls)) {
      answer.className = cls.replace(/\s*view\s*/,"");
    }
    else {
      answer.className = cls + " view";
    }
  });
});
</script>
